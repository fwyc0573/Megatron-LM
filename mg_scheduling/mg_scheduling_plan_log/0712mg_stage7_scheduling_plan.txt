stage:7:recv_forward(batch_id=0, mg_state=help, duration=None, description=None, group_kind=pp, input__shape=[2048, 2, 5120], input__dtype=torch.float32)
stage:7:get_batch(batch_id=0, mg_state=steady, duration=None, description=None, group_kind=None, input__shape=None, input__dtype=None)
stage:7:forward_step(batch_id=0, mg_state=steady, duration=None, description=None, group_kind=None, input__shape=None, input__dtype=None)
stage:7:loss_func(batch_id=0, mg_state=steady, duration=None, description=None, group_kind=None, input__shape=None, input__dtype=None)
stage:7:backward_step(batch_id=0, mg_state=steady, duration=None, description=None, group_kind=None, input__shape=None, input__dtype=None)
stage:7:send_backward(batch_id=0, mg_state=steady, duration=None, description=None, group_kind=pp, input__shape=[2048, 2, 5120], input__dtype=torch.float32)
stage:7:dp_allreduce(batch_id=0, mg_state=finalize, duration=None, description=model_chunk.finish_grad_sync(), All-reduce / reduce-scatter across DP replicas, group_kind=dp, input__shape=None, input__dtype=None)
stage:7:ep_allreduce(batch_id=0, mg_state=finalize, duration=None, description=_allreduce_embedding_grads, All-reduce embedding grads (for pipeline parallelism), group_kind=ep, input__shape=None, input__dtype=None)
stage:7:optimizer_step(batch_id=0, mg_state=finalize, duration=None, description=None, group_kind=None, input__shape=None, input__dtype=None)
