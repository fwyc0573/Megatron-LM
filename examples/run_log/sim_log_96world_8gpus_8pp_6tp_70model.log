using world size: 1, data-parallel size: 1, context-parallel size: 1 tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
using torch.float32 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.999
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. True
  add_position_embedding .......................... True
  add_qkv_bias .................................... False
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  apply_rope_fusion ............................... True
  async_tensor_model_parallel_allreduce ........... True
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  auto_detect_ckpt_format ......................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  bias_swiglu_fusion .............................. True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  check_for_nan_in_loss_and_grad .................. True
  check_weight_hash_across_dp_replicas_interval ... None
  ckpt_fully_parallel_save ........................ False
  ckpt_step ....................................... None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  clone_scatter_output_in_embedding ............... True
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_size ........................... 1
  create_attention_mask_in_dataloader ............. True
  data_cache_path ................................. None
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 1
  data_path ....................................... ['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  ddp_bucket_size ................................. None
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  decoupled_lr .................................... None
  decoupled_min_lr ................................ None
  delay_grad_reduce ............................... True
  delay_param_gather .............................. False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  disable_straggler_on_startup .................... False
  dist_ckpt_format ................................ torch_dist
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 10
  do_trace ........................................ True
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  enable_one_logger ............................... False
  encoder_num_layers .............................. 80
  encoder_seq_length .............................. 2048
  end_weight_decay ................................ 0.01
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_model_parallel_size ...................... 1
  fake_dp ......................................... 2
  fake_gpus_per_node .............................. 8
  fake_local_rank ................................. 0
  fake_pp ......................................... 8
  fake_tp ......................................... 6
  fake_world_size ................................. 96
  fake_wrank ...................................... 0
  ffn_hidden_size ................................. 32768
  finetune ........................................ False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 4
  gradient_accumulation_fusion .................... True
  group_query_attention ........................... False
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 8192
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  is_scaling_mode ................................. True
  iter_per_epoch .................................. 1250
  kv_channels ..................................... 128
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 100
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_progress .................................... False
  log_straggler ................................... False
  log_throughput .................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.00015
  lr_decay_iters .................................. 320000
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. 0.01
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  main_tokenizer_type ............................. GPT2BPETokenizer
  make_vocab_size_divisible_by .................... 128
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 2048
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/gpt2-merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_bin_files .................................. True
  mock_data ....................................... False
  moe_aux_loss_coeff .............................. 0.0
  moe_grouped_gemm ................................ False
  moe_input_jitter_eps ............................ None
  moe_per_layer_logging ........................... False
  moe_router_load_balancing_type .................. aux_loss
  moe_router_topk ................................. 2
  moe_token_dispatcher_type ....................... allgather
  moe_token_dropping .............................. False
  moe_z_loss_coeff ................................ None
  nccl_communicator_config_path ................... None
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... LayerNorm
  nsight_start .................................... 10
  num_attention_heads ............................. 64
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... None
  num_layers ...................................... 80
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 1
  num_workers ..................................... 2
  one_logger_entity ............................... hwinf_dcm
  one_logger_project .............................. e2e-tracking
  one_logger_run_name ............................. None
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  overlap_param_gather ............................ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.float32
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... learned_absolute
  pretrained_checkpoint ........................... None
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  qk_layernorm .................................... False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... None
  recompute_method ................................ None
  recompute_num_layers ............................ None
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_attention_gate ............................ 1
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_project_dir ............................... None
  retro_verify_neighbor_count ..................... True
  rotary_interleaved .............................. False
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 2048
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  spec ............................................ None
  split ........................................... 949,50,1
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.01
  straggler_ctrlr_port ............................ 65535
  straggler_minmax_count .......................... 1
  swiglu .......................................... False
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  test_mode ....................................... False
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. None
  tokenizer_type .................................. GPT2BPETokenizer
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. False
  tp_comm_overlap_ag .............................. True
  tp_comm_overlap_cfg ............................. None
  tp_comm_overlap_rs .............................. True
  tp_comm_overlap_rs_dgrad ........................ False
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  trace_start ..................................... 10
  train_data_path ................................. None
  train_iters ..................................... 10
  train_samples ................................... None
  transformer_impl ................................ transformer_engine
  transformer_pipeline_model_parallel_size ........ 1
  untie_embeddings_and_output_weights ............. False
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_dist_ckpt ................................... False
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. False
  use_mcore_models ................................ True
  use_one_sent_docs ............................... False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  use_tp_pp_dp_mapping ............................ False
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/gpt2-vocab.json
  vocab_size ...................................... 3200
  wandb_exp_name .................................. 
  wandb_project ................................... 
  wandb_save_dir .................................. 
  weight_decay .................................... 0.01
  weight_decay_incr_style ......................... constant
  world_size ...................................... 1
  yaml_cfg ........................................ None
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 2
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initialized tensor model parallel with size 1
> initialized pipeline model parallel with size 1
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/datasets'
make: Nothing to be done for 'default'.
make: Leaving directory '/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.070 seconds
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
> compiling and loading fused kernels ...
proj187:3281697:3281697 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ens81f0
proj187:3281697:3281697 [0] NCCL INFO NCCL_SOCKET_IFNAME set to ens81f0
proj187:3281697:3281697 [0] NCCL INFO Bootstrap : Using ens81f0:192.168.50.187<0>
proj187:3281697:3281697 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
proj187:3281697:3281697 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
proj187:3281697:3281697 NCCL CALL ncclGetUniqueId(0xea063011c4c51421)
proj187:3281697:3281697 NCCL CALL ncclGroupStart()
proj187:3281697:3281697 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.18.6+cuda12.1
proj187:3281697:3281697 [0] NCCL INFO init.cc:1584 Cuda Host Alloc Size 4 pointer 0x7fbee5400000
proj187:3281697:3281941 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
proj187:3281697:3281941 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ens81f0
proj187:3281697:3281941 [0] NCCL INFO NET/Socket : Using [0]ens81f0:192.168.50.187<0>
proj187:3281697:3281941 [0] NCCL INFO Using network Socket
proj187:3281697:3281941 [0] NCCL INFO comm 0x8e232f0 rank 0 nranks 1 cudaDev 0 nvmlDev 7 busId ca000 commId 0xea063011c4c51421 - Init START
proj187:3281697:3281941 [0] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'ens81f0'
proj187:3281697:3281941 [0] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
proj187:3281697:3281941 [0] NCCL INFO CPU/1 (1/1/2)
proj187:3281697:3281941 [0] NCCL INFO + PCI[24.0] - PCI/C1000 (1000c01010000000)
proj187:3281697:3281941 [0] NCCL INFO               + PCI[24.0] - PCI/C8000 (1000c01010de13b8)
proj187:3281697:3281941 [0] NCCL INFO                             + PCI[24.0] - GPU/CA000 (0)
proj187:3281697:3281941 [0] NCCL INFO                                           + NVL[160.0] - NVS/0
proj187:3281697:3281941 [0] NCCL INFO + SYS[10.0] - CPU/0
proj187:3281697:3281941 [0] NCCL INFO CPU/0 (1/1/2)
proj187:3281697:3281941 [0] NCCL INFO + SYS[10.0] - CPU/1
proj187:3281697:3281941 [0] NCCL INFO + PCI[3.0] - NIC/17000
proj187:3281697:3281941 [0] NCCL INFO ==========================================
proj187:3281697:3281941 [0] NCCL INFO GPU/CA000 :GPU/CA000 (0/5000.000000/LOC) NVS/0 (1/160.000000/NVL) CPU/1 (3/24.000000/PHB) CPU/0 (4/10.000000/SYS) 
proj187:3281697:3281941 [0] NCCL INFO Setting affinity for GPU 7 to ffff0000,ffff0000
proj187:3281697:3281941 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 40.000000/40.000000, type LOC/PIX, sameChannels 1
proj187:3281697:3281941 [0] NCCL INFO  0 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO  1 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO  2 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO  3 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO  4 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO  5 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO  6 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO  7 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO  8 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO  9 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO 10 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO 11 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO 12 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO 13 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO 14 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO 15 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 40.000000/40.000000, type LOC/PIX, sameChannels 1
proj187:3281697:3281941 [0] NCCL INFO  0 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO  1 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO  2 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO  3 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO  4 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO  5 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO  6 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO  7 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO  8 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO  9 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO 10 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO 11 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO 12 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO 13 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO 14 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO 15 : GPU/0
proj187:3281697:3281941 [0] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
proj187:3281697:3281941 [0] NCCL INFO Channel 00/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 01/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 02/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 03/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 04/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 05/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 06/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 07/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 08/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 09/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 10/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 11/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 12/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 13/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 14/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 15/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 16/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 17/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 18/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 19/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 20/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 21/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 22/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 23/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 24/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 25/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 26/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 27/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 28/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 29/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 30/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Channel 31/32 :    0
proj187:3281697:3281941 [0] NCCL INFO Ring 00 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 01 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 02 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 03 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 04 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 05 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 06 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 07 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 08 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 09 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 10 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 11 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 12 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 13 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 14 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 15 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 16 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 17 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 18 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 19 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 20 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 21 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 22 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 23 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 24 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 25 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 26 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 27 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 28 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 29 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 30 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Ring 31 : 0 -> 0 -> 0
proj187:3281697:3281941 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
proj187:3281697:3281941 [0] NCCL INFO P2P Chunksize set to 131072
proj187:3281697:3281941 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c00000
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c00200
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c00400
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c00600
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c00800
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c00a00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c00c00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c00e00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c01000
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c01200
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c01400
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c01600
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c01800
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c01a00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c01c00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c01e00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c02000
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c02200
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c02400
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c02600
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c02800
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c02a00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c02c00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c02e00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c03000
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c03200
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c03400
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c03600
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c03800
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c03a00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c03c00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c03e00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c04000
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c04200
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c04400
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c04600
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c04800
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c04a00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c04c00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c04e00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c05000
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c05200
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c05400
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c05600
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c05800
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c05a00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c05c00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c05e00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c06000
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c06200
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c06400
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c06600
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c06800
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c06a00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c06c00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c06e00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c07000
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c07200
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c07400
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c07600
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c07800
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c07a00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c07c00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c07e00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c08000
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c08200
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c08400
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c08600
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c08800
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c08a00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c08c00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c08e00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c09000
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c09200
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c09400
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c09600
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c09800
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c09a00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c09c00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c09e00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c0a000
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c0a200
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c0a400
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c0a600
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c0a800
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c0aa00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c0ac00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c0ae00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c0b000
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c0b200
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c0b400
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c0b600
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c0b800
proj187:3281697:3281941 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7fbeb7c0ba00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7fbeb7c0bc00
proj187:3281697:3281941 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7fbeb7c0be00
proj187:3281697:3281941 [0] NCCL INFO Connected all rings
proj187:3281697:3281941 [0] NCCL INFO Connected all trees
proj187:3281697:3281941 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
proj187:3281697:3281947 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x7fbec8002f20
proj187:3281697:3281947 [0] NCCL INFO Allocated 4194660 bytes of shared memory in /dev/shm/nccl-KuZPQP
proj187:3281697:3281947 [0] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
proj187:3281697:3281947 [0] NCCL INFO proxyProgressAsync opId=0x7fbeba1b33e0 op.type=1 op.reqBuff=0x7fbec8000bb0 op.respSize=16 done
proj187:3281697:3281941 [0] NCCL INFO ncclPollProxyResponse Received new opId=0x7fbeba1b33e0
proj187:3281697:3281941 [0] NCCL INFO recvOpId=0x7fbeba1b33e0 matches expected opId=0x7fbeba1b33e0
proj187:3281697:3281947 [0] NCCL INFO Received and initiated operation=Init res=0
proj187:3281697:3281941 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x7fbec8003160
proj187:3281697:3281947 [0] NCCL INFO transport/net.cc:446 Cuda Alloc Size 67108864 pointer 0x7fbeb2000000
proj187:3281697:3281947 [0] NCCL INFO proxyProgressAsync opId=0x7fbeba1b33e0 op.type=2 op.reqBuff=0x7fbec8005cc0 op.respSize=0 done
proj187:3281697:3281941 [0] NCCL INFO ncclPollProxyResponse Received new opId=0x7fbeba1b33e0
proj187:3281697:3281947 [0] NCCL INFO Received and initiated operation=SharedInit res=0
proj187:3281697:3281941 [0] NCCL INFO recvOpId=0x7fbeba1b33e0 matches expected opId=0x7fbeba1b33e0
proj187:3281697:3281941 [0] NCCL INFO init.cc:387 Cuda Alloc Size 7728 pointer 0x7fbeb7c0c000
proj187:3281697:3281941 [0] NCCL INFO init.cc:412 Cuda Host Alloc Size 33554432 pointer 0x7fbeaa000000
proj187:3281697:3281941 [0] NCCL INFO init.cc:418 Cuda Host Alloc Size 128 pointer 0x7fbee5400200
proj187:3281697:3281941 NCCL CALL ncclCommInitRank(0x8e232f0, 1, 0xea063011c4c51421, 0, 0)
proj187:3281697:3281941 [0] NCCL INFO comm 0x8e232f0 rank 0 nranks 1 cudaDev 0 nvmlDev 7 busId ca000 commId 0xea063011c4c51421 - Init COMPLETE
proj187:3281697:3281697 NCCL CALL ncclGroupEnd()
proj187:3281697:3281697 NCCL CALL ncclGroupStart()
proj187:3281697:3281697 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7fbee5200200 recvbuff 0x7fbee5200200 count 1 datatype 1 op 0 root 0 comm 0x8e232f0 [nranks=1] stream 0x86328e0
proj187:3281697:3281697 NCCL CALL ncclAllReduce(7fbee5200200,7fbee5200200,1,1,0,0,0x8e232f0,0x86328e0)
proj187:3281697:3281697 NCCL CALL ncclGroupEnd()
proj187:3281697:3281697 NCCL CALL ncclGroupStart()
proj187:3281697:3281697 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7fbee5200000 recvbuff 0x7fbee5200000 count 1 datatype 1 op 0 root 0 comm 0x8e232f0 [nranks=1] stream 0x86328e0
proj187:3281697:3281697 NCCL CALL ncclAllReduce(7fbee5200000,7fbee5200000,1,1,0,0,0x8e232f0,0x86328e0)
proj187:3281697:3281697 NCCL CALL ncclGroupEnd()
>>> done with compiling and loading fused kernels. Compilation time: 0.886 seconds
/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/training/initialize.py:405: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400412039/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
proj187:3281697:3281697 NCCL CALL ncclGroupStart()
proj187:3281697:3281697 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7fbee5200000 recvbuff 0x7fbee5200000 count 1 datatype 8 op 3 root 0 comm 0x8e232f0 [nranks=1] stream 0x86328e0
proj187:3281697:3281697 NCCL CALL ncclAllReduce(7fbee5200000,7fbee5200000,1,8,3,0,0x8e232f0,0x86328e0)
proj187:3281697:3281697 NCCL CALL ncclGroupEnd()
time to initialize megatron (seconds): 2.091
proj187:3281697:3281697 NCCL CALL ncclGroupStart()
proj187:3281697:3281697 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7fbee5200200 recvbuff 0x7fbee5200200 count 1 datatype 1 op 0 root 0 comm 0x8e232f0 [nranks=1] stream 0x86328e0
proj187:3281697:3281697 NCCL CALL ncclAllReduce(7fbee5200200,7fbee5200200,1,1,0,0,0x8e232f0,0x86328e0)
proj187:3281697:3281697 NCCL CALL ncclGroupEnd()
[after megatron is initialized] datetime: 2024-09-22 16:59:37 
mpu_info:MPUInfo:
	dp_size=2
	tp_size=6
	pp_size=8
	mp_size=48
	world_size=96
	dp_groups=[[0, 6], [1, 7], [2, 8], [3, 9], [4, 10], [5, 11], [12, 18], [13, 19], [14, 20], [15, 21], [16, 22], [17, 23], [24, 30], [25, 31], [26, 32], [27, 33], [28, 34], [29, 35], [36, 42], [37, 43], [38, 44], [39, 45], [40, 46], [41, 47], [48, 54], [49, 55], [50, 56], [51, 57], [52, 58], [53, 59], [60, 66], [61, 67], [62, 68], [63, 69], [64, 70], [65, 71], [72, 78], [73, 79], [74, 80], [75, 81], [76, 82], [77, 83], [84, 90], [85, 91], [86, 92], [87, 93], [88, 94], [89, 95]]
	pp_groups=[[0, 12, 24, 36, 48, 60, 72, 84], [1, 13, 25, 37, 49, 61, 73, 85], [2, 14, 26, 38, 50, 62, 74, 86], [3, 15, 27, 39, 51, 63, 75, 87], [4, 16, 28, 40, 52, 64, 76, 88], [5, 17, 29, 41, 53, 65, 77, 89], [6, 18, 30, 42, 54, 66, 78, 90], [7, 19, 31, 43, 55, 67, 79, 91], [8, 20, 32, 44, 56, 68, 80, 92], [9, 21, 33, 45, 57, 69, 81, 93], [10, 22, 34, 46, 58, 70, 82, 94], [11, 23, 35, 47, 59, 71, 83, 95]]
	tp_groups=[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35], [36, 37, 38, 39, 40, 41], [42, 43, 44, 45, 46, 47], [48, 49, 50, 51, 52, 53], [54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65], [66, 67, 68, 69, 70, 71], [72, 73, 74, 75, 76, 77], [78, 79, 80, 81, 82, 83], [84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95]]
	mp_groups=[[0, 1, 2, 3, 4, 5, 12, 13, 14, 15, 16, 17, 24, 25, 26, 27, 28, 29, 36, 37, 38, 39, 40, 41, 48, 49, 50, 51, 52, 53, 60, 61, 62, 63, 64, 65, 72, 73, 74, 75, 76, 77, 84, 85, 86, 87, 88, 89], [6, 7, 8, 9, 10, 11, 18, 19, 20, 21, 22, 23, 30, 31, 32, 33, 34, 35, 42, 43, 44, 45, 46, 47, 54, 55, 56, 57, 58, 59, 66, 67, 68, 69, 70, 71, 78, 79, 80, 81, 82, 83, 90, 91, 92, 93, 94, 95]]
	ep_groups=[[0, 84], [1, 85], [2, 86], [3, 87], [4, 88], [5, 89], [6, 90], [7, 91], [8, 92], [9, 93], [10, 94], [11, 95]]
	pep_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11]]
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7fbf5d53a8b0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3281697:3281697 NCCL CALL ncclGroupStart()
proj187:3281697:3281697 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7fbee5200400 recvbuff 0x7fbee5200400 count 1 datatype 1 op 0 root 0 comm 0x8e232f0 [nranks=1] stream 0x86328e0
proj187:3281697:3281697 NCCL CALL ncclAllReduce(7fbee5200400,7fbee5200400,1,1,0,0,0x8e232f0,0x86328e0)
proj187:3281697:3281697 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3281697:3281697 NCCL CALL ncclGroupStart()
proj187:3281697:3281697 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7fbee5200200 recvbuff 0x7fbee5200200 count 1 datatype 1 op 0 root 0 comm 0x8e232f0 [nranks=1] stream 0x86328e0
proj187:3281697:3281697 NCCL CALL ncclAllReduce(7fbee5200200,7fbee5200200,1,1,0,0,0x8e232f0,0x86328e0)
proj187:3281697:3281697 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3281697:3281697 NCCL CALL ncclGroupStart()
proj187:3281697:3281697 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7fbee5200400 recvbuff 0x7fbee5200400 count 1 datatype 1 op 0 root 0 comm 0x8e232f0 [nranks=1] stream 0x86328e0
proj187:3281697:3281697 NCCL CALL ncclAllReduce(7fbee5200400,7fbee5200400,1,1,0,0,0x8e232f0,0x86328e0)
proj187:3281697:3281697 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
Traceback (most recent call last):
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/transformer/spec_utils.py", line 99, in build_module
    return module(
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/transformer/attention.py", line 357, in __init__
    super().__init__(
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/transformer/attention.py", line 84, in __init__
    self.num_attention_heads_per_partition = divide(self.config.num_attention_heads, world_size)
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/utils.py", line 37, in divide
    ensure_divisibility(numerator, denominator)
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/utils.py", line 31, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 64 is not divisible by 6

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/transformer/spec_utils.py", line 99, in build_module
    return module(
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/transformer/transformer_layer.py", line 84, in __init__
    self.self_attention = build_module(
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/transformer/spec_utils.py", line 107, in build_module
    raise type(e)(f"{str(e)} when instantiating {module.__name__}").with_traceback(
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/transformer/spec_utils.py", line 99, in build_module
    return module(
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/transformer/attention.py", line 357, in __init__
    super().__init__(
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/transformer/attention.py", line 84, in __init__
    self.num_attention_heads_per_partition = divide(self.config.num_attention_heads, world_size)
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/utils.py", line 37, in divide
    ensure_divisibility(numerator, denominator)
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/utils.py", line 31, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 64 is not divisible by 6 when instantiating SelfAttention

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/pretrain_llama.py", line 318, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/training/training.py", line 308, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(model_provider, model_type)
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/training/training.py", line 805, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/training/training.py", line 658, in get_model
    model = model_provider_func(
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/pretrain_llama.py", line 94, in model_provider
    model = GPTModel(
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/models/gpt/gpt_model.py", line 97, in __init__
    self.decoder = TransformerBlock(
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/transformer/transformer_block.py", line 148, in __init__
    self._build_layers()
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/transformer/transformer_block.py", line 163, in _build_layers
    [
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/transformer/transformer_block.py", line 164, in <listcomp>
    build_layer(layer_spec, i + 1)
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/transformer/transformer_block.py", line 159, in build_layer
    return build_module(layer_spec, config=self.config, layer_number=layer_number,)
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/transformer/spec_utils.py", line 107, in build_module
    raise type(e)(f"{str(e)} when instantiating {module.__name__}").with_traceback(
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/transformer/spec_utils.py", line 99, in build_module
    return module(
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/transformer/transformer_layer.py", line 84, in __init__
    self.self_attention = build_module(
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/transformer/spec_utils.py", line 107, in build_module
    raise type(e)(f"{str(e)} when instantiating {module.__name__}").with_traceback(
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/transformer/spec_utils.py", line 99, in build_module
    return module(
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/transformer/attention.py", line 357, in __init__
    super().__init__(
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/transformer/attention.py", line 84, in __init__
    self.num_attention_heads_per_partition = divide(self.config.num_attention_heads, world_size)
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/utils.py", line 37, in divide
    ensure_divisibility(numerator, denominator)
  File "/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/utils.py", line 31, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(numerator, denominator)
AssertionError: 64 is not divisible by 6 when instantiating SelfAttention when instantiating TransformerLayer
[2024-09-22 16:59:52,970] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3281697) of binary: /research/d1/gds/ytyang/anaconda3/envs/myenv_yc/bin/python
Traceback (most recent call last):
  File "/research/d1/gds/ytyang/anaconda3/envs/myenv_yc/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.2', 'console_scripts', 'torchrun')())
  File "/research/d1/gds/ytyang/anaconda3/envs/myenv_yc/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/research/d1/gds/ytyang/anaconda3/envs/myenv_yc/lib/python3.9/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/research/d1/gds/ytyang/anaconda3/envs/myenv_yc/lib/python3.9/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/research/d1/gds/ytyang/anaconda3/envs/myenv_yc/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/research/d1/gds/ytyang/anaconda3/envs/myenv_yc/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/research/d1/gds/ytyang/yichengfeng/Megatron-LM/pretrain_llama.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-22_16:59:52
  host      : proj187
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3281697)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
