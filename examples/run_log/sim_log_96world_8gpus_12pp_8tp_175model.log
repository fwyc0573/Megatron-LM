using world size: 1, data-parallel size: 1, context-parallel size: 1 tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
using torch.float32 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.999
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. True
  add_position_embedding .......................... True
  add_qkv_bias .................................... False
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  apply_rope_fusion ............................... True
  async_tensor_model_parallel_allreduce ........... True
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  auto_detect_ckpt_format ......................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  bias_swiglu_fusion .............................. True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  check_for_nan_in_loss_and_grad .................. True
  check_weight_hash_across_dp_replicas_interval ... None
  ckpt_fully_parallel_save ........................ False
  ckpt_step ....................................... None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  clone_scatter_output_in_embedding ............... True
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_size ........................... 1
  create_attention_mask_in_dataloader ............. True
  data_cache_path ................................. None
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 1
  data_path ....................................... ['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  ddp_bucket_size ................................. None
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  decoupled_lr .................................... None
  decoupled_min_lr ................................ None
  delay_grad_reduce ............................... True
  delay_param_gather .............................. False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  disable_straggler_on_startup .................... False
  dist_ckpt_format ................................ torch_dist
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 10
  do_trace ........................................ True
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  enable_one_logger ............................... False
  encoder_num_layers .............................. 96
  encoder_seq_length .............................. 2048
  end_weight_decay ................................ 0.01
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_model_parallel_size ...................... 1
  fake_dp ......................................... 1
  fake_gpus_per_node .............................. 8
  fake_local_rank ................................. 0
  fake_pp ......................................... 12
  fake_tp ......................................... 8
  fake_world_size ................................. 96
  fake_wrank ...................................... 0
  ffn_hidden_size ................................. 49152
  finetune ........................................ False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 2
  gradient_accumulation_fusion .................... True
  group_query_attention ........................... False
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 12288
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  is_scaling_mode ................................. True
  iter_per_epoch .................................. 1250
  kv_channels ..................................... 128
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 100
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_progress .................................... False
  log_straggler ................................... False
  log_throughput .................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.00015
  lr_decay_iters .................................. 320000
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. 0.01
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  main_tokenizer_type ............................. GPT2BPETokenizer
  make_vocab_size_divisible_by .................... 128
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 2048
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/gpt2-merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_bin_files .................................. True
  mock_data ....................................... False
  moe_aux_loss_coeff .............................. 0.0
  moe_grouped_gemm ................................ False
  moe_input_jitter_eps ............................ None
  moe_per_layer_logging ........................... False
  moe_router_load_balancing_type .................. aux_loss
  moe_router_topk ................................. 2
  moe_token_dispatcher_type ....................... allgather
  moe_token_dropping .............................. False
  moe_z_loss_coeff ................................ None
  nccl_communicator_config_path ................... None
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... LayerNorm
  nsight_start .................................... 10
  num_attention_heads ............................. 96
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... None
  num_layers ...................................... 96
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 1
  num_workers ..................................... 2
  one_logger_entity ............................... hwinf_dcm
  one_logger_project .............................. e2e-tracking
  one_logger_run_name ............................. None
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  overlap_param_gather ............................ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.float32
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... learned_absolute
  pretrained_checkpoint ........................... None
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  qk_layernorm .................................... False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... None
  recompute_method ................................ None
  recompute_num_layers ............................ None
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_attention_gate ............................ 1
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_project_dir ............................... None
  retro_verify_neighbor_count ..................... True
  rotary_interleaved .............................. False
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 2048
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  spec ............................................ None
  split ........................................... 949,50,1
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.01
  straggler_ctrlr_port ............................ 65535
  straggler_minmax_count .......................... 1
  swiglu .......................................... False
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  test_mode ....................................... False
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. None
  tokenizer_type .................................. GPT2BPETokenizer
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. False
  tp_comm_overlap_ag .............................. True
  tp_comm_overlap_cfg ............................. None
  tp_comm_overlap_rs .............................. True
  tp_comm_overlap_rs_dgrad ........................ False
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  trace_start ..................................... 10
  train_data_path ................................. None
  train_iters ..................................... 10
  train_samples ................................... None
  transformer_impl ................................ transformer_engine
  transformer_pipeline_model_parallel_size ........ 1
  untie_embeddings_and_output_weights ............. False
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_dist_ckpt ................................... False
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. False
  use_mcore_models ................................ True
  use_one_sent_docs ............................... False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  use_tp_pp_dp_mapping ............................ False
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/gpt2-vocab.json
  vocab_size ...................................... 3200
  wandb_exp_name .................................. 
  wandb_project ................................... 
  wandb_save_dir .................................. 
  weight_decay .................................... 0.01
  weight_decay_incr_style ......................... constant
  world_size ...................................... 1
  yaml_cfg ........................................ None
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 1
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initialized tensor model parallel with size 1
> initialized pipeline model parallel with size 1
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/datasets'
make: Nothing to be done for 'default'.
make: Leaving directory '/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.064 seconds
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
> compiling and loading fused kernels ...
proj187:3264676:3264676 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ens81f0
proj187:3264676:3264676 [0] NCCL INFO NCCL_SOCKET_IFNAME set to ens81f0
proj187:3264676:3264676 [0] NCCL INFO Bootstrap : Using ens81f0:192.168.50.187<0>
proj187:3264676:3264676 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
proj187:3264676:3264676 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
proj187:3264676:3264676 NCCL CALL ncclGetUniqueId(0x944568744622d990)
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.18.6+cuda12.1
proj187:3264676:3264676 [0] NCCL INFO init.cc:1584 Cuda Host Alloc Size 4 pointer 0x7f4ccf400000
proj187:3264676:3264905 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
proj187:3264676:3264905 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ens81f0
proj187:3264676:3264905 [0] NCCL INFO NET/Socket : Using [0]ens81f0:192.168.50.187<0>
proj187:3264676:3264905 [0] NCCL INFO Using network Socket
proj187:3264676:3264905 [0] NCCL INFO comm 0xa19d050 rank 0 nranks 1 cudaDev 0 nvmlDev 1 busId 2a000 commId 0x944568744622d990 - Init START
proj187:3264676:3264905 [0] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'ens81f0'
proj187:3264676:3264905 [0] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
proj187:3264676:3264905 [0] NCCL INFO CPU/0 (1/1/2)
proj187:3264676:3264905 [0] NCCL INFO + PCI[24.0] - PCI/21000 (1000c01010000000)
proj187:3264676:3264905 [0] NCCL INFO               + PCI[24.0] - PCI/28000 (1000c01010de13b8)
proj187:3264676:3264905 [0] NCCL INFO                             + PCI[24.0] - GPU/2A000 (0)
proj187:3264676:3264905 [0] NCCL INFO                                           + NVL[160.0] - NVS/0
proj187:3264676:3264905 [0] NCCL INFO + PCI[3.0] - NIC/17000
proj187:3264676:3264905 [0] NCCL INFO ==========================================
proj187:3264676:3264905 [0] NCCL INFO GPU/2A000 :GPU/2A000 (0/5000.000000/LOC) NVS/0 (1/160.000000/NVL) CPU/0 (3/24.000000/PHB) 
proj187:3264676:3264905 [0] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
proj187:3264676:3264905 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 40.000000/40.000000, type LOC/PIX, sameChannels 1
proj187:3264676:3264905 [0] NCCL INFO  0 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO  1 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO  2 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO  3 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO  4 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO  5 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO  6 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO  7 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO  8 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO  9 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO 10 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO 11 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO 12 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO 13 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO 14 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO 15 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 40.000000/40.000000, type LOC/PIX, sameChannels 1
proj187:3264676:3264905 [0] NCCL INFO  0 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO  1 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO  2 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO  3 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO  4 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO  5 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO  6 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO  7 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO  8 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO  9 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO 10 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO 11 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO 12 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO 13 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO 14 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO 15 : GPU/0
proj187:3264676:3264905 [0] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3264905 [0] NCCL INFO Channel 00/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 01/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 02/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 03/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 04/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 05/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 06/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 07/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 08/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 09/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 10/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 11/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 12/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 13/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 14/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 15/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 16/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 17/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 18/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 19/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 20/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 21/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 22/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 23/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 24/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 25/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 26/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 27/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 28/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 29/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 30/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Channel 31/32 :    0
proj187:3264676:3264905 [0] NCCL INFO Ring 00 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 01 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 02 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 03 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 04 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 05 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 06 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 07 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 08 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 09 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 10 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 11 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 12 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 13 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 14 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 15 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 16 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 17 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 18 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 19 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 20 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 21 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 22 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 23 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 24 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 25 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 26 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 27 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 28 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 29 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 30 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Ring 31 : 0 -> 0 -> 0
proj187:3264676:3264905 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
proj187:3264676:3264905 [0] NCCL INFO P2P Chunksize set to 131072
proj187:3264676:3264905 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc00000
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc00200
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc00400
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc00600
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc00800
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc00a00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc00c00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc00e00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc01000
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc01200
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc01400
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc01600
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc01800
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc01a00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc01c00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc01e00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc02000
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc02200
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc02400
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc02600
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc02800
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc02a00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc02c00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc02e00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc03000
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc03200
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc03400
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc03600
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc03800
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc03a00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc03c00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc03e00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc04000
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc04200
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc04400
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc04600
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc04800
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc04a00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc04c00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc04e00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc05000
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc05200
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc05400
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc05600
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc05800
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc05a00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc05c00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc05e00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc06000
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc06200
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc06400
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc06600
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc06800
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc06a00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc06c00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc06e00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc07000
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc07200
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc07400
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc07600
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc07800
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc07a00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc07c00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc07e00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc08000
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc08200
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc08400
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc08600
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc08800
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc08a00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc08c00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc08e00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc09000
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc09200
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc09400
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc09600
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc09800
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc09a00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc09c00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc09e00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc0a000
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc0a200
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc0a400
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc0a600
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc0a800
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc0aa00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc0ac00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc0ae00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc0b000
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc0b200
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc0b400
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc0b600
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc0b800
proj187:3264676:3264905 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc0ba00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc0bc00
proj187:3264676:3264905 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc0be00
proj187:3264676:3264905 [0] NCCL INFO Connected all rings
proj187:3264676:3264905 [0] NCCL INFO Connected all trees
proj187:3264676:3264905 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
proj187:3264676:3264914 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x7f4cb4002f20
proj187:3264676:3264914 [0] NCCL INFO Allocated 4194660 bytes of shared memory in /dev/shm/nccl-oR4Vl0
proj187:3264676:3264914 [0] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
proj187:3264676:3264914 [0] NCCL INFO proxyProgressAsync opId=0x7f4ca218c9c0 op.type=1 op.reqBuff=0x7f4cb4000bb0 op.respSize=16 done
proj187:3264676:3264905 [0] NCCL INFO ncclPollProxyResponse Received new opId=0x7f4ca218c9c0
proj187:3264676:3264914 [0] NCCL INFO Received and initiated operation=Init res=0
proj187:3264676:3264905 [0] NCCL INFO recvOpId=0x7f4ca218c9c0 matches expected opId=0x7f4ca218c9c0
proj187:3264676:3264905 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x7f4cb4003160
proj187:3264676:3264914 [0] NCCL INFO transport/net.cc:446 Cuda Alloc Size 67108864 pointer 0x7f4c94000000
proj187:3264676:3264914 [0] NCCL INFO proxyProgressAsync opId=0x7f4ca218c9c0 op.type=2 op.reqBuff=0x7f4cb4005cc0 op.respSize=0 done
proj187:3264676:3264905 [0] NCCL INFO ncclPollProxyResponse Received new opId=0x7f4ca218c9c0
proj187:3264676:3264914 [0] NCCL INFO Received and initiated operation=SharedInit res=0
proj187:3264676:3264905 [0] NCCL INFO recvOpId=0x7f4ca218c9c0 matches expected opId=0x7f4ca218c9c0
proj187:3264676:3264905 [0] NCCL INFO init.cc:387 Cuda Alloc Size 7728 pointer 0x7f4c9fc0c000
proj187:3264676:3264905 [0] NCCL INFO init.cc:412 Cuda Host Alloc Size 33554432 pointer 0x7f4c92000000
proj187:3264676:3264905 [0] NCCL INFO init.cc:418 Cuda Host Alloc Size 128 pointer 0x7f4ccf400200
proj187:3264676:3264905 NCCL CALL ncclCommInitRank(0xa19d050, 1, 0x944568744622d990, 0, 0)
proj187:3264676:3264905 [0] NCCL INFO comm 0xa19d050 rank 0 nranks 1 cudaDev 0 nvmlDev 1 busId 2a000 commId 0x944568744622d990 - Init COMPLETE
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf200200 recvbuff 0x7f4ccf200200 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf200200,7f4ccf200200,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf200000 recvbuff 0x7f4ccf200000 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf200000,7f4ccf200000,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
>>> done with compiling and loading fused kernels. Compilation time: 0.885 seconds
/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/training/initialize.py:405: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400412039/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf200000 recvbuff 0x7f4ccf200000 count 1 datatype 8 op 3 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf200000,7f4ccf200000,1,8,3,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
time to initialize megatron (seconds): 2.080
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf200200 recvbuff 0x7f4ccf200200 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf200200,7f4ccf200200,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
[after megatron is initialized] datetime: 2024-09-22 16:31:54 
mpu_info:MPUInfo:
	dp_size=1
	tp_size=8
	pp_size=12
	mp_size=96
	world_size=96
	dp_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95]]
	pp_groups=[[0, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88], [1, 9, 17, 25, 33, 41, 49, 57, 65, 73, 81, 89], [2, 10, 18, 26, 34, 42, 50, 58, 66, 74, 82, 90], [3, 11, 19, 27, 35, 43, 51, 59, 67, 75, 83, 91], [4, 12, 20, 28, 36, 44, 52, 60, 68, 76, 84, 92], [5, 13, 21, 29, 37, 45, 53, 61, 69, 77, 85, 93], [6, 14, 22, 30, 38, 46, 54, 62, 70, 78, 86, 94], [7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95]]
	tp_groups=[[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31], [32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47], [48, 49, 50, 51, 52, 53, 54, 55], [56, 57, 58, 59, 60, 61, 62, 63], [64, 65, 66, 67, 68, 69, 70, 71], [72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87], [88, 89, 90, 91, 92, 93, 94, 95]]
	mp_groups=[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]]
	ep_groups=[[0, 88], [1, 89], [2, 90], [3, 91], [4, 92], [5, 93], [6, 94], [7, 95]]
	pep_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf200400 recvbuff 0x7f4ccf200400 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf200400,7f4ccf200400,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf200200 recvbuff 0x7f4ccf200200 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf200200,7f4ccf200200,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf200400 recvbuff 0x7f4ccf200400 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf200400,7f4ccf200400,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3264676:3264676 NCCL CALL ncclGetUniqueId(0x90072bb49f1bd96)
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO init.cc:1584 Cuda Host Alloc Size 4 pointer 0x7f4ccf470400
proj187:3264676:3265240 [0] NCCL INFO Using network Socket
proj187:3264676:3265240 [0] NCCL INFO comm 0xe59efc0 rank 0 nranks 1 cudaDev 0 nvmlDev 1 busId 2a000 commId 0x90072bb49f1bd96 - Init START
proj187:3264676:3265240 [0] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'ens81f0'
proj187:3264676:3265240 [0] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
proj187:3264676:3265240 [0] NCCL INFO CPU/0 (1/1/2)
proj187:3264676:3265240 [0] NCCL INFO + PCI[24.0] - PCI/21000 (1000c01010000000)
proj187:3264676:3265240 [0] NCCL INFO               + PCI[24.0] - PCI/28000 (1000c01010de13b8)
proj187:3264676:3265240 [0] NCCL INFO                             + PCI[24.0] - GPU/2A000 (0)
proj187:3264676:3265240 [0] NCCL INFO                                           + NVL[160.0] - NVS/0
proj187:3264676:3265240 [0] NCCL INFO + PCI[3.0] - NIC/17000
proj187:3264676:3265240 [0] NCCL INFO ==========================================
proj187:3264676:3265240 [0] NCCL INFO GPU/2A000 :GPU/2A000 (0/5000.000000/LOC) NVS/0 (1/160.000000/NVL) CPU/0 (3/24.000000/PHB) 
proj187:3264676:3265240 [0] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
proj187:3264676:3265240 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 40.000000/40.000000, type LOC/PIX, sameChannels 1
proj187:3264676:3265240 [0] NCCL INFO  0 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO  1 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO  2 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO  3 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO  4 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO  5 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO  6 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO  7 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO  8 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO  9 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO 10 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO 11 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO 12 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO 13 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO 14 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO 15 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 40.000000/40.000000, type LOC/PIX, sameChannels 1
proj187:3264676:3265240 [0] NCCL INFO  0 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO  1 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO  2 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO  3 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO  4 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO  5 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO  6 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO  7 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO  8 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO  9 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO 10 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO 11 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO 12 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO 13 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO 14 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO 15 : GPU/0
proj187:3264676:3265240 [0] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265240 [0] NCCL INFO Channel 00/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 01/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 02/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 03/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 04/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 05/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 06/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 07/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 08/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 09/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 10/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 11/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 12/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 13/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 14/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 15/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 16/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 17/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 18/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 19/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 20/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 21/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 22/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 23/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 24/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 25/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 26/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 27/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 28/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 29/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 30/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Channel 31/32 :    0
proj187:3264676:3265240 [0] NCCL INFO Ring 00 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 01 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 02 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 03 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 04 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 05 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 06 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 07 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 08 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 09 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 10 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 11 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 12 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 13 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 14 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 15 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 16 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 17 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 18 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 19 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 20 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 21 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 22 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 23 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 24 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 25 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 26 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 27 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 28 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 29 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 30 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Ring 31 : 0 -> 0 -> 0
proj187:3264676:3265240 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
proj187:3264676:3265240 [0] NCCL INFO P2P Chunksize set to 131072
proj187:3264676:3265240 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc0e000
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc0e200
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc0e400
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc0e600
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc0e800
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc0ea00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc0ec00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc0ee00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc0f000
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc0f200
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc0f400
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc0f600
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc0f800
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc0fa00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc0fc00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc0fe00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc10000
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc10200
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc10400
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc10600
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc10800
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc10a00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc10c00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc10e00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc11000
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc11200
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc11400
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc11600
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc11800
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc11a00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc11c00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc11e00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc12000
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc12200
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc12400
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc12600
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc12800
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc12a00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc12c00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc12e00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc13000
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc13200
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc13400
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc13600
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc13800
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc13a00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc13c00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc13e00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc14000
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc14200
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc14400
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc14600
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc14800
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc14a00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc14c00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc14e00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc15000
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc15200
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc15400
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc15600
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc15800
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc15a00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc15c00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc15e00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc16000
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc16200
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc16400
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc16600
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc16800
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc16a00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc16c00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc16e00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc17000
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc17200
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc17400
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc17600
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc17800
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc17a00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc17c00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc17e00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc18000
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc18200
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc18400
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc18600
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc18800
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc18a00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc18c00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc18e00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc19000
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc19200
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc19400
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc19600
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc19800
proj187:3264676:3265240 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc19a00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc19c00
proj187:3264676:3265240 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc19e00
proj187:3264676:3265240 [0] NCCL INFO Connected all rings
proj187:3264676:3265240 [0] NCCL INFO Connected all trees
proj187:3264676:3265240 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
proj187:3264676:3265241 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x7f4c6c002f20
proj187:3264676:3265241 [0] NCCL INFO Allocated 4194660 bytes of shared memory in /dev/shm/nccl-EHEPNn
proj187:3264676:3265241 [0] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
proj187:3264676:3265241 [0] NCCL INFO proxyProgressAsync opId=0x7f4a78058c40 op.type=1 op.reqBuff=0x7f4c6c000bb0 op.respSize=16 done
proj187:3264676:3265240 [0] NCCL INFO ncclPollProxyResponse Received new opId=0x7f4a78058c40
proj187:3264676:3265241 [0] NCCL INFO Received and initiated operation=Init res=0
proj187:3264676:3265240 [0] NCCL INFO recvOpId=0x7f4a78058c40 matches expected opId=0x7f4a78058c40
proj187:3264676:3265240 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x7f4c6c003160
proj187:3264676:3265241 [0] NCCL INFO transport/net.cc:446 Cuda Alloc Size 67108864 pointer 0x7f4a6c000000
proj187:3264676:3265241 [0] NCCL INFO proxyProgressAsync opId=0x7f4a78058c40 op.type=2 op.reqBuff=0x7f4c6c005cc0 op.respSize=0 done
proj187:3264676:3265240 [0] NCCL INFO ncclPollProxyResponse Received new opId=0x7f4a78058c40
proj187:3264676:3265241 [0] NCCL INFO Received and initiated operation=SharedInit res=0
proj187:3264676:3265240 [0] NCCL INFO recvOpId=0x7f4a78058c40 matches expected opId=0x7f4a78058c40
proj187:3264676:3265240 [0] NCCL INFO init.cc:387 Cuda Alloc Size 7728 pointer 0x7f4c9fc1a000
proj187:3264676:3265240 [0] NCCL INFO init.cc:412 Cuda Host Alloc Size 33554432 pointer 0x7f4c5e000000
proj187:3264676:3265240 [0] NCCL INFO init.cc:418 Cuda Host Alloc Size 128 pointer 0x7f4ccf470600
proj187:3264676:3265240 NCCL CALL ncclCommInitRank(0xe59efc0, 1, 0x90072bb49f1bd96, 0, 0)
proj187:3264676:3265240 [0] NCCL INFO comm 0xe59efc0 rank 0 nranks 1 cudaDev 0 nvmlDev 1 busId 2a000 commId 0x90072bb49f1bd96 - Init COMPLETE
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4cba000000 recvbuff 0x7f4cba000000 count 77266944 datatype 7 op 0 root 0 comm 0xe59efc0 [nranks=1] stream 0xa1b3c10
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4cba000000,7f4cba000000,77266944,7,0,0,0xe59efc0,0xa1b3c10)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1915047936
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1915047936 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGetUniqueId(0x4d30b5387fb3ff7f)
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO init.cc:1584 Cuda Host Alloc Size 4 pointer 0x7f4ccf470800
proj187:3264676:3265246 [0] NCCL INFO Using network Socket
proj187:3264676:3265246 [0] NCCL INFO comm 0x1a6dea30 rank 0 nranks 1 cudaDev 0 nvmlDev 1 busId 2a000 commId 0x4d30b5387fb3ff7f - Init START
proj187:3264676:3265246 [0] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'ens81f0'
proj187:3264676:3265246 [0] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
proj187:3264676:3265246 [0] NCCL INFO CPU/0 (1/1/2)
proj187:3264676:3265246 [0] NCCL INFO + PCI[24.0] - PCI/21000 (1000c01010000000)
proj187:3264676:3265246 [0] NCCL INFO               + PCI[24.0] - PCI/28000 (1000c01010de13b8)
proj187:3264676:3265246 [0] NCCL INFO                             + PCI[24.0] - GPU/2A000 (0)
proj187:3264676:3265246 [0] NCCL INFO                                           + NVL[160.0] - NVS/0
proj187:3264676:3265246 [0] NCCL INFO + PCI[3.0] - NIC/17000
proj187:3264676:3265246 [0] NCCL INFO ==========================================
proj187:3264676:3265246 [0] NCCL INFO GPU/2A000 :GPU/2A000 (0/5000.000000/LOC) NVS/0 (1/160.000000/NVL) CPU/0 (3/24.000000/PHB) 
proj187:3264676:3265246 [0] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
proj187:3264676:3265246 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 40.000000/40.000000, type LOC/PIX, sameChannels 1
proj187:3264676:3265246 [0] NCCL INFO  0 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO  1 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO  2 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO  3 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO  4 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO  5 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO  6 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO  7 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO  8 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO  9 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO 10 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO 11 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO 12 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO 13 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO 14 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO 15 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 40.000000/40.000000, type LOC/PIX, sameChannels 1
proj187:3264676:3265246 [0] NCCL INFO  0 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO  1 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO  2 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO  3 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO  4 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO  5 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO  6 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO  7 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO  8 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO  9 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO 10 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO 11 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO 12 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO 13 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO 14 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO 15 : GPU/0
proj187:3264676:3265246 [0] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
proj187:3264676:3265246 [0] NCCL INFO Channel 00/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 01/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 02/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 03/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 04/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 05/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 06/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 07/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 08/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 09/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 10/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 11/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 12/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 13/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 14/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 15/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 16/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 17/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 18/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 19/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 20/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 21/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 22/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 23/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 24/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 25/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 26/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 27/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 28/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 29/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 30/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Channel 31/32 :    0
proj187:3264676:3265246 [0] NCCL INFO Ring 00 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 01 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 02 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 03 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 04 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 05 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 06 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 07 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 08 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 09 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 10 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 11 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 12 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 13 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 14 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 15 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 16 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 17 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 18 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 19 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 20 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 21 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 22 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 23 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 24 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 25 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 26 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 27 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 28 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 29 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 30 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Ring 31 : 0 -> 0 -> 0
proj187:3264676:3265246 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
proj187:3264676:3265246 [0] NCCL INFO P2P Chunksize set to 131072
proj187:3264676:3265246 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc5c800
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc5ca00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc5cc00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc5ce00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc5d000
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc5d200
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc5d400
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc5d600
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc5d800
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc5da00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc5dc00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc5de00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc5e000
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc5e200
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc5e400
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc5e600
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc5e800
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc5ea00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc5ec00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc5ee00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc5f000
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc5f200
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc5f400
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc5f600
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc5f800
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc5fa00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc5fc00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc5fe00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc60000
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc60200
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc60400
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc60600
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc60800
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc60a00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc60c00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc60e00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc61000
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc61200
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc61400
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc61600
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc61800
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc61a00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc61c00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc61e00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc62000
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc62200
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc62400
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc62600
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc62800
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc62a00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc62c00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc62e00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc63000
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc63200
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc63400
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc63600
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc63800
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc63a00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc63c00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc63e00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc64000
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc64200
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc64400
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc64600
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc64800
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc64a00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc64c00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc64e00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc65000
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc65200
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc65400
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc65600
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc65800
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc65a00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc65c00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc65e00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc66000
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc66200
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc66400
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc66600
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc66800
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc66a00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc66c00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc66e00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc67000
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc67200
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc67400
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc67600
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc67800
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc67a00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc67c00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc67e00
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc68000
proj187:3264676:3265246 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f4c9fc68200
proj187:3264676:3265246 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f4c9fc68400
proj187:3264676:3265246 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f4c9fc68600
proj187:3264676:3265246 [0] NCCL INFO Connected all rings
proj187:3264676:3265246 [0] NCCL INFO Connected all trees
proj187:3264676:3265246 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
proj187:3264676:3265247 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x7f4444002f20
proj187:3264676:3265247 [0] NCCL INFO Allocated 4194660 bytes of shared memory in /dev/shm/nccl-BcucRB
proj187:3264676:3265247 [0] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
proj187:3264676:3265247 [0] NCCL INFO proxyProgressAsync opId=0x7f4438058c40 op.type=1 op.reqBuff=0x7f4444000bb0 op.respSize=16 done
proj187:3264676:3265246 [0] NCCL INFO ncclPollProxyResponse Received new opId=0x7f4438058c40
proj187:3264676:3265247 [0] NCCL INFO Received and initiated operation=Init res=0
proj187:3264676:3265246 [0] NCCL INFO recvOpId=0x7f4438058c40 matches expected opId=0x7f4438058c40
proj187:3264676:3265246 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x7f4444003160
proj187:3264676:3265247 [0] NCCL INFO transport/net.cc:446 Cuda Alloc Size 67108864 pointer 0x7f4434000000
proj187:3264676:3265247 [0] NCCL INFO proxyProgressAsync opId=0x7f4438058c40 op.type=2 op.reqBuff=0x7f4444005cc0 op.respSize=0 done
proj187:3264676:3265246 [0] NCCL INFO ncclPollProxyResponse Received new opId=0x7f4438058c40
proj187:3264676:3265247 [0] NCCL INFO Received and initiated operation=SharedInit res=0
proj187:3264676:3265246 [0] NCCL INFO recvOpId=0x7f4438058c40 matches expected opId=0x7f4438058c40
proj187:3264676:3265246 [0] NCCL INFO init.cc:387 Cuda Alloc Size 7728 pointer 0x7f4c9fc68800
proj187:3264676:3265246 [0] NCCL INFO init.cc:412 Cuda Host Alloc Size 33554432 pointer 0x7f4432000000
proj187:3264676:3265246 [0] NCCL INFO init.cc:418 Cuda Host Alloc Size 128 pointer 0x7f4ccf470a00
proj187:3264676:3265246 NCCL CALL ncclCommInitRank(0x1a6dea30, 1, 0x4d30b5387fb3ff7f, 0, 0)
proj187:3264676:3265246 [0] NCCL INFO comm 0x1a6dea30 rank 0 nranks 1 cudaDev 0 nvmlDev 1 busId 2a000 commId 0x4d30b5387fb3ff7f - Init COMPLETE
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec894a00 recvbuff 0x7f4aec894a00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec894a00,7f4aec894a00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 0, finish warm up ...
rank_id = 0, input_tensor_shapes: []
rank:0,cuda fwd time: 170.9943389892578
rank:0, fwd_subop num: 17, fwd_subop: ['trace_src_func=_reduce,duration=0.42,timestamp=4407349013.26,input__shape=[2, 2048, 12288],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.38,timestamp=4407349024.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.39,timestamp=4407349035.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.95,timestamp=4407349045.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.2,timestamp=4407349055.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.17,timestamp=4407349066.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.27,timestamp=4407349076.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.2,timestamp=4407349087.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.38,timestamp=4407349097.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.27,timestamp=4407349108.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.37,timestamp=4407349118.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.31,timestamp=4407349129.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.92,timestamp=4407349139.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.02,timestamp=4407349150.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.43,timestamp=4407349160.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.17,timestamp=4407349171.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.56,timestamp=4407349182.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 0, finish FWD profile ...
rank:0, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.01,timestamp=4407349199.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.74,timestamp=4407349223.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.34,timestamp=4407349243.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.68,timestamp=4407349268.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.11,timestamp=4407349288.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.27,timestamp=4407349312.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.47,timestamp=4407349331.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.23,timestamp=4407349355.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.52,timestamp=4407349374.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.07,timestamp=4407349399.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.61,timestamp=4407349418.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.14,timestamp=4407349442.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.77,timestamp=4407349461.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.21,timestamp=4407349486.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.97,timestamp=4407349505.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.19,timestamp=4407349529.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:0, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f42f79ffe00 recvbuff 0x7f42f79ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f42f79ffe00,7f42f79ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:0,optimizer_step time: 52.109310150146484
rank:0, finish optimizer.step profile ...
rank:0, Before memory release - Allocated: 30868228608, Reserved: 50092572672
rank:0, trace log has been written to txt...
rank:0, finish release GPU memory ...
rank:0, After memory release - Allocated: 15346516992, Reserved: 15546187776
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf200200 recvbuff 0x7f4ccf200200 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf200200,7f4ccf200200,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf200200 recvbuff 0x7f4ccf200200 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf200200,7f4ccf200200,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4870000000 recvbuff 0x7f4870000000 count 77266944 datatype 7 op 0 root 0 comm 0xe59efc0 [nranks=1] stream 0xa1b3c10
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4870000000,7f4870000000,77266944,7,0,0,0xe59efc0,0xa1b3c10)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1915047936
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1915047936 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffe00 recvbuff 0x7f4aec9ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffe00,7f4aec9ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 1, finish warm up ...
rank_id = 1, input_tensor_shapes: []
rank:1,cuda fwd time: 172.7836151123047
rank:1, fwd_subop num: 17, fwd_subop: ['trace_src_func=_reduce,duration=0.45,timestamp=4407351020.67,input__shape=[2, 2048, 12288],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.22,timestamp=4407351032.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.24,timestamp=4407351042.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.49,timestamp=4407351052.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.52,timestamp=4407351063.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4407351073.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.54,timestamp=4407351083.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.28,timestamp=4407351094.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.59,timestamp=4407351104.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4407351115.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.04,timestamp=4407351125.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4407351136.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.79,timestamp=4407351147.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.43,timestamp=4407351158.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.64,timestamp=4407351169.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4407351180.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.72,timestamp=4407351191.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 1, finish FWD profile ...
rank:1, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.39,timestamp=4407351208.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.21,timestamp=4407351233.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.9,timestamp=4407351252.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.06,timestamp=4407351276.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.79,timestamp=4407351296.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.28,timestamp=4407351320.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.67,timestamp=4407351339.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.45,timestamp=4407351363.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.89,timestamp=4407351383.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.36,timestamp=4407351407.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.94,timestamp=4407351427.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.34,timestamp=4407351451.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.17,timestamp=4407351471.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.55,timestamp=4407351495.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.91,timestamp=4407351515.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.57,timestamp=4407351539.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:1, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3f659ffc00 recvbuff 0x7f3f659ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3f659ffc00,7f3f659ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:1,optimizer_step time: 52.01203155517578
rank:1, finish optimizer.step profile ...
rank:1, Before memory release - Allocated: 46189316608, Reserved: 65603108864
rank:1, trace log has been written to txt...
rank:1, finish release GPU memory ...
rank:1, After memory release - Allocated: 15346516992, Reserved: 30792482816
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4cba000000 recvbuff 0x7f4cba000000 count 77266944 datatype 7 op 0 root 0 comm 0xe59efc0 [nranks=1] stream 0xa1b3c10
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4cba000000,7f4cba000000,77266944,7,0,0,0xe59efc0,0xa1b3c10)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 1915047936
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1915047936 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec893200 recvbuff 0x7f4aec893200 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec893200,7f4aec893200,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 2, finish warm up ...
rank_id = 2, input_tensor_shapes: []
rank:2,cuda fwd time: 173.25465393066406
rank:2, fwd_subop num: 17, fwd_subop: ['trace_src_func=_reduce,duration=0.45,timestamp=4407352954.29,input__shape=[2, 2048, 12288],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.54,timestamp=4407352965.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.24,timestamp=4407352976.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.02,timestamp=4407352986.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.57,timestamp=4407352996.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.63,timestamp=4407353007.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.92,timestamp=4407353018.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.68,timestamp=4407353029.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.94,timestamp=4407353039.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.7,timestamp=4407353050.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.9,timestamp=4407353061.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4407353071.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.52,timestamp=4407353082.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.11,timestamp=4407353093.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.45,timestamp=4407353103.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.91,timestamp=4407353115.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.41,timestamp=4407353125.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 2, finish FWD profile ...
rank:2, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.82,timestamp=4407353142.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.91,timestamp=4407353166.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.93,timestamp=4407353186.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.28,timestamp=4407353210.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.91,timestamp=4407353229.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.28,timestamp=4407353254.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.97,timestamp=4407353273.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.12,timestamp=4407353297.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.83,timestamp=4407353317.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.14,timestamp=4407353341.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.91,timestamp=4407353360.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.64,timestamp=4407353385.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.64,timestamp=4407353404.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.42,timestamp=4407353428.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.62,timestamp=4407353447.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.25,timestamp=4407353472.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:2, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:2,optimizer_step time: 52.11955261230469
rank:2, finish optimizer.step profile ...
rank:2, Before memory release - Allocated: 46189316608, Reserved: 65603108864
rank:2, trace log has been written to txt...
rank:2, finish release GPU memory ...
rank:2, After memory release - Allocated: 15346516992, Reserved: 30792482816
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffe00 recvbuff 0x7f4aec9ffe00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffe00,7f4aec9ffe00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f479a9fc800 recvbuff 0x7f479a9fc800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f479a9fc800,7f479a9fc800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f479a9fca00 recvbuff 0x7f479a9fca00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f479a9fca00,7f479a9fca00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4870000000 recvbuff 0x7f4870000000 count 77266944 datatype 7 op 0 root 0 comm 0xe59efc0 [nranks=1] stream 0xa1b3c10
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4870000000,7f4870000000,77266944,7,0,0,0xe59efc0,0xa1b3c10)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 1915047936
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1915047936 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f479a9fcc00 recvbuff 0x7f479a9fcc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f479a9fcc00,7f479a9fcc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 3, finish warm up ...
rank_id = 3, input_tensor_shapes: []
rank:3,cuda fwd time: 172.98431396484375
rank:3, fwd_subop num: 17, fwd_subop: ['trace_src_func=_reduce,duration=0.47,timestamp=4407354797.05,input__shape=[2, 2048, 12288],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.84,timestamp=4407354808.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.54,timestamp=4407354818.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.61,timestamp=4407354829.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.7,timestamp=4407354839.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.82,timestamp=4407354850.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.97,timestamp=4407354860.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.04,timestamp=4407354871.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.09,timestamp=4407354882.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.06,timestamp=4407354893.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.09,timestamp=4407354903.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.75,timestamp=4407354914.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.77,timestamp=4407354925.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.64,timestamp=4407354936.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.59,timestamp=4407354947.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4407354958.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.53,timestamp=4407354968.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 3, finish FWD profile ...
rank:3, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.88,timestamp=4407354984.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.12,timestamp=4407355009.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.82,timestamp=4407355028.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.0,timestamp=4407355052.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.69,timestamp=4407355072.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.12,timestamp=4407355096.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.0,timestamp=4407355116.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.3,timestamp=4407355140.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.94,timestamp=4407355159.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.68,timestamp=4407355184.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.91,timestamp=4407355204.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.73,timestamp=4407355228.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.77,timestamp=4407355248.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.39,timestamp=4407355272.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.88,timestamp=4407355291.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.28,timestamp=4407355315.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:3, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3f659ffa00 recvbuff 0x7f3f659ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3f659ffa00,7f3f659ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:3,optimizer_step time: 51.99359893798828
rank:3, finish optimizer.step profile ...
rank:3, Before memory release - Allocated: 46189316608, Reserved: 65603108864
rank:3, trace log has been written to txt...
rank:3, finish release GPU memory ...
rank:3, After memory release - Allocated: 15346516992, Reserved: 30792482816
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffa00 recvbuff 0x7f4ccf3ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffa00,7f4ccf3ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4cba000000 recvbuff 0x7f4cba000000 count 77266944 datatype 7 op 0 root 0 comm 0xe59efc0 [nranks=1] stream 0xa1b3c10
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4cba000000,7f4cba000000,77266944,7,0,0,0xe59efc0,0xa1b3c10)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 0): 1915047936
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1915047936 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f479a9ffc00 recvbuff 0x7f479a9ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f479a9ffc00,7f479a9ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 4, finish warm up ...
rank_id = 4, input_tensor_shapes: []
rank:4,cuda fwd time: 177.04652404785156
rank:4, fwd_subop num: 17, fwd_subop: ['trace_src_func=_reduce,duration=0.48,timestamp=4407356687.6,input__shape=[2, 2048, 12288],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.61,timestamp=4407356705.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.34,timestamp=4407356715.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.45,timestamp=4407356725.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.39,timestamp=4407356736.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.6,timestamp=4407356746.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.39,timestamp=4407356756.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4407356767.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.61,timestamp=4407356777.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.55,timestamp=4407356787.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.77,timestamp=4407356798.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4407356808.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.62,timestamp=4407356819.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.51,timestamp=4407356830.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.67,timestamp=4407356840.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.16,timestamp=4407356852.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4407356863.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 4, finish FWD profile ...
rank:4, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.45,timestamp=4407356879.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.14,timestamp=4407356904.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.6,timestamp=4407356924.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.94,timestamp=4407356949.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.61,timestamp=4407356969.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.64,timestamp=4407356994.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.13,timestamp=4407357013.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.48,timestamp=4407357038.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.8,timestamp=4407357057.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.25,timestamp=4407357081.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.84,timestamp=4407357101.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.35,timestamp=4407357125.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.94,timestamp=4407357144.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.4,timestamp=4407357169.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.2,timestamp=4407357188.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.59,timestamp=4407357213.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:4, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3f659ffc00 recvbuff 0x7f3f659ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3f659ffc00,7f3f659ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:4,optimizer_step time: 51.954689025878906
rank:4, finish optimizer.step profile ...
rank:4, Before memory release - Allocated: 46189316608, Reserved: 65603108864
rank:4, trace log has been written to txt...
rank:4, finish release GPU memory ...
rank:4, After memory release - Allocated: 15346516992, Reserved: 30792482816
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4870000000 recvbuff 0x7f4870000000 count 77266944 datatype 7 op 0 root 0 comm 0xe59efc0 [nranks=1] stream 0xa1b3c10
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4870000000,7f4870000000,77266944,7,0,0,0xe59efc0,0xa1b3c10)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 0): 1915047936
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1915047936 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f479a927a00 recvbuff 0x7f479a927a00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f479a927a00,7f479a927a00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 5, finish warm up ...
rank_id = 5, input_tensor_shapes: []
rank:5,cuda fwd time: 174.39642333984375
rank:5, fwd_subop num: 17, fwd_subop: ['trace_src_func=_reduce,duration=0.48,timestamp=4407358482.63,input__shape=[2, 2048, 12288],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.84,timestamp=4407358494.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.55,timestamp=4407358504.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4407358514.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.77,timestamp=4407358524.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.8,timestamp=4407358535.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.09,timestamp=4407358546.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.09,timestamp=4407358557.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.2,timestamp=4407358567.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.23,timestamp=4407358578.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.19,timestamp=4407358589.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.84,timestamp=4407358600.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.93,timestamp=4407358611.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.76,timestamp=4407358622.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.89,timestamp=4407358633.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.73,timestamp=4407358644.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.81,timestamp=4407358655.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 5, finish FWD profile ...
rank:5, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.49,timestamp=4407358672.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.29,timestamp=4407358696.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.81,timestamp=4407358716.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.47,timestamp=4407358740.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.66,timestamp=4407358759.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.21,timestamp=4407358783.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.84,timestamp=4407358803.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.41,timestamp=4407358827.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.1,timestamp=4407358847.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.48,timestamp=4407358871.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.12,timestamp=4407358891.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.58,timestamp=4407358915.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.93,timestamp=4407358935.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.58,timestamp=4407358959.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.68,timestamp=4407358979.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.61,timestamp=4407359003.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:5, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:5,optimizer_step time: 52.05196762084961
rank:5, finish optimizer.step profile ...
rank:5, Before memory release - Allocated: 46189316608, Reserved: 65603108864
rank:5, trace log has been written to txt...
rank:5, finish release GPU memory ...
rank:5, After memory release - Allocated: 15346516992, Reserved: 30792482816
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffc00 recvbuff 0x7f4aec9ffc00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffc00,7f4aec9ffc00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffe00 recvbuff 0x7f4aec9ffe00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffe00,7f4aec9ffe00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf200200 recvbuff 0x7f4ccf200200 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf200200,7f4ccf200200,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4cba000000 recvbuff 0x7f4cba000000 count 77266944 datatype 7 op 0 root 0 comm 0xe59efc0 [nranks=1] stream 0xa1b3c10
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4cba000000,7f4cba000000,77266944,7,0,0,0xe59efc0,0xa1b3c10)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 0): 1915047936
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1915047936 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffc00 recvbuff 0x7f4aec9ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffc00,7f4aec9ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 6, finish warm up ...
rank_id = 6, input_tensor_shapes: []
rank:6,cuda fwd time: 174.71180725097656
rank:6, fwd_subop num: 17, fwd_subop: ['trace_src_func=_reduce,duration=0.48,timestamp=4407360283.37,input__shape=[2, 2048, 12288],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.12,timestamp=4407360294.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.57,timestamp=4407360305.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4407360315.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.78,timestamp=4407360325.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.82,timestamp=4407360336.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.1,timestamp=4407360347.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.04,timestamp=4407360358.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.2,timestamp=4407360368.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.21,timestamp=4407360379.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.19,timestamp=4407360390.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.83,timestamp=4407360401.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.86,timestamp=4407360412.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.68,timestamp=4407360423.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.91,timestamp=4407360434.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.78,timestamp=4407360445.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.9,timestamp=4407360456.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 6, finish FWD profile ...
rank:6, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.45,timestamp=4407360473.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.19,timestamp=4407360497.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.59,timestamp=4407360517.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.33,timestamp=4407360541.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.62,timestamp=4407360560.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.25,timestamp=4407360584.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.83,timestamp=4407360604.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.3,timestamp=4407360628.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.92,timestamp=4407360647.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.36,timestamp=4407360671.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.2,timestamp=4407360691.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.56,timestamp=4407360716.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.06,timestamp=4407360736.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.78,timestamp=4407360760.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.75,timestamp=4407360780.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.51,timestamp=4407360804.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:6, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3f659ffc00 recvbuff 0x7f3f659ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3f659ffc00,7f3f659ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:6,optimizer_step time: 52.192256927490234
rank:6, finish optimizer.step profile ...
rank:6, Before memory release - Allocated: 46189316608, Reserved: 65603108864
rank:6, trace log has been written to txt...
rank:6, finish release GPU memory ...
rank:6, After memory release - Allocated: 15346516992, Reserved: 30792482816
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3fcc00 recvbuff 0x7f4ccf3fcc00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3fcc00,7f4ccf3fcc00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3fca00 recvbuff 0x7f4ccf3fca00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3fca00,7f4ccf3fca00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3fcc00 recvbuff 0x7f4ccf3fcc00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3fcc00,7f4ccf3fcc00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4870000000 recvbuff 0x7f4870000000 count 77266944 datatype 7 op 0 root 0 comm 0xe59efc0 [nranks=1] stream 0xa1b3c10
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4870000000,7f4870000000,77266944,7,0,0,0xe59efc0,0xa1b3c10)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 0): 1915047936
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1915047936 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf32aa00 recvbuff 0x7f4ccf32aa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf32aa00,7f4ccf32aa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 7, finish warm up ...
rank_id = 7, input_tensor_shapes: []
rank:7,cuda fwd time: 174.61351013183594
rank:7, fwd_subop num: 17, fwd_subop: ['trace_src_func=_reduce,duration=0.48,timestamp=4407362088.59,input__shape=[2, 2048, 12288],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.9,timestamp=4407362100.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.59,timestamp=4407362110.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4407362120.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.81,timestamp=4407362130.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.9,timestamp=4407362141.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.14,timestamp=4407362152.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.17,timestamp=4407362163.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.21,timestamp=4407362174.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.16,timestamp=4407362185.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.21,timestamp=4407362195.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.86,timestamp=4407362206.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.9,timestamp=4407362217.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.74,timestamp=4407362229.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.89,timestamp=4407362239.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.73,timestamp=4407362250.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.77,timestamp=4407362261.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 7, finish FWD profile ...
rank:7, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.46,timestamp=4407362278.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.31,timestamp=4407362302.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.77,timestamp=4407362322.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.43,timestamp=4407362346.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.88,timestamp=4407362366.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.32,timestamp=4407362390.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.98,timestamp=4407362409.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.33,timestamp=4407362434.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.99,timestamp=4407362453.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.3,timestamp=4407362477.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.22,timestamp=4407362497.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.75,timestamp=4407362522.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.17,timestamp=4407362542.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.68,timestamp=4407362566.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.67,timestamp=4407362585.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.6,timestamp=4407362610.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:7, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f40d31ff800 recvbuff 0x7f40d31ff800 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f40d31ff800,7f40d31ff800,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:7,optimizer_step time: 52.07347106933594
rank:7, finish optimizer.step profile ...
rank:7, Before memory release - Allocated: 46189316608, Reserved: 65603108864
rank:7, trace log has been written to txt...
rank:7, finish release GPU memory ...
rank:7, After memory release - Allocated: 15346516992, Reserved: 30792482816
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f479a9fca00 recvbuff 0x7f479a9fca00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f479a9fca00,7f479a9fca00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 8, finish warm up ...
rank_id = 8, input_tensor_shapes: [(2048, 2, 12288)]
rank:8,cuda fwd time: 171.51589965820312
rank:8, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=6.53,timestamp=4407364367.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.27,timestamp=4407364378.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.42,timestamp=4407364388.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.38,timestamp=4407364398.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4407364409.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.72,timestamp=4407364419.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.73,timestamp=4407364430.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.96,timestamp=4407364440.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4407364451.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.88,timestamp=4407364462.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4407364473.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.88,timestamp=4407364484.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.71,timestamp=4407364495.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.89,timestamp=4407364506.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.76,timestamp=4407364517.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.55,timestamp=4407364528.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 8, finish FWD profile ...
rank:8, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.59,timestamp=4407364545.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.35,timestamp=4407364569.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.86,timestamp=4407364589.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.57,timestamp=4407364613.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.41,timestamp=4407364633.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.72,timestamp=4407364658.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.4,timestamp=4407364678.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.73,timestamp=4407364703.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.73,timestamp=4407364722.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.21,timestamp=4407364746.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.02,timestamp=4407364766.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.38,timestamp=4407364790.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.99,timestamp=4407364810.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.66,timestamp=4407364834.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.99,timestamp=4407364854.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.83,timestamp=4407364878.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:8, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3fc59ffa00 recvbuff 0x7f3fc59ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3fc59ffa00,7f3fc59ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:8,optimizer_step time: 49.21446228027344
rank:8, finish optimizer.step profile ...
rank:8, Before memory release - Allocated: 44952340992, Reserved: 64395149312
rank:8, trace log has been written to txt...
rank:8, finish release GPU memory ...
rank:8, After memory release - Allocated: 30250091520, Reserved: 30784094208
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3fe400 recvbuff 0x7f4ccf3fe400 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3fe400,7f4ccf3fe400,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 9, finish warm up ...
rank_id = 9, input_tensor_shapes: [(2048, 2, 12288)]
rank:9,cuda fwd time: 171.8190155029297
rank:9, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=5.33,timestamp=4407366680.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.25,timestamp=4407366690.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.36,timestamp=4407366700.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.42,timestamp=4407366711.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.41,timestamp=4407366721.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.74,timestamp=4407366732.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.85,timestamp=4407366743.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.06,timestamp=4407366753.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.03,timestamp=4407366764.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.75,timestamp=4407366775.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4407366786.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.76,timestamp=4407366797.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.58,timestamp=4407366808.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.75,timestamp=4407366819.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4407366830.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.78,timestamp=4407366840.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 9, finish FWD profile ...
rank:9, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.33,timestamp=4407366857.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.98,timestamp=4407366882.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.17,timestamp=4407366901.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.55,timestamp=4407366926.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.64,timestamp=4407366945.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.57,timestamp=4407366970.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.87,timestamp=4407366989.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.33,timestamp=4407367013.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.05,timestamp=4407367033.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.54,timestamp=4407367057.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.23,timestamp=4407367077.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.61,timestamp=4407367102.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.18,timestamp=4407367121.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.54,timestamp=4407367146.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.72,timestamp=4407367165.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.57,timestamp=4407367190.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:9, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3c239ffa00 recvbuff 0x7f3c239ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3c239ffa00,7f3c239ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:9,optimizer_step time: 49.256446838378906
rank:9, finish optimizer.step profile ...
rank:9, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:9, trace log has been written to txt...
rank:9, finish release GPU memory ...
rank:9, After memory release - Allocated: 30250091520, Reserved: 45311066112
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3fe400 recvbuff 0x7f4ccf3fe400 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3fe400,7f4ccf3fe400,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 10, finish warm up ...
rank_id = 10, input_tensor_shapes: [(2048, 2, 12288)]
rank:10,cuda fwd time: 173.05906677246094
rank:10, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.21,timestamp=4407368970.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407368980.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4407368991.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.66,timestamp=4407369001.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.7,timestamp=4407369011.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.7,timestamp=4407369022.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.91,timestamp=4407369033.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.05,timestamp=4407369044.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4407369055.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.5,timestamp=4407369066.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.72,timestamp=4407369077.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.91,timestamp=4407369088.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.7,timestamp=4407369099.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.99,timestamp=4407369110.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.92,timestamp=4407369122.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.96,timestamp=4407369132.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 10, finish FWD profile ...
rank:10, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.38,timestamp=4407369149.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.29,timestamp=4407369174.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.81,timestamp=4407369193.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.44,timestamp=4407369218.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.83,timestamp=4407369237.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.23,timestamp=4407369261.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.72,timestamp=4407369281.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.09,timestamp=4407369305.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.77,timestamp=4407369324.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.12,timestamp=4407369348.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.7,timestamp=4407369367.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.45,timestamp=4407369392.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.17,timestamp=4407369412.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.59,timestamp=4407369436.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.76,timestamp=4407369456.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.51,timestamp=4407369480.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:10, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3f8400 recvbuff 0x7f4ccf3f8400 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3f8400,7f4ccf3f8400,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:10,optimizer_step time: 49.3834228515625
rank:10, finish optimizer.step profile ...
rank:10, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:10, trace log has been written to txt...
rank:10, finish release GPU memory ...
rank:10, After memory release - Allocated: 30250091520, Reserved: 45311066112
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 11, finish warm up ...
rank_id = 11, input_tensor_shapes: [(2048, 2, 12288)]
rank:11,cuda fwd time: 171.5845184326172
rank:11, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.14,timestamp=4407371143.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.46,timestamp=4407371153.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.24,timestamp=4407371164.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.59,timestamp=4407371174.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4407371184.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.85,timestamp=4407371195.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.89,timestamp=4407371205.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.95,timestamp=4407371216.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.98,timestamp=4407371227.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.58,timestamp=4407371238.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.77,timestamp=4407371249.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4407371260.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.94,timestamp=4407371271.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.89,timestamp=4407371282.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.64,timestamp=4407371293.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.79,timestamp=4407371304.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 11, finish FWD profile ...
rank:11, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.42,timestamp=4407371321.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.95,timestamp=4407371345.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.13,timestamp=4407371365.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.46,timestamp=4407371389.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.96,timestamp=4407371409.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.22,timestamp=4407371433.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.76,timestamp=4407371452.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.32,timestamp=4407371476.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.21,timestamp=4407371496.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.53,timestamp=4407371521.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.16,timestamp=4407371540.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.62,timestamp=4407371565.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.84,timestamp=4407371584.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.53,timestamp=4407371609.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.64,timestamp=4407371628.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.42,timestamp=4407371652.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:11, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:11,optimizer_step time: 49.293312072753906
rank:11, finish optimizer.step profile ...
rank:11, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:11, trace log has been written to txt...
rank:11, finish release GPU memory ...
rank:11, After memory release - Allocated: 30250091520, Reserved: 45512392704
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 1): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 12, finish warm up ...
rank_id = 12, input_tensor_shapes: [(2048, 2, 12288)]
rank:12,cuda fwd time: 168.00767517089844
rank:12, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=6.94,timestamp=4407373383.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.45,timestamp=4407373394.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4407373404.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.55,timestamp=4407373414.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.18,timestamp=4407373425.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.63,timestamp=4407373435.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.63,timestamp=4407373445.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.7,timestamp=4407373456.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.52,timestamp=4407373466.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.39,timestamp=4407373476.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4407373487.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.45,timestamp=4407373498.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4407373509.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.44,timestamp=4407373519.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4407373530.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.46,timestamp=4407373540.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 12, finish FWD profile ...
rank:12, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.92,timestamp=4407373556.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.27,timestamp=4407373580.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.53,timestamp=4407373599.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.26,timestamp=4407373624.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.86,timestamp=4407373643.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.27,timestamp=4407373667.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.07,timestamp=4407373687.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.6,timestamp=4407373711.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.28,timestamp=4407373731.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.79,timestamp=4407373756.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.7,timestamp=4407373775.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.67,timestamp=4407373800.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.81,timestamp=4407373819.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.33,timestamp=4407373843.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.83,timestamp=4407373863.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.3,timestamp=4407373887.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:12, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:12,optimizer_step time: 49.22265625
rank:12, finish optimizer.step profile ...
rank:12, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:12, trace log has been written to txt...
rank:12, finish release GPU memory ...
rank:12, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 1): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3fe400 recvbuff 0x7f4ccf3fe400 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3fe400,7f4ccf3fe400,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 13, finish warm up ...
rank_id = 13, input_tensor_shapes: [(2048, 2, 12288)]
rank:13,cuda fwd time: 171.9736328125
rank:13, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.18,timestamp=4407375561.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.5,timestamp=4407375571.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4407375582.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.65,timestamp=4407375592.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.63,timestamp=4407375602.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.01,timestamp=4407375613.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.92,timestamp=4407375624.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.25,timestamp=4407375635.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.17,timestamp=4407375646.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.83,timestamp=4407375656.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.72,timestamp=4407375668.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.77,timestamp=4407375678.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4407375690.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.78,timestamp=4407375700.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4407375712.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.75,timestamp=4407375722.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 13, finish FWD profile ...
rank:13, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.66,timestamp=4407375739.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.95,timestamp=4407375763.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.34,timestamp=4407375783.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.88,timestamp=4407375808.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.37,timestamp=4407375828.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.29,timestamp=4407375852.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.86,timestamp=4407375871.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.34,timestamp=4407375896.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.94,timestamp=4407375915.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.39,timestamp=4407375939.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.35,timestamp=4407375959.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.68,timestamp=4407375984.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.16,timestamp=4407376004.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.66,timestamp=4407376028.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.8,timestamp=4407376048.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.25,timestamp=4407376072.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:13, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a7984a00 recvbuff 0x7f44a7984a00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a7984a00,7f44a7984a00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:13,optimizer_step time: 49.27385711669922
rank:13, finish optimizer.step profile ...
rank:13, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:13, trace log has been written to txt...
rank:13, finish release GPU memory ...
rank:13, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 1): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3fe400 recvbuff 0x7f4ccf3fe400 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3fe400,7f4ccf3fe400,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 14, finish warm up ...
rank_id = 14, input_tensor_shapes: [(2048, 2, 12288)]
rank:14,cuda fwd time: 167.70150756835938
rank:14, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.02,timestamp=4407377748.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.42,timestamp=4407377758.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.41,timestamp=4407377769.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.53,timestamp=4407377779.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.11,timestamp=4407377789.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.64,timestamp=4407377799.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.69,timestamp=4407377810.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.75,timestamp=4407377820.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.72,timestamp=4407377831.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.44,timestamp=4407377841.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4407377852.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.45,timestamp=4407377862.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4407377873.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407377884.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4407377895.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.56,timestamp=4407377905.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 14, finish FWD profile ...
rank:14, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.93,timestamp=4407377922.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.74,timestamp=4407377946.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.92,timestamp=4407377966.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.18,timestamp=4407377990.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.29,timestamp=4407378010.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.52,timestamp=4407378035.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.31,timestamp=4407378055.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.53,timestamp=4407378079.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.01,timestamp=4407378099.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.57,timestamp=4407378123.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.86,timestamp=4407378143.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.46,timestamp=4407378167.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.8,timestamp=4407378187.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.19,timestamp=4407378211.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.81,timestamp=4407378230.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.4,timestamp=4407378254.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:14, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3c4b9ffa00 recvbuff 0x7f3c4b9ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3c4b9ffa00,7f3c4b9ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:14,optimizer_step time: 49.20217514038086
rank:14, finish optimizer.step profile ...
rank:14, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:14, trace log has been written to txt...
rank:14, finish release GPU memory ...
rank:14, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 1): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a79fe200 recvbuff 0x7f44a79fe200 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a79fe200,7f44a79fe200,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 15, finish warm up ...
rank_id = 15, input_tensor_shapes: [(2048, 2, 12288)]
rank:15,cuda fwd time: 169.00096130371094
rank:15, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.22,timestamp=4407379919.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407379929.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4407379939.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.58,timestamp=4407379949.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4407379960.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.76,timestamp=4407379970.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.85,timestamp=4407379981.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.89,timestamp=4407379991.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.87,timestamp=4407380002.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.73,timestamp=4407380013.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4407380024.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.51,timestamp=4407380034.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4407380045.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.52,timestamp=4407380056.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4407380067.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.62,timestamp=4407380077.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 15, finish FWD profile ...
rank:15, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=8.47,timestamp=4407380093.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.98,timestamp=4407380118.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.98,timestamp=4407380138.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.34,timestamp=4407380162.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.11,timestamp=4407380182.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.5,timestamp=4407380206.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.3,timestamp=4407380226.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.7,timestamp=4407380251.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.91,timestamp=4407380270.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.56,timestamp=4407380295.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.7,timestamp=4407380314.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.19,timestamp=4407380338.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.72,timestamp=4407380357.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.48,timestamp=4407380382.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.99,timestamp=4407380401.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.32,timestamp=4407380425.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:15, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a794a200 recvbuff 0x7f44a794a200 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a794a200,7f44a794a200,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:15,optimizer_step time: 49.302528381347656
rank:15, finish optimizer.step profile ...
rank:15, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:15, trace log has been written to txt...
rank:15, finish release GPU memory ...
rank:15, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a79fe200 recvbuff 0x7f44a79fe200 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a79fe200,7f44a79fe200,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 16, finish warm up ...
rank_id = 16, input_tensor_shapes: [(2048, 2, 12288)]
rank:16,cuda fwd time: 169.997314453125
rank:16, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.26,timestamp=4407382099.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407382109.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.41,timestamp=4407382119.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.62,timestamp=4407382130.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4407382140.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.65,timestamp=4407382150.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4407382161.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.88,timestamp=4407382171.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.87,timestamp=4407382182.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.64,timestamp=4407382193.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.51,timestamp=4407382204.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.76,timestamp=4407382214.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4407382226.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.8,timestamp=4407382236.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4407382247.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.81,timestamp=4407382258.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 16, finish FWD profile ...
rank:16, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.67,timestamp=4407382275.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.97,timestamp=4407382299.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.17,timestamp=4407382319.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.59,timestamp=4407382343.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.04,timestamp=4407382363.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.58,timestamp=4407382387.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.54,timestamp=4407382407.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.45,timestamp=4407382431.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.78,timestamp=4407382450.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.43,timestamp=4407382475.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.0,timestamp=4407382494.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.39,timestamp=4407382519.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.1,timestamp=4407382538.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.46,timestamp=4407382563.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.12,timestamp=4407382582.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.6,timestamp=4407382607.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:16, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a79c0a00 recvbuff 0x7f44a79c0a00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a79c0a00,7f44a79c0a00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:16,optimizer_step time: 49.261566162109375
rank:16, finish optimizer.step profile ...
rank:16, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:16, trace log has been written to txt...
rank:16, finish release GPU memory ...
rank:16, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3fe400 recvbuff 0x7f4ccf3fe400 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3fe400,7f4ccf3fe400,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 17, finish warm up ...
rank_id = 17, input_tensor_shapes: [(2048, 2, 12288)]
rank:17,cuda fwd time: 170.99571228027344
rank:17, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.21,timestamp=4407384312.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407384322.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4407384332.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.61,timestamp=4407384342.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.6,timestamp=4407384353.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.87,timestamp=4407384363.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.8,timestamp=4407384374.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.06,timestamp=4407384385.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.07,timestamp=4407384396.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.89,timestamp=4407384406.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.71,timestamp=4407384418.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.76,timestamp=4407384428.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.53,timestamp=4407384439.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.77,timestamp=4407384450.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.61,timestamp=4407384461.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.78,timestamp=4407384472.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 17, finish FWD profile ...
rank:17, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.35,timestamp=4407384489.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.97,timestamp=4407384513.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.22,timestamp=4407384533.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.33,timestamp=4407384557.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.05,timestamp=4407384577.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.31,timestamp=4407384601.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.81,timestamp=4407384620.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.3,timestamp=4407384645.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.0,timestamp=4407384664.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.32,timestamp=4407384688.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.92,timestamp=4407384708.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.43,timestamp=4407384732.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.15,timestamp=4407384752.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.58,timestamp=4407384777.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.9,timestamp=4407384796.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.54,timestamp=4407384820.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:17, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a790fa00 recvbuff 0x7f44a790fa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a790fa00,7f44a790fa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:17,optimizer_step time: 49.13663864135742
rank:17, finish optimizer.step profile ...
rank:17, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:17, trace log has been written to txt...
rank:17, finish release GPU memory ...
rank:17, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 2): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3fe400 recvbuff 0x7f4ccf3fe400 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3fe400,7f4ccf3fe400,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 18, finish warm up ...
rank_id = 18, input_tensor_shapes: [(2048, 2, 12288)]
rank:18,cuda fwd time: 170.5902099609375
rank:18, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.18,timestamp=4407386527.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407386537.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4407386547.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.55,timestamp=4407386558.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.81,timestamp=4407386568.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.87,timestamp=4407386579.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.98,timestamp=4407386590.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.11,timestamp=4407386600.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.05,timestamp=4407386611.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.74,timestamp=4407386622.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.64,timestamp=4407386633.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.74,timestamp=4407386644.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4407386655.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.62,timestamp=4407386665.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4407386676.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.49,timestamp=4407386687.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 18, finish FWD profile ...
rank:18, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.35,timestamp=4407386703.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.16,timestamp=4407386728.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.12,timestamp=4407386747.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.44,timestamp=4407386772.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.68,timestamp=4407386791.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.16,timestamp=4407386815.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.9,timestamp=4407386835.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.41,timestamp=4407386859.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.12,timestamp=4407386879.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.61,timestamp=4407386903.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.86,timestamp=4407386923.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.56,timestamp=4407386947.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.75,timestamp=4407386966.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.35,timestamp=4407386991.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.8,timestamp=4407387010.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.34,timestamp=4407387034.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:18, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a7986200 recvbuff 0x7f44a7986200 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a7986200,7f44a7986200,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:18,optimizer_step time: 49.303550720214844
rank:18, finish optimizer.step profile ...
rank:18, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:18, trace log has been written to txt...
rank:18, finish release GPU memory ...
rank:18, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 2): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3fe400 recvbuff 0x7f4ccf3fe400 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3fe400,7f4ccf3fe400,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 19, finish warm up ...
rank_id = 19, input_tensor_shapes: [(2048, 2, 12288)]
rank:19,cuda fwd time: 168.7889862060547
rank:19, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.16,timestamp=4407388722.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.44,timestamp=4407388732.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.28,timestamp=4407388742.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.58,timestamp=4407388753.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4407388763.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.72,timestamp=4407388773.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4407388784.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.76,timestamp=4407388794.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.33,timestamp=4407388805.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.5,timestamp=4407388815.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.44,timestamp=4407388826.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.51,timestamp=4407388837.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4407388848.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.66,timestamp=4407388858.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.53,timestamp=4407388869.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.77,timestamp=4407388880.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 19, finish FWD profile ...
rank:19, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.33,timestamp=4407388897.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.03,timestamp=4407388921.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.14,timestamp=4407388941.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.27,timestamp=4407388965.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.99,timestamp=4407388985.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.16,timestamp=4407389009.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.07,timestamp=4407389028.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.81,timestamp=4407389053.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.78,timestamp=4407389072.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.44,timestamp=4407389097.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.8,timestamp=4407389116.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.19,timestamp=4407389140.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.78,timestamp=4407389160.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.25,timestamp=4407389184.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.1,timestamp=4407389203.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.49,timestamp=4407389228.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:19, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3c4b9ffa00 recvbuff 0x7f3c4b9ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3c4b9ffa00,7f3c4b9ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:19,optimizer_step time: 49.26054382324219
rank:19, finish optimizer.step profile ...
rank:19, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:19, trace log has been written to txt...
rank:19, finish release GPU memory ...
rank:19, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 2): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3fe400 recvbuff 0x7f4ccf3fe400 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3fe400,7f4ccf3fe400,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 20, finish warm up ...
rank_id = 20, input_tensor_shapes: [(2048, 2, 12288)]
rank:20,cuda fwd time: 169.9000244140625
rank:20, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.04,timestamp=4407390903.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407390913.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4407390924.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.57,timestamp=4407390934.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.71,timestamp=4407390945.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.76,timestamp=4407390955.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.61,timestamp=4407390966.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.83,timestamp=4407390976.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.81,timestamp=4407390987.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.52,timestamp=4407390997.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4407391008.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.53,timestamp=4407391019.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.27,timestamp=4407391030.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.5,timestamp=4407391041.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.28,timestamp=4407391052.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.49,timestamp=4407391062.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 20, finish FWD profile ...
rank:20, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.78,timestamp=4407391079.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.82,timestamp=4407391103.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.89,timestamp=4407391123.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.25,timestamp=4407391147.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.0,timestamp=4407391167.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.29,timestamp=4407391191.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.02,timestamp=4407391210.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.23,timestamp=4407391235.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.76,timestamp=4407391254.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.43,timestamp=4407391278.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.82,timestamp=4407391298.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.72,timestamp=4407391322.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.75,timestamp=4407391342.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.59,timestamp=4407391366.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.8,timestamp=4407391386.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.14,timestamp=4407391410.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:20, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3c4b9ffa00 recvbuff 0x7f3c4b9ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3c4b9ffa00,7f3c4b9ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:20,optimizer_step time: 49.29228973388672
rank:20, finish optimizer.step profile ...
rank:20, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:20, trace log has been written to txt...
rank:20, finish release GPU memory ...
rank:20, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 2): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a7899200 recvbuff 0x7f44a7899200 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a7899200,7f44a7899200,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 21, finish warm up ...
rank_id = 21, input_tensor_shapes: [(2048, 2, 12288)]
rank:21,cuda fwd time: 191.12754821777344
rank:21, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.22,timestamp=4407393076.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407393086.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4407393097.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.6,timestamp=4407393107.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.6,timestamp=4407393117.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.72,timestamp=4407393128.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.77,timestamp=4407393138.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.78,timestamp=4407393149.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.81,timestamp=4407393159.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407393170.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4407393181.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.44,timestamp=4407393191.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4407393202.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.43,timestamp=4407393212.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.39,timestamp=4407393235.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.53,timestamp=4407393245.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 21, finish FWD profile ...
rank:21, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.83,timestamp=4407393273.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=20.46,timestamp=4407393296.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.4,timestamp=4407393315.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.14,timestamp=4407393339.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.16,timestamp=4407393359.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.67,timestamp=4407393384.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.1,timestamp=4407393403.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.6,timestamp=4407393428.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.66,timestamp=4407393447.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.5,timestamp=4407393472.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.82,timestamp=4407393491.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.42,timestamp=4407393515.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.88,timestamp=4407393535.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.34,timestamp=4407393559.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.96,timestamp=4407393578.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.35,timestamp=4407393603.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:21, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3c4b9ffa00 recvbuff 0x7f3c4b9ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3c4b9ffa00,7f3c4b9ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:21,optimizer_step time: 49.3230094909668
rank:21, finish optimizer.step profile ...
rank:21, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:21, trace log has been written to txt...
rank:21, finish release GPU memory ...
rank:21, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 2): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a790fa00 recvbuff 0x7f44a790fa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a790fa00,7f44a790fa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 22, finish warm up ...
rank_id = 22, input_tensor_shapes: [(2048, 2, 12288)]
rank:22,cuda fwd time: 171.29881286621094
rank:22, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.25,timestamp=4407395456.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407395466.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4407395477.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.66,timestamp=4407395487.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4407395497.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.98,timestamp=4407395508.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.87,timestamp=4407395519.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.07,timestamp=4407395529.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.02,timestamp=4407395540.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.76,timestamp=4407395551.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.64,timestamp=4407395562.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.76,timestamp=4407395573.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4407395584.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.76,timestamp=4407395595.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.64,timestamp=4407395606.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.78,timestamp=4407395617.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 22, finish FWD profile ...
rank:22, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.27,timestamp=4407395633.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.02,timestamp=4407395658.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.2,timestamp=4407395678.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.58,timestamp=4407395702.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.13,timestamp=4407395722.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.45,timestamp=4407395746.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.06,timestamp=4407395766.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.44,timestamp=4407395790.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.15,timestamp=4407395810.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.54,timestamp=4407395834.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.15,timestamp=4407395854.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.74,timestamp=4407395879.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.97,timestamp=4407395898.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.79,timestamp=4407395923.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.77,timestamp=4407395942.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.6,timestamp=4407395967.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:22, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a790fa00 recvbuff 0x7f44a790fa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a790fa00,7f44a790fa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:22,optimizer_step time: 49.265663146972656
rank:22, finish optimizer.step profile ...
rank:22, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:22, trace log has been written to txt...
rank:22, finish release GPU memory ...
rank:22, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 2): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3fe400 recvbuff 0x7f4ccf3fe400 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3fe400,7f4ccf3fe400,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 23, finish warm up ...
rank_id = 23, input_tensor_shapes: [(2048, 2, 12288)]
rank:23,cuda fwd time: 167.86227416992188
rank:23, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=6.99,timestamp=4407397617.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.46,timestamp=4407397627.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4407397637.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.52,timestamp=4407397647.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.6,timestamp=4407397658.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.68,timestamp=4407397668.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.77,timestamp=4407397679.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.76,timestamp=4407397689.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.75,timestamp=4407397700.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.45,timestamp=4407397710.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4407397721.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.45,timestamp=4407397731.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.22,timestamp=4407397742.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4407397753.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4407397764.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.44,timestamp=4407397774.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 23, finish FWD profile ...
rank:23, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.73,timestamp=4407397790.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.94,timestamp=4407397814.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.09,timestamp=4407397834.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.44,timestamp=4407397859.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.38,timestamp=4407397879.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.69,timestamp=4407397903.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.12,timestamp=4407397923.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.51,timestamp=4407397947.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.7,timestamp=4407397967.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.74,timestamp=4407397991.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.78,timestamp=4407398011.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.42,timestamp=4407398035.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.84,timestamp=4407398054.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.4,timestamp=4407398079.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.0,timestamp=4407398098.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.37,timestamp=4407398123.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:23, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3fe400 recvbuff 0x7f4ccf3fe400 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3fe400,7f4ccf3fe400,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:23,optimizer_step time: 49.23494338989258
rank:23, finish optimizer.step profile ...
rank:23, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:23, trace log has been written to txt...
rank:23, finish release GPU memory ...
rank:23, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec891a00 recvbuff 0x7f4aec891a00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec891a00,7f4aec891a00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 24, finish warm up ...
rank_id = 24, input_tensor_shapes: [(2048, 2, 12288)]
rank:24,cuda fwd time: 170.16114807128906
rank:24, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=6.96,timestamp=4407399908.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.41,timestamp=4407399918.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.09,timestamp=4407399929.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407399939.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.99,timestamp=4407399949.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.55,timestamp=4407399960.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4407399970.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.81,timestamp=4407399981.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.7,timestamp=4407399992.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.54,timestamp=4407400002.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.14,timestamp=4407400014.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.42,timestamp=4407400025.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.86,timestamp=4407400036.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.38,timestamp=4407400046.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.04,timestamp=4407400057.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.38,timestamp=4407400068.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 24, finish FWD profile ...
rank:24, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.49,timestamp=4407400084.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.39,timestamp=4407400119.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.14,timestamp=4407400139.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.45,timestamp=4407400163.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.82,timestamp=4407400183.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.35,timestamp=4407400207.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.64,timestamp=4407400226.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.2,timestamp=4407400250.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.8,timestamp=4407400270.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.29,timestamp=4407400294.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.94,timestamp=4407400314.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.37,timestamp=4407400338.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.16,timestamp=4407400358.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.58,timestamp=4407400382.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.84,timestamp=4407400402.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.39,timestamp=4407400426.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:24, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec891a00 recvbuff 0x7f4aec891a00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec891a00,7f4aec891a00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:24,optimizer_step time: 49.24620819091797
rank:24, finish optimizer.step profile ...
rank:24, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:24, trace log has been written to txt...
rank:24, finish release GPU memory ...
rank:24, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 25, finish warm up ...
rank_id = 25, input_tensor_shapes: [(2048, 2, 12288)]
rank:25,cuda fwd time: 173.08364868164062
rank:25, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.1,timestamp=4407402107.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407402117.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4407402127.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.65,timestamp=4407402137.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4407402148.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.99,timestamp=4407402158.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.71,timestamp=4407402169.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.08,timestamp=4407402180.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.08,timestamp=4407402191.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.9,timestamp=4407402202.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.54,timestamp=4407402213.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.53,timestamp=4407402224.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.36,timestamp=4407402236.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.76,timestamp=4407402247.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.68,timestamp=4407402258.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.73,timestamp=4407402269.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 25, finish FWD profile ...
rank:25, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.15,timestamp=4407402285.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.08,timestamp=4407402310.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.37,timestamp=4407402330.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.68,timestamp=4407402355.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.52,timestamp=4407402375.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.56,timestamp=4407402399.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.75,timestamp=4407402419.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.21,timestamp=4407402443.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.95,timestamp=4407402462.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.4,timestamp=4407402487.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.16,timestamp=4407402506.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.61,timestamp=4407402531.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.09,timestamp=4407402551.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.62,timestamp=4407402575.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.67,timestamp=4407402594.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.41,timestamp=4407402619.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:25, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:25,optimizer_step time: 49.311744689941406
rank:25, finish optimizer.step profile ...
rank:25, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:25, trace log has been written to txt...
rank:25, finish release GPU memory ...
rank:25, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 3): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 26, finish warm up ...
rank_id = 26, input_tensor_shapes: [(2048, 2, 12288)]
rank:26,cuda fwd time: 169.41465759277344
rank:26, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=5.08,timestamp=4407404274.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407404284.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4407404294.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.59,timestamp=4407404304.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4407404315.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.7,timestamp=4407404325.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.75,timestamp=4407404336.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.76,timestamp=4407404346.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.74,timestamp=4407404357.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.45,timestamp=4407404367.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4407404378.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.45,timestamp=4407404388.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.24,timestamp=4407404399.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.64,timestamp=4407404411.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4407404422.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.51,timestamp=4407404432.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 26, finish FWD profile ...
rank:26, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.16,timestamp=4407404449.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.94,timestamp=4407404473.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.11,timestamp=4407404493.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.5,timestamp=4407404517.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.94,timestamp=4407404537.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.42,timestamp=4407404561.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.45,timestamp=4407404581.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.88,timestamp=4407404606.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.17,timestamp=4407404626.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.6,timestamp=4407404650.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.8,timestamp=4407404670.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.26,timestamp=4407404694.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.84,timestamp=4407404713.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.27,timestamp=4407404737.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.99,timestamp=4407404757.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.14,timestamp=4407404781.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:26, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:26,optimizer_step time: 49.21958541870117
rank:26, finish optimizer.step profile ...
rank:26, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:26, trace log has been written to txt...
rank:26, finish release GPU memory ...
rank:26, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 3): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 27, finish warm up ...
rank_id = 27, input_tensor_shapes: [(2048, 2, 12288)]
rank:27,cuda fwd time: 171.0847930908203
rank:27, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.13,timestamp=4407406516.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.46,timestamp=4407406526.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4407406536.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.57,timestamp=4407406546.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.62,timestamp=4407406557.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.86,timestamp=4407406567.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.84,timestamp=4407406578.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.86,timestamp=4407406588.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.85,timestamp=4407406599.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.55,timestamp=4407406610.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4407406621.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.53,timestamp=4407406631.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.03,timestamp=4407406643.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.67,timestamp=4407406654.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4407406666.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.78,timestamp=4407406676.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 27, finish FWD profile ...
rank:27, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.22,timestamp=4407406693.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.09,timestamp=4407406717.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.6,timestamp=4407406737.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.19,timestamp=4407406761.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.62,timestamp=4407406780.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.93,timestamp=4407406804.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.65,timestamp=4407406823.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.93,timestamp=4407406847.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.83,timestamp=4407406866.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.55,timestamp=4407406891.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.15,timestamp=4407406910.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.67,timestamp=4407406935.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.18,timestamp=4407406955.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.79,timestamp=4407406980.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.88,timestamp=4407406999.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.75,timestamp=4407407024.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:27, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:27,optimizer_step time: 49.19705581665039
rank:27, finish optimizer.step profile ...
rank:27, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:27, trace log has been written to txt...
rank:27, finish release GPU memory ...
rank:27, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 3): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 28, finish warm up ...
rank_id = 28, input_tensor_shapes: [(2048, 2, 12288)]
rank:28,cuda fwd time: 168.30361938476562
rank:28, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=3.65,timestamp=4407408699.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407408709.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.1,timestamp=4407408719.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.59,timestamp=4407408729.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.53,timestamp=4407408740.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.66,timestamp=4407408750.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.6,timestamp=4407408760.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.75,timestamp=4407408771.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.77,timestamp=4407408781.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.55,timestamp=4407408792.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4407408803.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.54,timestamp=4407408813.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4407408824.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.54,timestamp=4407408835.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4407408846.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.54,timestamp=4407408856.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 28, finish FWD profile ...
rank:28, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.98,timestamp=4407408873.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.98,timestamp=4407408897.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.15,timestamp=4407408917.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.49,timestamp=4407408941.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.93,timestamp=4407408961.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.49,timestamp=4407408985.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.85,timestamp=4407409005.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.79,timestamp=4407409029.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.83,timestamp=4407409049.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.69,timestamp=4407409073.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.04,timestamp=4407409093.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.22,timestamp=4407409117.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.77,timestamp=4407409136.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.36,timestamp=4407409161.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.17,timestamp=4407409180.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.64,timestamp=4407409205.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:28, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:28,optimizer_step time: 49.185791015625
rank:28, finish optimizer.step profile ...
rank:28, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:28, trace log has been written to txt...
rank:28, finish release GPU memory ...
rank:28, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 3): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f479a8f9200 recvbuff 0x7f479a8f9200 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f479a8f9200,7f479a8f9200,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 29, finish warm up ...
rank_id = 29, input_tensor_shapes: [(2048, 2, 12288)]
rank:29,cuda fwd time: 168.0773162841797
rank:29, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.18,timestamp=4407410862.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407410873.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4407410883.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.56,timestamp=4407410893.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.6,timestamp=4407410904.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.79,timestamp=4407410914.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.79,timestamp=4407410925.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.86,timestamp=4407410935.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.79,timestamp=4407410946.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.53,timestamp=4407410956.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4407410967.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.44,timestamp=4407410977.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.27,timestamp=4407410988.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.45,timestamp=4407410999.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4407411009.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.25,timestamp=4407411020.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 29, finish FWD profile ...
rank:29, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.78,timestamp=4407411036.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.35,timestamp=4407411060.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.51,timestamp=4407411079.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.86,timestamp=4407411103.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.81,timestamp=4407411122.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.46,timestamp=4407411147.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.2,timestamp=4407411166.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.63,timestamp=4407411191.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.21,timestamp=4407411211.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.76,timestamp=4407411236.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.89,timestamp=4407411255.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.53,timestamp=4407411279.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.79,timestamp=4407411299.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.4,timestamp=4407411323.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.88,timestamp=4407411343.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.37,timestamp=4407411367.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:29, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3c4b9ffa00 recvbuff 0x7f3c4b9ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3c4b9ffa00,7f3c4b9ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:29,optimizer_step time: 49.277950286865234
rank:29, finish optimizer.step profile ...
rank:29, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:29, trace log has been written to txt...
rank:29, finish release GPU memory ...
rank:29, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 3): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec97ea00 recvbuff 0x7f4aec97ea00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec97ea00,7f4aec97ea00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 30, finish warm up ...
rank_id = 30, input_tensor_shapes: [(2048, 2, 12288)]
rank:30,cuda fwd time: 169.1084747314453
rank:30, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.22,timestamp=4407413078.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.46,timestamp=4407413088.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4407413099.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.59,timestamp=4407413109.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.52,timestamp=4407413119.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.71,timestamp=4407413130.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.73,timestamp=4407413140.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.84,timestamp=4407413151.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.81,timestamp=4407413161.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.54,timestamp=4407413172.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4407413183.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.52,timestamp=4407413193.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.44,timestamp=4407413204.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407413215.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4407413226.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.75,timestamp=4407413237.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 30, finish FWD profile ...
rank:30, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.06,timestamp=4407413253.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.96,timestamp=4407413278.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.12,timestamp=4407413297.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.51,timestamp=4407413322.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.26,timestamp=4407413342.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.67,timestamp=4407413366.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.64,timestamp=4407413385.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.42,timestamp=4407413410.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.71,timestamp=4407413429.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.47,timestamp=4407413453.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.0,timestamp=4407413473.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.32,timestamp=4407413497.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.9,timestamp=4407413517.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.35,timestamp=4407413541.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.15,timestamp=4407413561.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.72,timestamp=4407413585.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:30, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3c4b9ffa00 recvbuff 0x7f3c4b9ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3c4b9ffa00,7f3c4b9ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:30,optimizer_step time: 49.12742233276367
rank:30, finish optimizer.step profile ...
rank:30, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:30, trace log has been written to txt...
rank:30, finish release GPU memory ...
rank:30, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 3): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a7950200 recvbuff 0x7f44a7950200 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a7950200,7f44a7950200,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 31, finish warm up ...
rank_id = 31, input_tensor_shapes: [(2048, 2, 12288)]
rank:31,cuda fwd time: 171.6090850830078
rank:31, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=6.95,timestamp=4407415371.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.43,timestamp=4407415381.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.09,timestamp=4407415391.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.6,timestamp=4407415401.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4407415412.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.79,timestamp=4407415422.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.61,timestamp=4407415433.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.03,timestamp=4407415444.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.82,timestamp=4407415455.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.8,timestamp=4407415466.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4407415477.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.7,timestamp=4407415488.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4407415499.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.72,timestamp=4407415510.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.19,timestamp=4407415521.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.7,timestamp=4407415532.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 31, finish FWD profile ...
rank:31, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.4,timestamp=4407415548.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.91,timestamp=4407415573.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.86,timestamp=4407415592.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.45,timestamp=4407415617.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.6,timestamp=4407415636.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.38,timestamp=4407415660.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.81,timestamp=4407415680.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.31,timestamp=4407415704.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.96,timestamp=4407415724.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.29,timestamp=4407415748.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.98,timestamp=4407415767.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.39,timestamp=4407415792.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.1,timestamp=4407415811.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.71,timestamp=4407415836.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.17,timestamp=4407415856.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.62,timestamp=4407415880.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:31, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3c4b9ffa00 recvbuff 0x7f3c4b9ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3c4b9ffa00,7f3c4b9ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:31,optimizer_step time: 49.43667221069336
rank:31, finish optimizer.step profile ...
rank:31, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:31, trace log has been written to txt...
rank:31, finish release GPU memory ...
rank:31, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9f9a00 recvbuff 0x7f4aec9f9a00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9f9a00,7f4aec9f9a00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 32, finish warm up ...
rank_id = 32, input_tensor_shapes: [(2048, 2, 12288)]
rank:32,cuda fwd time: 171.58758544921875
rank:32, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.19,timestamp=4407417811.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.45,timestamp=4407417821.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.44,timestamp=4407417831.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.61,timestamp=4407417841.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.78,timestamp=4407417852.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.86,timestamp=4407417863.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.01,timestamp=4407417873.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.07,timestamp=4407417884.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.04,timestamp=4407417895.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.78,timestamp=4407417906.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.61,timestamp=4407417917.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.78,timestamp=4407417927.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.63,timestamp=4407417939.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.79,timestamp=4407417949.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.63,timestamp=4407417960.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.76,timestamp=4407417971.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 32, finish FWD profile ...
rank:32, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.97,timestamp=4407417988.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.15,timestamp=4407418013.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.77,timestamp=4407418032.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.3,timestamp=4407418056.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.03,timestamp=4407418076.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.41,timestamp=4407418100.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.01,timestamp=4407418120.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.11,timestamp=4407418144.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.82,timestamp=4407418163.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.1,timestamp=4407418187.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.0,timestamp=4407418207.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.64,timestamp=4407418231.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.68,timestamp=4407418251.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.55,timestamp=4407418275.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.8,timestamp=4407418294.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.38,timestamp=4407418319.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:32, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3c4b9ffa00 recvbuff 0x7f3c4b9ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3c4b9ffa00,7f3c4b9ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:32,optimizer_step time: 49.263614654541016
rank:32, finish optimizer.step profile ...
rank:32, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:32, trace log has been written to txt...
rank:32, finish release GPU memory ...
rank:32, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f479a8c3200 recvbuff 0x7f479a8c3200 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f479a8c3200,7f479a8c3200,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 33, finish warm up ...
rank_id = 33, input_tensor_shapes: [(2048, 2, 12288)]
rank:33,cuda fwd time: 168.03021240234375
rank:33, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.15,timestamp=4407419965.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407419975.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4407419986.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.6,timestamp=4407419996.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.53,timestamp=4407420006.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.67,timestamp=4407420016.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.61,timestamp=4407420027.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.7,timestamp=4407420038.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.56,timestamp=4407420048.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.73,timestamp=4407420058.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4407420069.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407420080.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.13,timestamp=4407420091.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.45,timestamp=4407420101.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4407420112.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.44,timestamp=4407420122.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 33, finish FWD profile ...
rank:33, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.3,timestamp=4407420139.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.9,timestamp=4407420163.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.14,timestamp=4407420183.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.46,timestamp=4407420207.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.17,timestamp=4407420227.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.5,timestamp=4407420252.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.15,timestamp=4407420271.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.58,timestamp=4407420296.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.94,timestamp=4407420315.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.58,timestamp=4407420340.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.64,timestamp=4407420359.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.41,timestamp=4407420383.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.73,timestamp=4407420403.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.47,timestamp=4407420427.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.04,timestamp=4407420447.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.29,timestamp=4407420471.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:33, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3c4b9ffa00 recvbuff 0x7f3c4b9ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3c4b9ffa00,7f3c4b9ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:33,optimizer_step time: 49.39468765258789
rank:33, finish optimizer.step profile ...
rank:33, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:33, trace log has been written to txt...
rank:33, finish release GPU memory ...
rank:33, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9f9a00 recvbuff 0x7f4aec9f9a00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9f9a00,7f4aec9f9a00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 34, finish warm up ...
rank_id = 34, input_tensor_shapes: [(2048, 2, 12288)]
rank:34,cuda fwd time: 170.87283325195312
rank:34, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.27,timestamp=4407422123.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.5,timestamp=4407422133.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4407422143.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.66,timestamp=4407422153.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.7,timestamp=4407422164.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.87,timestamp=4407422174.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.84,timestamp=4407422185.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.86,timestamp=4407422196.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.86,timestamp=4407422206.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.52,timestamp=4407422217.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4407422228.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.21,timestamp=4407422238.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.8,timestamp=4407422250.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.64,timestamp=4407422261.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.68,timestamp=4407422272.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.8,timestamp=4407422283.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 34, finish FWD profile ...
rank:34, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.66,timestamp=4407422299.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.28,timestamp=4407422324.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.86,timestamp=4407422344.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.45,timestamp=4407422368.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.83,timestamp=4407422388.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.15,timestamp=4407422412.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.83,timestamp=4407422431.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.2,timestamp=4407422455.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.02,timestamp=4407422475.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.46,timestamp=4407422499.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.15,timestamp=4407422519.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.63,timestamp=4407422543.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.12,timestamp=4407422563.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.63,timestamp=4407422587.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.78,timestamp=4407422607.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.64,timestamp=4407422631.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:34, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3c4b9ffa00 recvbuff 0x7f3c4b9ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3c4b9ffa00,7f3c4b9ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:34,optimizer_step time: 49.3045768737793
rank:34, finish optimizer.step profile ...
rank:34, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:34, trace log has been written to txt...
rank:34, finish release GPU memory ...
rank:34, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a785d200 recvbuff 0x7f44a785d200 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a785d200,7f44a785d200,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 35, finish warm up ...
rank_id = 35, input_tensor_shapes: [(2048, 2, 12288)]
rank:35,cuda fwd time: 169.01426696777344
rank:35, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.24,timestamp=4407424343.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407424353.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4407424364.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.6,timestamp=4407424374.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.51,timestamp=4407424384.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.7,timestamp=4407424394.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.72,timestamp=4407424405.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.78,timestamp=4407424415.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.75,timestamp=4407424426.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407424436.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4407424447.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407424458.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.28,timestamp=4407424470.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.42,timestamp=4407424480.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4407424491.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.66,timestamp=4407424501.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 35, finish FWD profile ...
rank:35, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.33,timestamp=4407424518.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.98,timestamp=4407424542.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.12,timestamp=4407424562.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.47,timestamp=4407424587.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.08,timestamp=4407424606.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.26,timestamp=4407424631.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.97,timestamp=4407424650.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.28,timestamp=4407424674.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.76,timestamp=4407424694.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.43,timestamp=4407424718.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.7,timestamp=4407424737.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.46,timestamp=4407424762.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.86,timestamp=4407424781.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.35,timestamp=4407424805.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.98,timestamp=4407424825.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.3,timestamp=4407424849.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:35, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3c4b9ffa00 recvbuff 0x7f3c4b9ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3c4b9ffa00,7f3c4b9ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:35,optimizer_step time: 49.27692794799805
rank:35, finish optimizer.step profile ...
rank:35, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:35, trace log has been written to txt...
rank:35, finish release GPU memory ...
rank:35, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 4): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a78a3a00 recvbuff 0x7f44a78a3a00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a78a3a00,7f44a78a3a00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 36, finish warm up ...
rank_id = 36, input_tensor_shapes: [(2048, 2, 12288)]
rank:36,cuda fwd time: 169.10643005371094
rank:36, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.21,timestamp=4407426590.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.45,timestamp=4407426600.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4407426610.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.56,timestamp=4407426621.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.63,timestamp=4407426631.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.78,timestamp=4407426642.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.93,timestamp=4407426652.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.95,timestamp=4407426663.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.98,timestamp=4407426674.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.59,timestamp=4407426684.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.44,timestamp=4407426695.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.56,timestamp=4407426706.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.89,timestamp=4407426717.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407426727.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4407426738.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.44,timestamp=4407426748.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 36, finish FWD profile ...
rank:36, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.7,timestamp=4407426765.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.11,timestamp=4407426789.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.22,timestamp=4407426809.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.64,timestamp=4407426834.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.48,timestamp=4407426854.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.85,timestamp=4407426879.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.47,timestamp=4407426899.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.64,timestamp=4407426923.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.69,timestamp=4407426942.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.55,timestamp=4407426967.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.57,timestamp=4407426986.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.39,timestamp=4407427010.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.81,timestamp=4407427030.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.22,timestamp=4407427054.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.8,timestamp=4407427073.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.36,timestamp=4407427098.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:36, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3c4b9ffa00 recvbuff 0x7f3c4b9ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3c4b9ffa00,7f3c4b9ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:36,optimizer_step time: 49.33427047729492
rank:36, finish optimizer.step profile ...
rank:36, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:36, trace log has been written to txt...
rank:36, finish release GPU memory ...
rank:36, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 4): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a791a200 recvbuff 0x7f44a791a200 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a791a200,7f44a791a200,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 37, finish warm up ...
rank_id = 37, input_tensor_shapes: [(2048, 2, 12288)]
rank:37,cuda fwd time: 167.9851531982422
rank:37, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.13,timestamp=4407428741.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.46,timestamp=4407428751.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4407428761.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.55,timestamp=4407428771.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4407428782.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.67,timestamp=4407428792.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4407428803.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.76,timestamp=4407428813.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.7,timestamp=4407428823.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.14,timestamp=4407428834.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4407428845.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407428855.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4407428866.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.41,timestamp=4407428876.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4407428887.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.45,timestamp=4407428898.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 37, finish FWD profile ...
rank:37, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.76,timestamp=4407428914.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.62,timestamp=4407428938.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.94,timestamp=4407428958.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.39,timestamp=4407428982.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.06,timestamp=4407429002.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.51,timestamp=4407429026.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.09,timestamp=4407429046.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.72,timestamp=4407429070.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.9,timestamp=4407429090.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.76,timestamp=4407429114.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.77,timestamp=4407429134.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.59,timestamp=4407429158.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.04,timestamp=4407429178.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.26,timestamp=4407429202.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.79,timestamp=4407429221.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.38,timestamp=4407429246.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:37, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3c4b9ffa00 recvbuff 0x7f3c4b9ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3c4b9ffa00,7f3c4b9ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:37,optimizer_step time: 49.3568000793457
rank:37, finish optimizer.step profile ...
rank:37, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:37, trace log has been written to txt...
rank:37, finish release GPU memory ...
rank:37, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 4): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 38, finish warm up ...
rank_id = 38, input_tensor_shapes: [(2048, 2, 12288)]
rank:38,cuda fwd time: 169.64813232421875
rank:38, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.18,timestamp=4407430883.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.55,timestamp=4407430893.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.26,timestamp=4407430904.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.51,timestamp=4407430914.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.68,timestamp=4407430924.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.77,timestamp=4407430935.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.85,timestamp=4407430945.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.97,timestamp=4407430956.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.95,timestamp=4407430967.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.63,timestamp=4407430977.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.42,timestamp=4407430988.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.61,timestamp=4407430999.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4407431010.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.49,timestamp=4407431020.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4407431031.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.44,timestamp=4407431042.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 38, finish FWD profile ...
rank:38, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.24,timestamp=4407431058.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.38,timestamp=4407431082.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.1,timestamp=4407431102.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.42,timestamp=4407431127.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.11,timestamp=4407431146.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.5,timestamp=4407431171.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.09,timestamp=4407431191.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.61,timestamp=4407431215.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.09,timestamp=4407431235.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.58,timestamp=4407431259.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.65,timestamp=4407431279.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.54,timestamp=4407431303.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.81,timestamp=4407431322.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.43,timestamp=4407431347.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.98,timestamp=4407431366.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.27,timestamp=4407431390.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:38, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:38,optimizer_step time: 49.272830963134766
rank:38, finish optimizer.step profile ...
rank:38, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:38, trace log has been written to txt...
rank:38, finish release GPU memory ...
rank:38, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 4): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 39, finish warm up ...
rank_id = 39, input_tensor_shapes: [(2048, 2, 12288)]
rank:39,cuda fwd time: 172.95872497558594
rank:39, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.21,timestamp=4407433019.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407433030.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4407433040.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.66,timestamp=4407433050.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4407433061.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.98,timestamp=4407433071.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.95,timestamp=4407433082.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.11,timestamp=4407433093.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.08,timestamp=4407433104.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.89,timestamp=4407433114.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.74,timestamp=4407433126.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.87,timestamp=4407433136.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.6,timestamp=4407433149.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.84,timestamp=4407433159.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4407433171.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.15,timestamp=4407433182.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 39, finish FWD profile ...
rank:39, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.25,timestamp=4407433198.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.12,timestamp=4407433223.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.67,timestamp=4407433242.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.5,timestamp=4407433267.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.78,timestamp=4407433286.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.4,timestamp=4407433310.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.97,timestamp=4407433330.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.49,timestamp=4407433354.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.08,timestamp=4407433374.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.52,timestamp=4407433398.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.36,timestamp=4407433418.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.66,timestamp=4407433443.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.1,timestamp=4407433463.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.61,timestamp=4407433487.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.78,timestamp=4407433506.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.61,timestamp=4407433531.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:39, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:39,optimizer_step time: 49.4202880859375
rank:39, finish optimizer.step profile ...
rank:39, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:39, trace log has been written to txt...
rank:39, finish release GPU memory ...
rank:39, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 40, finish warm up ...
rank_id = 40, input_tensor_shapes: [(2048, 2, 12288)]
rank:40,cuda fwd time: 171.31826782226562
rank:40, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=5.81,timestamp=4407435215.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407435226.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4407435236.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.58,timestamp=4407435246.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.15,timestamp=4407435257.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.86,timestamp=4407435267.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.89,timestamp=4407435278.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.09,timestamp=4407435288.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.14,timestamp=4407435299.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.87,timestamp=4407435310.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.76,timestamp=4407435322.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.9,timestamp=4407435332.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4407435343.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.78,timestamp=4407435354.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4407435365.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.76,timestamp=4407435376.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 40, finish FWD profile ...
rank:40, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.34,timestamp=4407435393.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.31,timestamp=4407435417.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.57,timestamp=4407435436.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.35,timestamp=4407435461.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.82,timestamp=4407435480.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.3,timestamp=4407435504.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.9,timestamp=4407435524.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.25,timestamp=4407435548.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.08,timestamp=4407435568.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.63,timestamp=4407435592.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.13,timestamp=4407435612.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.71,timestamp=4407435636.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.89,timestamp=4407435656.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.76,timestamp=4407435681.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.82,timestamp=4407435700.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.68,timestamp=4407435725.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:40, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:40,optimizer_step time: 49.04447937011719
rank:40, finish optimizer.step profile ...
rank:40, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:40, trace log has been written to txt...
rank:40, finish release GPU memory ...
rank:40, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 5): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 41, finish warm up ...
rank_id = 41, input_tensor_shapes: [(2048, 2, 12288)]
rank:41,cuda fwd time: 168.16639709472656
rank:41, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.22,timestamp=4407437392.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407437402.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4407437412.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.6,timestamp=4407437422.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.61,timestamp=4407437433.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.69,timestamp=4407437443.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.72,timestamp=4407437454.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.76,timestamp=4407437464.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.77,timestamp=4407437475.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.45,timestamp=4407437485.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4407437496.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407437506.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.27,timestamp=4407437517.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407437527.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.65,timestamp=4407437539.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.71,timestamp=4407437549.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 41, finish FWD profile ...
rank:41, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.66,timestamp=4407437565.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.71,timestamp=4407437590.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.1,timestamp=4407437610.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.38,timestamp=4407437634.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.87,timestamp=4407437653.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.41,timestamp=4407437678.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.12,timestamp=4407437697.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.76,timestamp=4407437722.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.87,timestamp=4407437742.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.72,timestamp=4407437766.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.8,timestamp=4407437786.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.42,timestamp=4407437810.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.9,timestamp=4407437829.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.49,timestamp=4407437854.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.02,timestamp=4407437873.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.53,timestamp=4407437898.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:41, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:41,optimizer_step time: 49.20524978637695
rank:41, finish optimizer.step profile ...
rank:41, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:41, trace log has been written to txt...
rank:41, finish release GPU memory ...
rank:41, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 5): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 42, finish warm up ...
rank_id = 42, input_tensor_shapes: [(2048, 2, 12288)]
rank:42,cuda fwd time: 168.19200134277344
rank:42, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.25,timestamp=4407439561.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.49,timestamp=4407439572.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.59,timestamp=4407439582.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.58,timestamp=4407439592.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.63,timestamp=4407439603.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.65,timestamp=4407439613.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.71,timestamp=4407439623.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.77,timestamp=4407439634.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.78,timestamp=4407439644.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.54,timestamp=4407439655.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.41,timestamp=4407439666.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.52,timestamp=4407439676.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4407439687.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.45,timestamp=4407439698.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4407439709.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407439719.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 42, finish FWD profile ...
rank:42, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.19,timestamp=4407439735.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.89,timestamp=4407439760.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.86,timestamp=4407439779.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.45,timestamp=4407439803.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.4,timestamp=4407439823.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.73,timestamp=4407439848.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.34,timestamp=4407439868.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.6,timestamp=4407439893.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.92,timestamp=4407439912.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.65,timestamp=4407439937.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.75,timestamp=4407439956.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.63,timestamp=4407439981.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.87,timestamp=4407440000.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.44,timestamp=4407440024.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.95,timestamp=4407440044.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.27,timestamp=4407440068.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:42, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:42,optimizer_step time: 49.2042236328125
rank:42, finish optimizer.step profile ...
rank:42, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:42, trace log has been written to txt...
rank:42, finish release GPU memory ...
rank:42, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 5): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 43, finish warm up ...
rank_id = 43, input_tensor_shapes: [(2048, 2, 12288)]
rank:43,cuda fwd time: 171.15341186523438
rank:43, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=6.91,timestamp=4407441864.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.42,timestamp=4407441874.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.17,timestamp=4407441884.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.55,timestamp=4407441894.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4407441905.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.81,timestamp=4407441915.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4407441926.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.0,timestamp=4407441937.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.79,timestamp=4407441948.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.67,timestamp=4407441958.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4407441970.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.67,timestamp=4407441980.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.18,timestamp=4407441992.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.56,timestamp=4407442002.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4407442014.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.56,timestamp=4407442024.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 43, finish FWD profile ...
rank:43, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.84,timestamp=4407442041.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.75,timestamp=4407442065.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.96,timestamp=4407442084.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.33,timestamp=4407442109.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.96,timestamp=4407442128.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.35,timestamp=4407442152.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.92,timestamp=4407442172.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.41,timestamp=4407442196.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.22,timestamp=4407442216.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.59,timestamp=4407442241.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.99,timestamp=4407442260.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.59,timestamp=4407442285.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.68,timestamp=4407442304.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.57,timestamp=4407442328.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.95,timestamp=4407442348.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.58,timestamp=4407442372.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:43, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:43,optimizer_step time: 49.34860610961914
rank:43, finish optimizer.step profile ...
rank:43, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:43, trace log has been written to txt...
rank:43, finish release GPU memory ...
rank:43, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 5): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 44, finish warm up ...
rank_id = 44, input_tensor_shapes: [(2048, 2, 12288)]
rank:44,cuda fwd time: 170.28504943847656
rank:44, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=6.96,timestamp=4407444140.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.44,timestamp=4407444150.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.11,timestamp=4407444161.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407444171.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.44,timestamp=4407444181.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.71,timestamp=4407444192.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4407444202.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.91,timestamp=4407444213.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.72,timestamp=4407444224.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.55,timestamp=4407444234.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.27,timestamp=4407444246.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.57,timestamp=4407444256.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.19,timestamp=4407444267.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.57,timestamp=4407444278.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.24,timestamp=4407444289.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.57,timestamp=4407444300.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 44, finish FWD profile ...
rank:44, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.12,timestamp=4407444316.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=20.47,timestamp=4407444341.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.03,timestamp=4407444360.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.51,timestamp=4407444385.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.05,timestamp=4407444405.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.4,timestamp=4407444429.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.25,timestamp=4407444449.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.46,timestamp=4407444473.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.17,timestamp=4407444493.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.81,timestamp=4407444517.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.89,timestamp=4407444537.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.58,timestamp=4407444561.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.01,timestamp=4407444581.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.62,timestamp=4407444606.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.03,timestamp=4407444625.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.35,timestamp=4407444649.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:44, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:44,optimizer_step time: 49.37625503540039
rank:44, finish optimizer.step profile ...
rank:44, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:44, trace log has been written to txt...
rank:44, finish release GPU memory ...
rank:44, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 5): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 45, finish warm up ...
rank_id = 45, input_tensor_shapes: [(2048, 2, 12288)]
rank:45,cuda fwd time: 194.63577270507812
rank:45, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.2,timestamp=4407446371.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407446381.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4407446391.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.58,timestamp=4407446401.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4407446412.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.85,timestamp=4407446422.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.94,timestamp=4407446433.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.08,timestamp=4407446444.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.05,timestamp=4407446455.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.75,timestamp=4407446466.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4407446477.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.77,timestamp=4407446487.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4407446499.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.78,timestamp=4407446509.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.21,timestamp=4407446542.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.14,timestamp=4407446553.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 45, finish FWD profile ...
rank:45, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=7.72,timestamp=4407446571.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=20.21,timestamp=4407446595.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.62,timestamp=4407446614.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.28,timestamp=4407446638.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.92,timestamp=4407446658.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.35,timestamp=4407446682.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.42,timestamp=4407446702.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.71,timestamp=4407446727.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.17,timestamp=4407446747.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.63,timestamp=4407446771.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.83,timestamp=4407446790.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.23,timestamp=4407446815.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.79,timestamp=4407446834.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.59,timestamp=4407446858.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.99,timestamp=4407446878.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.29,timestamp=4407446902.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:45, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:45,optimizer_step time: 49.179649353027344
rank:45, finish optimizer.step profile ...
rank:45, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:45, trace log has been written to txt...
rank:45, finish release GPU memory ...
rank:45, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 5): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 46, finish warm up ...
rank_id = 46, input_tensor_shapes: [(2048, 2, 12288)]
rank:46,cuda fwd time: 168.71116638183594
rank:46, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.2,timestamp=4407448548.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407448558.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.71,timestamp=4407448569.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.58,timestamp=4407448579.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.6,timestamp=4407448589.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.69,timestamp=4407448600.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.77,timestamp=4407448610.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.77,timestamp=4407448621.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4407448631.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.65,timestamp=4407448642.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4407448653.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.61,timestamp=4407448663.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4407448674.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.53,timestamp=4407448685.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4407448696.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.5,timestamp=4407448706.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 46, finish FWD profile ...
rank:46, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.08,timestamp=4407448722.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.37,timestamp=4407448746.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.51,timestamp=4407448765.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.87,timestamp=4407448789.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.76,timestamp=4407448809.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.19,timestamp=4407448833.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.95,timestamp=4407448852.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.41,timestamp=4407448876.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.09,timestamp=4407448896.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.66,timestamp=4407448921.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.32,timestamp=4407448941.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.82,timestamp=4407448965.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.81,timestamp=4407448985.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.51,timestamp=4407449009.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.82,timestamp=4407449029.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.39,timestamp=4407449053.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:46, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:46,optimizer_step time: 49.18783950805664
rank:46, finish optimizer.step profile ...
rank:46, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:46, trace log has been written to txt...
rank:46, finish release GPU memory ...
rank:46, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 5): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 47, finish warm up ...
rank_id = 47, input_tensor_shapes: [(2048, 2, 12288)]
rank:47,cuda fwd time: 168.3732452392578
rank:47, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.22,timestamp=4407450725.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407450735.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4407450745.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.6,timestamp=4407450755.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4407450766.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.7,timestamp=4407450776.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.73,timestamp=4407450787.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.77,timestamp=4407450797.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.7,timestamp=4407450808.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.54,timestamp=4407450818.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4407450829.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.53,timestamp=4407450839.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4407450850.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.54,timestamp=4407450861.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.41,timestamp=4407450872.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.66,timestamp=4407450882.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 47, finish FWD profile ...
rank:47, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.25,timestamp=4407450899.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.89,timestamp=4407450923.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.17,timestamp=4407450943.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.3,timestamp=4407450967.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.03,timestamp=4407450987.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.32,timestamp=4407451011.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.04,timestamp=4407451031.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.58,timestamp=4407451055.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.93,timestamp=4407451075.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.41,timestamp=4407451099.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.82,timestamp=4407451118.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.34,timestamp=4407451143.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.98,timestamp=4407451162.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.38,timestamp=4407451187.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.12,timestamp=4407451206.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.52,timestamp=4407451231.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:47, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:47,optimizer_step time: 49.293312072753906
rank:47, finish optimizer.step profile ...
rank:47, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:47, trace log has been written to txt...
rank:47, finish release GPU memory ...
rank:47, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 48, finish warm up ...
rank_id = 48, input_tensor_shapes: [(2048, 2, 12288)]
rank:48,cuda fwd time: 170.4990692138672
rank:48, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.21,timestamp=4407452876.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407452887.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4407452897.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.63,timestamp=4407452907.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4407452918.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.83,timestamp=4407452928.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.82,timestamp=4407452939.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.96,timestamp=4407452949.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4407452960.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.74,timestamp=4407452971.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.64,timestamp=4407452982.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.78,timestamp=4407452993.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4407453004.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.77,timestamp=4407453014.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4407453026.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.72,timestamp=4407453036.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 48, finish FWD profile ...
rank:48, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.37,timestamp=4407453053.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.6,timestamp=4407453077.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.03,timestamp=4407453097.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.56,timestamp=4407453122.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.82,timestamp=4407453141.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.64,timestamp=4407453166.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.82,timestamp=4407453185.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.37,timestamp=4407453210.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.88,timestamp=4407453229.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.33,timestamp=4407453253.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.95,timestamp=4407453273.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.36,timestamp=4407453297.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.13,timestamp=4407453317.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.69,timestamp=4407453341.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.11,timestamp=4407453361.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.61,timestamp=4407453386.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:48, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:48,optimizer_step time: 49.29024124145508
rank:48, finish optimizer.step profile ...
rank:48, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:48, trace log has been written to txt...
rank:48, finish release GPU memory ...
rank:48, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 6): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 49, finish warm up ...
rank_id = 49, input_tensor_shapes: [(2048, 2, 12288)]
rank:49,cuda fwd time: 169.8744354248047
rank:49, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=6.95,timestamp=4407455219.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.36,timestamp=4407455229.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.14,timestamp=4407455240.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.5,timestamp=4407455250.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.51,timestamp=4407455260.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.74,timestamp=4407455271.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.63,timestamp=4407455282.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.93,timestamp=4407455292.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.76,timestamp=4407455303.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.55,timestamp=4407455314.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4407455325.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.52,timestamp=4407455335.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.18,timestamp=4407455346.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.43,timestamp=4407455357.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.01,timestamp=4407455368.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.37,timestamp=4407455378.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 49, finish FWD profile ...
rank:49, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.15,timestamp=4407455395.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.96,timestamp=4407455419.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.17,timestamp=4407455439.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.52,timestamp=4407455463.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.19,timestamp=4407455483.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.61,timestamp=4407455507.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.14,timestamp=4407455527.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.76,timestamp=4407455552.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.13,timestamp=4407455572.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.67,timestamp=4407455596.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.64,timestamp=4407455615.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.36,timestamp=4407455640.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.87,timestamp=4407455659.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.46,timestamp=4407455683.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.01,timestamp=4407455703.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.33,timestamp=4407455727.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:49, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:49,optimizer_step time: 49.29228973388672
rank:49, finish optimizer.step profile ...
rank:49, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:49, trace log has been written to txt...
rank:49, finish release GPU memory ...
rank:49, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 6): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a789f200 recvbuff 0x7f44a789f200 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a789f200,7f44a789f200,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 50, finish warm up ...
rank_id = 50, input_tensor_shapes: [(2048, 2, 12288)]
rank:50,cuda fwd time: 169.14944458007812
rank:50, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.23,timestamp=4407457469.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407457479.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4407457490.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.57,timestamp=4407457500.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.07,timestamp=4407457510.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.88,timestamp=4407457521.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.72,timestamp=4407457532.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.96,timestamp=4407457542.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.97,timestamp=4407457553.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.66,timestamp=4407457563.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4407457575.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.63,timestamp=4407457585.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.27,timestamp=4407457596.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.51,timestamp=4407457606.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4407457617.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407457628.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 50, finish FWD profile ...
rank:50, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.45,timestamp=4407457644.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.41,timestamp=4407457668.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.63,timestamp=4407457687.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.89,timestamp=4407457711.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.81,timestamp=4407457730.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.26,timestamp=4407457754.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.07,timestamp=4407457774.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.62,timestamp=4407457798.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.31,timestamp=4407457818.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.79,timestamp=4407457843.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.93,timestamp=4407457863.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.5,timestamp=4407457887.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.58,timestamp=4407457906.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.43,timestamp=4407457931.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.87,timestamp=4407457950.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.37,timestamp=4407457974.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:50, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:50,optimizer_step time: 49.21651077270508
rank:50, finish optimizer.step profile ...
rank:50, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:50, trace log has been written to txt...
rank:50, finish release GPU memory ...
rank:50, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 6): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 51, finish warm up ...
rank_id = 51, input_tensor_shapes: [(2048, 2, 12288)]
rank:51,cuda fwd time: 169.28256225585938
rank:51, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.23,timestamp=4407459679.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407459689.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4407459699.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.58,timestamp=4407459709.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.69,timestamp=4407459720.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.75,timestamp=4407459730.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.96,timestamp=4407459741.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.98,timestamp=4407459751.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.0,timestamp=4407459762.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.66,timestamp=4407459773.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4407459784.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.59,timestamp=4407459794.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4407459805.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.56,timestamp=4407459816.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.53,timestamp=4407459827.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407459837.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 51, finish FWD profile ...
rank:51, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.82,timestamp=4407459853.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.94,timestamp=4407459878.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.18,timestamp=4407459898.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.5,timestamp=4407459922.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.17,timestamp=4407459942.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.45,timestamp=4407459966.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.14,timestamp=4407459986.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.75,timestamp=4407460011.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.9,timestamp=4407460030.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.44,timestamp=4407460055.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.76,timestamp=4407460074.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.27,timestamp=4407460098.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.81,timestamp=4407460118.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.18,timestamp=4407460142.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.86,timestamp=4407460162.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.43,timestamp=4407460186.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:51, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:51,optimizer_step time: 49.35782241821289
rank:51, finish optimizer.step profile ...
rank:51, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:51, trace log has been written to txt...
rank:51, finish release GPU memory ...
rank:51, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 6): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 52, finish warm up ...
rank_id = 52, input_tensor_shapes: [(2048, 2, 12288)]
rank:52,cuda fwd time: 172.91162109375
rank:52, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.29,timestamp=4407461844.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.49,timestamp=4407461854.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.41,timestamp=4407461864.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.58,timestamp=4407461874.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.69,timestamp=4407461885.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.88,timestamp=4407461895.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.84,timestamp=4407461906.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.84,timestamp=4407461917.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.86,timestamp=4407461927.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.56,timestamp=4407461938.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4407461949.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.56,timestamp=4407461959.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.43,timestamp=4407461973.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.39,timestamp=4407461983.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.48,timestamp=4407461995.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.64,timestamp=4407462006.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 52, finish FWD profile ...
rank:52, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=11.81,timestamp=4407462022.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.91,timestamp=4407462047.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.93,timestamp=4407462066.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.26,timestamp=4407462091.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.99,timestamp=4407462110.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.28,timestamp=4407462134.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.02,timestamp=4407462154.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.25,timestamp=4407462178.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.12,timestamp=4407462198.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.72,timestamp=4407462223.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.89,timestamp=4407462242.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.74,timestamp=4407462267.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.77,timestamp=4407462286.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.7,timestamp=4407462311.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.23,timestamp=4407462330.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.62,timestamp=4407462355.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:52, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:52,optimizer_step time: 49.19398498535156
rank:52, finish optimizer.step profile ...
rank:52, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:52, trace log has been written to txt...
rank:52, finish release GPU memory ...
rank:52, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 6): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 53, finish warm up ...
rank_id = 53, input_tensor_shapes: [(2048, 2, 12288)]
rank:53,cuda fwd time: 172.95462036132812
rank:53, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.2,timestamp=4407464033.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407464043.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.28,timestamp=4407464054.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.58,timestamp=4407464064.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.72,timestamp=4407464074.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.86,timestamp=4407464085.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.0,timestamp=4407464096.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.08,timestamp=4407464106.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.08,timestamp=4407464117.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.82,timestamp=4407464128.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4407464139.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.8,timestamp=4407464150.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.61,timestamp=4407464161.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.79,timestamp=4407464172.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4407464183.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.77,timestamp=4407464195.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 53, finish FWD profile ...
rank:53, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.75,timestamp=4407464212.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.85,timestamp=4407464236.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.78,timestamp=4407464256.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.27,timestamp=4407464280.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.88,timestamp=4407464299.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.34,timestamp=4407464323.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.23,timestamp=4407464343.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.7,timestamp=4407464368.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.17,timestamp=4407464388.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.64,timestamp=4407464412.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.74,timestamp=4407464431.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.53,timestamp=4407464456.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.63,timestamp=4407464475.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.61,timestamp=4407464500.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.98,timestamp=4407464519.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.31,timestamp=4407464543.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:53, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:53,optimizer_step time: 49.298431396484375
rank:53, finish optimizer.step profile ...
rank:53, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:53, trace log has been written to txt...
rank:53, finish release GPU memory ...
rank:53, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 6): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 54, finish warm up ...
rank_id = 54, input_tensor_shapes: [(2048, 2, 12288)]
rank:54,cuda fwd time: 169.59487915039062
rank:54, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.16,timestamp=4407466231.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.49,timestamp=4407466241.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4407466252.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.55,timestamp=4407466262.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4407466272.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.67,timestamp=4407466282.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4407466293.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.75,timestamp=4407466303.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.74,timestamp=4407466314.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.45,timestamp=4407466324.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4407466335.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.51,timestamp=4407466346.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.14,timestamp=4407466359.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.39,timestamp=4407466369.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4407466380.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.46,timestamp=4407466390.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 54, finish FWD profile ...
rank:54, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.2,timestamp=4407466406.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.53,timestamp=4407466430.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.0,timestamp=4407466450.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.46,timestamp=4407466474.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.12,timestamp=4407466494.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.73,timestamp=4407466519.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.9,timestamp=4407466538.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.77,timestamp=4407466563.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.86,timestamp=4407466582.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.59,timestamp=4407466607.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.97,timestamp=4407466626.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.21,timestamp=4407466650.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.76,timestamp=4407466670.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.35,timestamp=4407466694.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.18,timestamp=4407466714.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.52,timestamp=4407466738.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:54, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:54,optimizer_step time: 49.344512939453125
rank:54, finish optimizer.step profile ...
rank:54, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:54, trace log has been written to txt...
rank:54, finish release GPU memory ...
rank:54, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 6): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 55, finish warm up ...
rank_id = 55, input_tensor_shapes: [(2048, 2, 12288)]
rank:55,cuda fwd time: 172.7508544921875
rank:55, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=3.66,timestamp=4407468411.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.51,timestamp=4407468421.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4407468432.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.58,timestamp=4407468442.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4407468452.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.87,timestamp=4407468462.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4407468473.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.08,timestamp=4407468484.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.15,timestamp=4407468495.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.89,timestamp=4407468506.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.64,timestamp=4407468517.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.76,timestamp=4407468528.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4407468539.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.74,timestamp=4407468549.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.53,timestamp=4407468560.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.64,timestamp=4407468571.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 55, finish FWD profile ...
rank:55, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.08,timestamp=4407468587.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.53,timestamp=4407468611.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.74,timestamp=4407468630.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.02,timestamp=4407468654.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.78,timestamp=4407468674.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.4,timestamp=4407468698.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.16,timestamp=4407468718.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.63,timestamp=4407468742.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.33,timestamp=4407468762.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.85,timestamp=4407468787.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.14,timestamp=4407468807.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.8,timestamp=4407468832.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.81,timestamp=4407468851.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.59,timestamp=4407468875.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.83,timestamp=4407468895.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.47,timestamp=4407468919.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:55, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:55,optimizer_step time: 49.27692794799805
rank:55, finish optimizer.step profile ...
rank:55, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:55, trace log has been written to txt...
rank:55, finish release GPU memory ...
rank:55, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 56, finish warm up ...
rank_id = 56, input_tensor_shapes: [(2048, 2, 12288)]
rank:56,cuda fwd time: 177.1304931640625
rank:56, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=6.93,timestamp=4407470701.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.42,timestamp=4407470711.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.15,timestamp=4407470722.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.5,timestamp=4407470732.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4407470743.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.7,timestamp=4407470753.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.72,timestamp=4407470764.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.02,timestamp=4407470774.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.28,timestamp=4407470785.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.65,timestamp=4407470796.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4407470807.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.68,timestamp=4407470818.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4407470829.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.77,timestamp=4407470840.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.61,timestamp=4407470851.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.74,timestamp=4407470862.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 56, finish FWD profile ...
rank:56, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.73,timestamp=4407470884.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.45,timestamp=4407470908.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.89,timestamp=4407470928.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.57,timestamp=4407470952.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.18,timestamp=4407470972.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.64,timestamp=4407470996.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.72,timestamp=4407471016.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.55,timestamp=4407471040.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.64,timestamp=4407471059.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.41,timestamp=4407471084.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.83,timestamp=4407471103.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.28,timestamp=4407471127.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.96,timestamp=4407471147.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.34,timestamp=4407471171.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.94,timestamp=4407471191.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.36,timestamp=4407471215.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:56, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:56,optimizer_step time: 49.38444900512695
rank:56, finish optimizer.step profile ...
rank:56, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:56, trace log has been written to txt...
rank:56, finish release GPU memory ...
rank:56, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 7): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 57, finish warm up ...
rank_id = 57, input_tensor_shapes: [(2048, 2, 12288)]
rank:57,cuda fwd time: 167.804931640625
rank:57, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.19,timestamp=4407473057.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407473067.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4407473077.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.59,timestamp=4407473087.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4407473098.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.7,timestamp=4407473108.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.73,timestamp=4407473119.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.79,timestamp=4407473129.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.78,timestamp=4407473140.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.41,timestamp=4407473150.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4407473161.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407473171.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.28,timestamp=4407473182.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.45,timestamp=4407473192.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4407473203.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.52,timestamp=4407473214.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 57, finish FWD profile ...
rank:57, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.43,timestamp=4407473230.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.88,timestamp=4407473255.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.16,timestamp=4407473274.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.49,timestamp=4407473299.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.76,timestamp=4407473318.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.36,timestamp=4407473342.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.19,timestamp=4407473362.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.76,timestamp=4407473387.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.94,timestamp=4407473406.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.62,timestamp=4407473431.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.82,timestamp=4407473450.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.45,timestamp=4407473475.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.89,timestamp=4407473494.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.47,timestamp=4407473518.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.02,timestamp=4407473538.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.4,timestamp=4407473562.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:57, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:57,optimizer_step time: 49.23187255859375
rank:57, finish optimizer.step profile ...
rank:57, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:57, trace log has been written to txt...
rank:57, finish release GPU memory ...
rank:57, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 7): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 58, finish warm up ...
rank_id = 58, input_tensor_shapes: [(2048, 2, 12288)]
rank:58,cuda fwd time: 168.98355102539062
rank:58, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.13,timestamp=4407475263.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407475273.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4407475284.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.56,timestamp=4407475294.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.42,timestamp=4407475304.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.71,timestamp=4407475315.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.77,timestamp=4407475325.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.86,timestamp=4407475336.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.82,timestamp=4407475347.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.53,timestamp=4407475357.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.44,timestamp=4407475368.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.53,timestamp=4407475378.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.99,timestamp=4407475390.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.91,timestamp=4407475400.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4407475411.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.44,timestamp=4407475422.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 58, finish FWD profile ...
rank:58, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.94,timestamp=4407475438.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.83,timestamp=4407475462.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.94,timestamp=4407475482.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.27,timestamp=4407475506.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.97,timestamp=4407475525.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.66,timestamp=4407475550.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.11,timestamp=4407475570.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.7,timestamp=4407475594.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.08,timestamp=4407475614.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.64,timestamp=4407475638.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.61,timestamp=4407475658.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.41,timestamp=4407475682.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.88,timestamp=4407475701.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.46,timestamp=4407475726.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.01,timestamp=4407475745.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.4,timestamp=4407475770.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:58, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:58,optimizer_step time: 49.117183685302734
rank:58, finish optimizer.step profile ...
rank:58, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:58, trace log has been written to txt...
rank:58, finish release GPU memory ...
rank:58, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 7): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 59, finish warm up ...
rank_id = 59, input_tensor_shapes: [(2048, 2, 12288)]
rank:59,cuda fwd time: 171.8435821533203
rank:59, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=5.79,timestamp=4407477459.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407477470.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4407477480.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.64,timestamp=4407477490.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.53,timestamp=4407477501.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.95,timestamp=4407477511.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.86,timestamp=4407477522.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.04,timestamp=4407477533.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.05,timestamp=4407477544.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.86,timestamp=4407477554.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.71,timestamp=4407477566.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.85,timestamp=4407477576.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4407477588.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.73,timestamp=4407477598.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.74,timestamp=4407477610.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.87,timestamp=4407477620.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 59, finish FWD profile ...
rank:59, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.59,timestamp=4407477637.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.34,timestamp=4407477662.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.64,timestamp=4407477681.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.62,timestamp=4407477706.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.38,timestamp=4407477726.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.7,timestamp=4407477750.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.92,timestamp=4407477770.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.32,timestamp=4407477794.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.02,timestamp=4407477814.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.48,timestamp=4407477838.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.16,timestamp=4407477858.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.73,timestamp=4407477883.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.12,timestamp=4407477902.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.77,timestamp=4407477927.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.85,timestamp=4407477946.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.54,timestamp=4407477971.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:59, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:59,optimizer_step time: 49.300479888916016
rank:59, finish optimizer.step profile ...
rank:59, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:59, trace log has been written to txt...
rank:59, finish release GPU memory ...
rank:59, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 7): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 60, finish warm up ...
rank_id = 60, input_tensor_shapes: [(2048, 2, 12288)]
rank:60,cuda fwd time: 177.84422302246094
rank:60, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.21,timestamp=4407479618.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407479628.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4407479638.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.57,timestamp=4407479649.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4407479659.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.7,timestamp=4407479669.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.68,timestamp=4407479680.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.86,timestamp=4407479691.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.81,timestamp=4407479701.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.51,timestamp=4407479712.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.42,timestamp=4407479723.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.9,timestamp=4407479733.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.74,timestamp=4407479752.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.52,timestamp=4407479763.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4407479775.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.46,timestamp=4407479785.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 60, finish FWD profile ...
rank:60, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.81,timestamp=4407479801.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.73,timestamp=4407479825.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.06,timestamp=4407479845.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.69,timestamp=4407479870.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.92,timestamp=4407479889.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.75,timestamp=4407479914.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.79,timestamp=4407479933.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.4,timestamp=4407479958.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.83,timestamp=4407479977.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.32,timestamp=4407480001.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.03,timestamp=4407480021.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.46,timestamp=4407480045.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.96,timestamp=4407480065.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.56,timestamp=4407480090.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.1,timestamp=4407480109.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.73,timestamp=4407480134.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:60, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:60,optimizer_step time: 49.13151931762695
rank:60, finish optimizer.step profile ...
rank:60, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:60, trace log has been written to txt...
rank:60, finish release GPU memory ...
rank:60, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 7): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 61, finish warm up ...
rank_id = 61, input_tensor_shapes: [(2048, 2, 12288)]
rank:61,cuda fwd time: 170.14886474609375
rank:61, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=5.42,timestamp=4407481770.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407481780.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4407481791.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.62,timestamp=4407481801.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.51,timestamp=4407481811.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.76,timestamp=4407481822.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.69,timestamp=4407481832.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.92,timestamp=4407481843.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.18,timestamp=4407481854.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.69,timestamp=4407481864.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.51,timestamp=4407481875.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.75,timestamp=4407481886.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4407481897.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.77,timestamp=4407481908.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.6,timestamp=4407481919.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.75,timestamp=4407481930.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 61, finish FWD profile ...
rank:61, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.21,timestamp=4407481946.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.92,timestamp=4407481971.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.1,timestamp=4407481991.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.6,timestamp=4407482015.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.73,timestamp=4407482034.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.52,timestamp=4407482059.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.67,timestamp=4407482078.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.4,timestamp=4407482102.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.86,timestamp=4407482122.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.34,timestamp=4407482146.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.04,timestamp=4407482166.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.32,timestamp=4407482190.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.12,timestamp=4407482210.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.74,timestamp=4407482234.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.13,timestamp=4407482254.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.6,timestamp=4407482278.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:61, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:61,optimizer_step time: 49.21036911010742
rank:61, finish optimizer.step profile ...
rank:61, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:61, trace log has been written to txt...
rank:61, finish release GPU memory ...
rank:61, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 7): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 62, finish warm up ...
rank_id = 62, input_tensor_shapes: [(2048, 2, 12288)]
rank:62,cuda fwd time: 171.09811401367188
rank:62, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.22,timestamp=4407483939.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.49,timestamp=4407483949.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.33,timestamp=4407483960.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.64,timestamp=4407483970.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4407483980.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.95,timestamp=4407483991.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.92,timestamp=4407484002.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.08,timestamp=4407484012.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.05,timestamp=4407484023.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.75,timestamp=4407484034.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4407484045.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.77,timestamp=4407484056.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4407484067.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.81,timestamp=4407484078.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4407484089.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.78,timestamp=4407484099.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 62, finish FWD profile ...
rank:62, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.12,timestamp=4407484116.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.15,timestamp=4407484141.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.65,timestamp=4407484160.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.47,timestamp=4407484185.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.24,timestamp=4407484204.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.55,timestamp=4407484229.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.83,timestamp=4407484248.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.33,timestamp=4407484272.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.84,timestamp=4407484292.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.33,timestamp=4407484316.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.15,timestamp=4407484336.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.64,timestamp=4407484360.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.12,timestamp=4407484380.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.79,timestamp=4407484405.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.85,timestamp=4407484424.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.6,timestamp=4407484449.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:62, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:62,optimizer_step time: 49.18476867675781
rank:62, finish optimizer.step profile ...
rank:62, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:62, trace log has been written to txt...
rank:62, finish release GPU memory ...
rank:62, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 7): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 63, finish warm up ...
rank_id = 63, input_tensor_shapes: [(2048, 2, 12288)]
rank:63,cuda fwd time: 169.57542419433594
rank:63, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.18,timestamp=4407486300.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407486310.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4407486321.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.58,timestamp=4407486331.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4407486341.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.72,timestamp=4407486352.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.81,timestamp=4407486362.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.85,timestamp=4407486373.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.85,timestamp=4407486384.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.55,timestamp=4407486394.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4407486405.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.56,timestamp=4407486415.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4407486426.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.51,timestamp=4407486437.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4407486448.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.69,timestamp=4407486459.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 63, finish FWD profile ...
rank:63, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.09,timestamp=4407486475.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.73,timestamp=4407486500.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.88,timestamp=4407486519.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.36,timestamp=4407486543.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.94,timestamp=4407486563.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.4,timestamp=4407486587.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.09,timestamp=4407486607.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.3,timestamp=4407486631.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.99,timestamp=4407486651.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.28,timestamp=4407486675.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.81,timestamp=4407486694.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.69,timestamp=4407486719.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.95,timestamp=4407486738.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.63,timestamp=4407486763.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.19,timestamp=4407486783.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.47,timestamp=4407486807.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:63, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:63,optimizer_step time: 49.279998779296875
rank:63, finish optimizer.step profile ...
rank:63, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:63, trace log has been written to txt...
rank:63, finish release GPU memory ...
rank:63, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 8): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 64, finish warm up ...
rank_id = 64, input_tensor_shapes: [(2048, 2, 12288)]
rank:64,cuda fwd time: 171.26707458496094
rank:64, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.21,timestamp=4407488680.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.49,timestamp=4407488690.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4407488700.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.61,timestamp=4407488710.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.53,timestamp=4407488721.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.85,timestamp=4407488731.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.84,timestamp=4407488742.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.97,timestamp=4407488752.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.92,timestamp=4407488763.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407488774.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4407488785.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.22,timestamp=4407488796.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.22,timestamp=4407488808.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.73,timestamp=4407488818.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.6,timestamp=4407488830.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.73,timestamp=4407488840.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 64, finish FWD profile ...
rank:64, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.4,timestamp=4407488857.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.93,timestamp=4407488881.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.18,timestamp=4407488901.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.81,timestamp=4407488926.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.88,timestamp=4407488945.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.56,timestamp=4407488970.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.79,timestamp=4407488989.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.44,timestamp=4407489013.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.88,timestamp=4407489033.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.28,timestamp=4407489057.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.97,timestamp=4407489076.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.48,timestamp=4407489101.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.17,timestamp=4407489121.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.63,timestamp=4407489145.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.18,timestamp=4407489165.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.65,timestamp=4407489189.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:64, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:64,optimizer_step time: 49.128448486328125
rank:64, finish optimizer.step profile ...
rank:64, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:64, trace log has been written to txt...
rank:64, finish release GPU memory ...
rank:64, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 8): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 65, finish warm up ...
rank_id = 65, input_tensor_shapes: [(2048, 2, 12288)]
rank:65,cuda fwd time: 174.38925170898438
rank:65, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.24,timestamp=4407490881.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407490891.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4407490901.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.55,timestamp=4407490912.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.1,timestamp=4407490929.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.76,timestamp=4407490939.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4407490950.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.66,timestamp=4407490960.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.6,timestamp=4407490970.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.4,timestamp=4407490980.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.73,timestamp=4407490992.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.45,timestamp=4407491002.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4407491013.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.45,timestamp=4407491023.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4407491034.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.54,timestamp=4407491045.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 65, finish FWD profile ...
rank:65, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.08,timestamp=4407491061.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.71,timestamp=4407491085.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.11,timestamp=4407491105.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.47,timestamp=4407491129.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.09,timestamp=4407491149.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.63,timestamp=4407491173.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.13,timestamp=4407491193.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.61,timestamp=4407491218.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.85,timestamp=4407491237.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.57,timestamp=4407491261.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.82,timestamp=4407491281.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.44,timestamp=4407491305.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.84,timestamp=4407491325.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.33,timestamp=4407491349.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.04,timestamp=4407491368.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.41,timestamp=4407491393.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:65, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:65,optimizer_step time: 49.33222579956055
rank:65, finish optimizer.step profile ...
rank:65, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:65, trace log has been written to txt...
rank:65, finish release GPU memory ...
rank:65, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 8): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 66, finish warm up ...
rank_id = 66, input_tensor_shapes: [(2048, 2, 12288)]
rank:66,cuda fwd time: 171.49952697753906
rank:66, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=6.01,timestamp=4407493047.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407493058.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4407493068.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.62,timestamp=4407493078.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.63,timestamp=4407493089.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.97,timestamp=4407493099.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.98,timestamp=4407493110.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.08,timestamp=4407493121.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.09,timestamp=4407493132.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.87,timestamp=4407493142.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.72,timestamp=4407493154.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.79,timestamp=4407493165.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.67,timestamp=4407493176.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.71,timestamp=4407493186.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.7,timestamp=4407493198.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.77,timestamp=4407493208.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 66, finish FWD profile ...
rank:66, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.36,timestamp=4407493225.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.23,timestamp=4407493250.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.79,timestamp=4407493269.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.47,timestamp=4407493293.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.98,timestamp=4407493313.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.27,timestamp=4407493337.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.95,timestamp=4407493357.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.7,timestamp=4407493381.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.18,timestamp=4407493401.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.68,timestamp=4407493426.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.1,timestamp=4407493445.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.6,timestamp=4407493470.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.68,timestamp=4407493489.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.48,timestamp=4407493514.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.82,timestamp=4407493533.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.66,timestamp=4407493557.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:66, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:66,optimizer_step time: 49.314815521240234
rank:66, finish optimizer.step profile ...
rank:66, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:66, trace log has been written to txt...
rank:66, finish release GPU memory ...
rank:66, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 8): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a79ffa00 recvbuff 0x7f44a79ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a79ffa00,7f44a79ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 67, finish warm up ...
rank_id = 67, input_tensor_shapes: [(2048, 2, 12288)]
rank:67,cuda fwd time: 168.96409606933594
rank:67, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.14,timestamp=4407495228.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407495238.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.27,timestamp=4407495248.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.61,timestamp=4407495258.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4407495269.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.72,timestamp=4407495279.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.69,timestamp=4407495290.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.79,timestamp=4407495300.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.76,timestamp=4407495311.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.5,timestamp=4407495321.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4407495332.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.54,timestamp=4407495343.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4407495354.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.58,timestamp=4407495364.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4407495375.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.7,timestamp=4407495386.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 67, finish FWD profile ...
rank:67, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.47,timestamp=4407495402.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.95,timestamp=4407495427.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.36,timestamp=4407495447.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.79,timestamp=4407495471.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.16,timestamp=4407495491.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.66,timestamp=4407495516.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.69,timestamp=4407495535.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.42,timestamp=4407495559.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.85,timestamp=4407495579.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.46,timestamp=4407495603.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.0,timestamp=4407495623.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.35,timestamp=4407495647.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.91,timestamp=4407495667.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.37,timestamp=4407495691.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.19,timestamp=4407495711.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.72,timestamp=4407495735.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:67, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a79ffa00 recvbuff 0x7f44a79ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a79ffa00,7f44a79ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:67,optimizer_step time: 49.179649353027344
rank:67, finish optimizer.step profile ...
rank:67, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:67, trace log has been written to txt...
rank:67, finish release GPU memory ...
rank:67, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 8): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a79ffa00 recvbuff 0x7f44a79ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a79ffa00,7f44a79ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 68, finish warm up ...
rank_id = 68, input_tensor_shapes: [(2048, 2, 12288)]
rank:68,cuda fwd time: 172.09446716308594
rank:68, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.19,timestamp=4407497420.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.51,timestamp=4407497430.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4407497440.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.65,timestamp=4407497451.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4407497461.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.97,timestamp=4407497472.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.91,timestamp=4407497482.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.07,timestamp=4407497493.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.1,timestamp=4407497504.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.87,timestamp=4407497515.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.77,timestamp=4407497526.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.88,timestamp=4407497537.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.77,timestamp=4407497548.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.89,timestamp=4407497559.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.79,timestamp=4407497570.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.92,timestamp=4407497581.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 68, finish FWD profile ...
rank:68, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.6,timestamp=4407497598.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.24,timestamp=4407497623.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.92,timestamp=4407497642.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.68,timestamp=4407497667.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.95,timestamp=4407497687.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.54,timestamp=4407497711.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.98,timestamp=4407497731.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.44,timestamp=4407497755.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.92,timestamp=4407497774.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.39,timestamp=4407497799.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.22,timestamp=4407497819.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.71,timestamp=4407497843.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.14,timestamp=4407497863.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.62,timestamp=4407497887.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.67,timestamp=4407497907.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.54,timestamp=4407497931.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:68, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a79ffa00 recvbuff 0x7f44a79ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a79ffa00,7f44a79ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:68,optimizer_step time: 49.3230094909668
rank:68, finish optimizer.step profile ...
rank:68, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:68, trace log has been written to txt...
rank:68, finish release GPU memory ...
rank:68, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 8): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf351c00 recvbuff 0x7f4ccf351c00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf351c00,7f4ccf351c00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 69, finish warm up ...
rank_id = 69, input_tensor_shapes: [(2048, 2, 12288)]
rank:69,cuda fwd time: 168.51046752929688
rank:69, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.2,timestamp=4407499566.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.49,timestamp=4407499576.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4407499586.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.59,timestamp=4407499597.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4407499607.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.71,timestamp=4407499617.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.71,timestamp=4407499628.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.79,timestamp=4407499638.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.74,timestamp=4407499649.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407499659.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4407499670.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.5,timestamp=4407499681.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4407499692.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.66,timestamp=4407499702.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.53,timestamp=4407499713.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.46,timestamp=4407499724.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 69, finish FWD profile ...
rank:69, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.23,timestamp=4407499740.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.91,timestamp=4407499764.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.38,timestamp=4407499784.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.71,timestamp=4407499809.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.39,timestamp=4407499829.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.67,timestamp=4407499854.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.73,timestamp=4407499873.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.59,timestamp=4407499897.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.82,timestamp=4407499917.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.45,timestamp=4407499941.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.78,timestamp=4407499961.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.33,timestamp=4407499985.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.02,timestamp=4407500004.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.48,timestamp=4407500029.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.11,timestamp=4407500049.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.48,timestamp=4407500073.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:69, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3c4b9ffa00 recvbuff 0x7f3c4b9ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3c4b9ffa00,7f3c4b9ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:69,optimizer_step time: 49.38956832885742
rank:69, finish optimizer.step profile ...
rank:69, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:69, trace log has been written to txt...
rank:69, finish release GPU memory ...
rank:69, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 8): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a7945a00 recvbuff 0x7f44a7945a00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a7945a00,7f44a7945a00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 70, finish warm up ...
rank_id = 70, input_tensor_shapes: [(2048, 2, 12288)]
rank:70,cuda fwd time: 171.52000427246094
rank:70, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.02,timestamp=4407501746.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407501756.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4407501766.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.62,timestamp=4407501776.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.83,timestamp=4407501787.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.0,timestamp=4407501798.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.93,timestamp=4407501808.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.99,timestamp=4407501819.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.96,timestamp=4407501830.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.64,timestamp=4407501840.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4407501852.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.68,timestamp=4407501862.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4407501874.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.73,timestamp=4407501885.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4407501896.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.62,timestamp=4407501906.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 70, finish FWD profile ...
rank:70, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.88,timestamp=4407501923.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.01,timestamp=4407501947.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.78,timestamp=4407501967.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.18,timestamp=4407501991.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.77,timestamp=4407502010.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.06,timestamp=4407502034.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.77,timestamp=4407502053.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.1,timestamp=4407502077.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.83,timestamp=4407502097.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.58,timestamp=4407502121.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.18,timestamp=4407502141.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.64,timestamp=4407502166.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.81,timestamp=4407502185.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.57,timestamp=4407502209.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.86,timestamp=4407502229.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.67,timestamp=4407502253.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:70, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3c4b9ffa00 recvbuff 0x7f3c4b9ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3c4b9ffa00,7f3c4b9ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:70,optimizer_step time: 49.21958541870117
rank:70, finish optimizer.step profile ...
rank:70, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:70, trace log has been written to txt...
rank:70, finish release GPU memory ...
rank:70, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 8): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3fe400 recvbuff 0x7f4ccf3fe400 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3fe400,7f4ccf3fe400,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 71, finish warm up ...
rank_id = 71, input_tensor_shapes: [(2048, 2, 12288)]
rank:71,cuda fwd time: 171.21177673339844
rank:71, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.24,timestamp=4407504009.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.49,timestamp=4407504020.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4407504030.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.63,timestamp=4407504040.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.61,timestamp=4407504051.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.85,timestamp=4407504061.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.8,timestamp=4407504072.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.09,timestamp=4407504082.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.05,timestamp=4407504093.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.87,timestamp=4407504104.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.81,timestamp=4407504115.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.91,timestamp=4407504126.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4407504137.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.8,timestamp=4407504148.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4407504159.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.78,timestamp=4407504170.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 71, finish FWD profile ...
rank:71, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.23,timestamp=4407504187.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.08,timestamp=4407504211.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.82,timestamp=4407504231.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.06,timestamp=4407504255.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.76,timestamp=4407504274.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.05,timestamp=4407504298.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.75,timestamp=4407504318.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.48,timestamp=4407504342.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.16,timestamp=4407504362.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.44,timestamp=4407504386.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.06,timestamp=4407504406.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.51,timestamp=4407504430.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.28,timestamp=4407504450.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.71,timestamp=4407504475.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.15,timestamp=4407504494.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.83,timestamp=4407504519.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:71, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3c4b9ffa00 recvbuff 0x7f3c4b9ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3c4b9ffa00,7f3c4b9ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:71,optimizer_step time: 49.188865661621094
rank:71, finish optimizer.step profile ...
rank:71, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:71, trace log has been written to txt...
rank:71, finish release GPU memory ...
rank:71, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 9): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a7944200 recvbuff 0x7f44a7944200 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a7944200,7f44a7944200,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 72, finish warm up ...
rank_id = 72, input_tensor_shapes: [(2048, 2, 12288)]
rank:72,cuda fwd time: 177.8401336669922
rank:72, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.19,timestamp=4407506207.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407506217.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4407506228.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.59,timestamp=4407506238.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.62,timestamp=4407506249.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.99,timestamp=4407506259.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.97,timestamp=4407506270.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.01,timestamp=4407506280.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.03,timestamp=4407506291.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.68,timestamp=4407506302.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.5,timestamp=4407506315.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.24,timestamp=4407506326.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.4,timestamp=4407506343.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.52,timestamp=4407506353.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.22,timestamp=4407506364.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.44,timestamp=4407506374.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 72, finish FWD profile ...
rank:72, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.03,timestamp=4407506391.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.76,timestamp=4407506415.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.88,timestamp=4407506434.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.31,timestamp=4407506458.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.4,timestamp=4407506478.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.7,timestamp=4407506503.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.4,timestamp=4407506523.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.72,timestamp=4407506548.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.11,timestamp=4407506567.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.6,timestamp=4407506592.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.65,timestamp=4407506611.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.42,timestamp=4407506635.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.82,timestamp=4407506655.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.31,timestamp=4407506679.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.98,timestamp=4407506699.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.47,timestamp=4407506723.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:72, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3c4b9ffa00 recvbuff 0x7f3c4b9ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3c4b9ffa00,7f3c4b9ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:72,optimizer_step time: 49.16940689086914
rank:72, finish optimizer.step profile ...
rank:72, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:72, trace log has been written to txt...
rank:72, finish release GPU memory ...
rank:72, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 9): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f479a8f9200 recvbuff 0x7f479a8f9200 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f479a8f9200,7f479a8f9200,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 73, finish warm up ...
rank_id = 73, input_tensor_shapes: [(2048, 2, 12288)]
rank:73,cuda fwd time: 173.5557098388672
rank:73, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=4.71,timestamp=4407508396.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.46,timestamp=4407508406.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.51,timestamp=4407508416.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.66,timestamp=4407508427.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.73,timestamp=4407508437.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.97,timestamp=4407508448.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.96,timestamp=4407508458.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.09,timestamp=4407508469.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.0,timestamp=4407508480.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.78,timestamp=4407508491.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.63,timestamp=4407508502.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.73,timestamp=4407508513.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4407508524.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.79,timestamp=4407508534.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.6,timestamp=4407508546.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.77,timestamp=4407508559.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 73, finish FWD profile ...
rank:73, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.73,timestamp=4407508575.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.97,timestamp=4407508599.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.84,timestamp=4407508619.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.61,timestamp=4407508644.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.04,timestamp=4407508663.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.28,timestamp=4407508687.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.98,timestamp=4407508707.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.36,timestamp=4407508731.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.14,timestamp=4407508751.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.69,timestamp=4407508776.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.19,timestamp=4407508795.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.71,timestamp=4407508820.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.88,timestamp=4407508839.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.79,timestamp=4407508864.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.81,timestamp=4407508883.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.53,timestamp=4407508908.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:73, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3c4b9ffa00 recvbuff 0x7f3c4b9ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3c4b9ffa00,7f3c4b9ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:73,optimizer_step time: 49.2492790222168
rank:73, finish optimizer.step profile ...
rank:73, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:73, trace log has been written to txt...
rank:73, finish release GPU memory ...
rank:73, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 9): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3fcc00 recvbuff 0x7f4ccf3fcc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3fcc00,7f4ccf3fcc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 74, finish warm up ...
rank_id = 74, input_tensor_shapes: [(2048, 2, 12288)]
rank:74,cuda fwd time: 169.8805694580078
rank:74, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.21,timestamp=4407510562.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407510572.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.98,timestamp=4407510583.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.56,timestamp=4407510593.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.44,timestamp=4407510603.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.66,timestamp=4407510614.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.72,timestamp=4407510624.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.77,timestamp=4407510635.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.11,timestamp=4407510645.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.05,timestamp=4407510656.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4407510667.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.66,timestamp=4407510677.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4407510689.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.67,timestamp=4407510699.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.6,timestamp=4407510710.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.76,timestamp=4407510721.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 74, finish FWD profile ...
rank:74, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.91,timestamp=4407510738.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.8,timestamp=4407510762.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.17,timestamp=4407510782.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.64,timestamp=4407510806.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.18,timestamp=4407510826.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.69,timestamp=4407510851.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.98,timestamp=4407510870.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.72,timestamp=4407510895.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.94,timestamp=4407510914.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.65,timestamp=4407510939.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.95,timestamp=4407510958.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.3,timestamp=4407510983.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.01,timestamp=4407511002.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.5,timestamp=4407511027.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.32,timestamp=4407511047.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.63,timestamp=4407511071.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:74, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3c4b9fe200 recvbuff 0x7f3c4b9fe200 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3c4b9fe200,7f3c4b9fe200,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:74,optimizer_step time: 49.165313720703125
rank:74, finish optimizer.step profile ...
rank:74, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:74, trace log has been written to txt...
rank:74, finish release GPU memory ...
rank:74, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 9): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a79ffa00 recvbuff 0x7f44a79ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a79ffa00,7f44a79ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 75, finish warm up ...
rank_id = 75, input_tensor_shapes: [(2048, 2, 12288)]
rank:75,cuda fwd time: 171.04486083984375
rank:75, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=5.93,timestamp=4407512717.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.5,timestamp=4407512728.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4407512738.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.61,timestamp=4407512748.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4407512759.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.69,timestamp=4407512769.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4407512779.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.97,timestamp=4407512790.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.95,timestamp=4407512801.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.79,timestamp=4407512811.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.63,timestamp=4407512823.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.89,timestamp=4407512833.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.7,timestamp=4407512845.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.91,timestamp=4407512856.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.75,timestamp=4407512867.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.72,timestamp=4407512878.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 75, finish FWD profile ...
rank:75, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=14.55,timestamp=4407512895.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.28,timestamp=4407512919.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.94,timestamp=4407512939.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.58,timestamp=4407512963.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.41,timestamp=4407512984.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.7,timestamp=4407513008.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.38,timestamp=4407513028.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.29,timestamp=4407513052.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.85,timestamp=4407513072.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.41,timestamp=4407513096.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.13,timestamp=4407513116.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.53,timestamp=4407513140.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.21,timestamp=4407513160.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.57,timestamp=4407513184.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.09,timestamp=4407513204.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.8,timestamp=4407513229.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:75, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a79ffa00 recvbuff 0x7f44a79ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a79ffa00,7f44a79ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:75,optimizer_step time: 49.22982406616211
rank:75, finish optimizer.step profile ...
rank:75, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:75, trace log has been written to txt...
rank:75, finish release GPU memory ...
rank:75, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 9): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 76, finish warm up ...
rank_id = 76, input_tensor_shapes: [(2048, 2, 12288)]
rank:76,cuda fwd time: 169.0449981689453
rank:76, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.26,timestamp=4407514861.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407514871.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4407514882.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.57,timestamp=4407514892.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4407514902.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.74,timestamp=4407514912.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.81,timestamp=4407514923.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.88,timestamp=4407514934.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.95,timestamp=4407514944.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.64,timestamp=4407514955.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4407514966.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.61,timestamp=4407514977.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4407514988.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.57,timestamp=4407514998.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4407515009.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.54,timestamp=4407515019.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 76, finish FWD profile ...
rank:76, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.54,timestamp=4407515036.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.58,timestamp=4407515060.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.72,timestamp=4407515079.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.15,timestamp=4407515104.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.97,timestamp=4407515123.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.42,timestamp=4407515148.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.17,timestamp=4407515167.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.57,timestamp=4407515192.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.44,timestamp=4407515212.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.79,timestamp=4407515237.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.9,timestamp=4407515256.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.64,timestamp=4407515281.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.84,timestamp=4407515300.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.4,timestamp=4407515324.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.05,timestamp=4407515344.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.43,timestamp=4407515368.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:76, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:76,optimizer_step time: 49.21753692626953
rank:76, finish optimizer.step profile ...
rank:76, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:76, trace log has been written to txt...
rank:76, finish release GPU memory ...
rank:76, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 9): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 77, finish warm up ...
rank_id = 77, input_tensor_shapes: [(2048, 2, 12288)]
rank:77,cuda fwd time: 167.74656677246094
rank:77, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=5.12,timestamp=4407517078.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.49,timestamp=4407517088.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.42,timestamp=4407517098.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.59,timestamp=4407517109.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4407517119.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.63,timestamp=4407517129.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4407517140.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.76,timestamp=4407517150.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.77,timestamp=4407517161.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.43,timestamp=4407517171.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4407517182.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407517192.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4407517203.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.5,timestamp=4407517214.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.44,timestamp=4407517225.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.55,timestamp=4407517235.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 77, finish FWD profile ...
rank:77, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.26,timestamp=4407517251.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.52,timestamp=4407517275.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.83,timestamp=4407517295.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.33,timestamp=4407517319.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.29,timestamp=4407517339.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.69,timestamp=4407517363.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.09,timestamp=4407517383.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.63,timestamp=4407517408.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.81,timestamp=4407517427.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.57,timestamp=4407517451.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.84,timestamp=4407517471.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.46,timestamp=4407517495.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.83,timestamp=4407517515.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.31,timestamp=4407517539.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.83,timestamp=4407517558.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.31,timestamp=4407517582.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:77, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:77,optimizer_step time: 49.311744689941406
rank:77, finish optimizer.step profile ...
rank:77, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:77, trace log has been written to txt...
rank:77, finish release GPU memory ...
rank:77, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 9): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 78, finish warm up ...
rank_id = 78, input_tensor_shapes: [(2048, 2, 12288)]
rank:78,cuda fwd time: 168.97740173339844
rank:78, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.25,timestamp=4407519251.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.5,timestamp=4407519262.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4407519272.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.56,timestamp=4407519282.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.23,timestamp=4407519292.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.58,timestamp=4407519303.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.04,timestamp=4407519313.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.75,timestamp=4407519323.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.74,timestamp=4407519334.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.59,timestamp=4407519344.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4407519355.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.61,timestamp=4407519366.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4407519377.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.77,timestamp=4407519388.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.6,timestamp=4407519399.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.74,timestamp=4407519410.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 78, finish FWD profile ...
rank:78, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.53,timestamp=4407519426.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.93,timestamp=4407519451.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.38,timestamp=4407519471.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.84,timestamp=4407519495.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.24,timestamp=4407519515.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.63,timestamp=4407519540.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.61,timestamp=4407519559.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.36,timestamp=4407519583.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.89,timestamp=4407519603.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.46,timestamp=4407519627.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.0,timestamp=4407519647.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.32,timestamp=4407519671.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.87,timestamp=4407519690.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.51,timestamp=4407519715.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.23,timestamp=4407519735.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.71,timestamp=4407519759.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:78, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:78,optimizer_step time: 49.179649353027344
rank:78, finish optimizer.step profile ...
rank:78, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:78, trace log has been written to txt...
rank:78, finish release GPU memory ...
rank:78, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 9): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 79, finish warm up ...
rank_id = 79, input_tensor_shapes: [(2048, 2, 12288)]
rank:79,cuda fwd time: 186.3055419921875
rank:79, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.2,timestamp=4407521648.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407521658.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4407521668.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.62,timestamp=4407521678.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4407521689.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.96,timestamp=4407521700.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.06,timestamp=4407521710.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.16,timestamp=4407521721.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.16,timestamp=4407521732.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.86,timestamp=4407521743.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4407521754.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.77,timestamp=4407521765.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4407521776.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.78,timestamp=4407521800.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.96,timestamp=4407521813.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.41,timestamp=4407521823.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 79, finish FWD profile ...
rank:79, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.38,timestamp=4407521839.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.95,timestamp=4407521864.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.77,timestamp=4407521883.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.08,timestamp=4407521907.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.82,timestamp=4407521927.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.24,timestamp=4407521951.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.07,timestamp=4407521971.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.6,timestamp=4407521995.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.97,timestamp=4407522015.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.5,timestamp=4407522039.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.17,timestamp=4407522059.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.64,timestamp=4407522083.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.35,timestamp=4407522103.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.85,timestamp=4407522128.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.91,timestamp=4407522147.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.64,timestamp=4407522172.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:79, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:79,optimizer_step time: 49.31071853637695
rank:79, finish optimizer.step profile ...
rank:79, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:79, trace log has been written to txt...
rank:79, finish release GPU memory ...
rank:79, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 10): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 80, finish warm up ...
rank_id = 80, input_tensor_shapes: [(2048, 2, 12288)]
rank:80,cuda fwd time: 174.8838348388672
rank:80, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=6.11,timestamp=4407523842.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.49,timestamp=4407523852.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.04,timestamp=4407523863.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.58,timestamp=4407523873.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.77,timestamp=4407523883.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.68,timestamp=4407523893.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.69,timestamp=4407523904.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.77,timestamp=4407523914.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.8,timestamp=4407523925.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.44,timestamp=4407523935.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4407523946.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.67,timestamp=4407523963.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.41,timestamp=4407523974.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.2,timestamp=4407523985.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4407523996.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.52,timestamp=4407524006.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 80, finish FWD profile ...
rank:80, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.31,timestamp=4407524023.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.76,timestamp=4407524047.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.11,timestamp=4407524067.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.57,timestamp=4407524091.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.72,timestamp=4407524110.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.9,timestamp=4407524134.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.62,timestamp=4407524154.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.14,timestamp=4407524178.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.82,timestamp=4407524197.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.39,timestamp=4407524221.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.87,timestamp=4407524241.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.35,timestamp=4407524265.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.93,timestamp=4407524284.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.37,timestamp=4407524309.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.19,timestamp=4407524328.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.67,timestamp=4407524353.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:80, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:80,optimizer_step time: 49.15302276611328
rank:80, finish optimizer.step profile ...
rank:80, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:80, trace log has been written to txt...
rank:80, finish release GPU memory ...
rank:80, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 10): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 81, finish warm up ...
rank_id = 81, input_tensor_shapes: [(2048, 2, 12288)]
rank:81,cuda fwd time: 175.34259033203125
rank:81, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.22,timestamp=4407526226.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.48,timestamp=4407526236.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.28,timestamp=4407526246.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.53,timestamp=4407526256.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.73,timestamp=4407526267.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.75,timestamp=4407526277.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.89,timestamp=4407526288.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.96,timestamp=4407526299.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.94,timestamp=4407526309.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.65,timestamp=4407526320.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4407526331.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.67,timestamp=4407526342.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4407526353.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.65,timestamp=4407526363.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4407526374.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.72,timestamp=4407526390.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 81, finish FWD profile ...
rank:81, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.36,timestamp=4407526407.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.54,timestamp=4407526431.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.82,timestamp=4407526450.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.54,timestamp=4407526475.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.2,timestamp=4407526494.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.76,timestamp=4407526519.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.91,timestamp=4407526539.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.6,timestamp=4407526563.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.66,timestamp=4407526582.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.41,timestamp=4407526607.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.91,timestamp=4407526626.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.44,timestamp=4407526650.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.95,timestamp=4407526670.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.46,timestamp=4407526694.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.15,timestamp=4407526714.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.65,timestamp=4407526739.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:81, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:81,optimizer_step time: 49.41414260864258
rank:81, finish optimizer.step profile ...
rank:81, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:81, trace log has been written to txt...
rank:81, finish release GPU memory ...
rank:81, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 10): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 82, finish warm up ...
rank_id = 82, input_tensor_shapes: [(2048, 2, 12288)]
rank:82,cuda fwd time: 169.83859252929688
rank:82, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.27,timestamp=4407528394.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.45,timestamp=4407528404.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4407528415.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.55,timestamp=4407528425.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4407528435.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.86,timestamp=4407528446.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.89,timestamp=4407528457.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.97,timestamp=4407528467.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.04,timestamp=4407528478.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.73,timestamp=4407528489.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4407528500.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.73,timestamp=4407528510.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4407528521.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.62,timestamp=4407528532.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4407528543.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.53,timestamp=4407528553.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 82, finish FWD profile ...
rank:82, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.84,timestamp=4407528570.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.51,timestamp=4407528594.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.76,timestamp=4407528613.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.3,timestamp=4407528637.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.96,timestamp=4407528657.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.4,timestamp=4407528681.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.11,timestamp=4407528701.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.53,timestamp=4407528725.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.12,timestamp=4407528745.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.65,timestamp=4407528770.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.92,timestamp=4407528789.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.57,timestamp=4407528813.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.81,timestamp=4407528833.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.45,timestamp=4407528857.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.86,timestamp=4407528877.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.34,timestamp=4407528901.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:82, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:82,optimizer_step time: 49.23699188232422
rank:82, finish optimizer.step profile ...
rank:82, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:82, trace log has been written to txt...
rank:82, finish release GPU memory ...
rank:82, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 10): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 83, finish warm up ...
rank_id = 83, input_tensor_shapes: [(2048, 2, 12288)]
rank:83,cuda fwd time: 179.2296905517578
rank:83, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=6.91,timestamp=4407530704.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.44,timestamp=4407530714.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.12,timestamp=4407530724.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.5,timestamp=4407530734.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4407530745.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.73,timestamp=4407530755.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.71,timestamp=4407530766.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.9,timestamp=4407530777.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.64,timestamp=4407530788.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.58,timestamp=4407530798.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4407530809.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.6,timestamp=4407530820.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.2,timestamp=4407530831.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.59,timestamp=4407530842.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.16,timestamp=4407530862.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.3,timestamp=4407530872.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 83, finish FWD profile ...
rank:83, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.8,timestamp=4407530888.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.5,timestamp=4407530912.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.86,timestamp=4407530932.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.57,timestamp=4407530956.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.07,timestamp=4407530976.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.6,timestamp=4407531000.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.38,timestamp=4407531020.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.73,timestamp=4407531045.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.73,timestamp=4407531064.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.46,timestamp=4407531089.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.99,timestamp=4407531108.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.26,timestamp=4407531133.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.81,timestamp=4407531152.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.29,timestamp=4407531176.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.12,timestamp=4407531196.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.71,timestamp=4407531221.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:83, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:83,optimizer_step time: 49.42335891723633
rank:83, finish optimizer.step profile ...
rank:83, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:83, trace log has been written to txt...
rank:83, finish release GPU memory ...
rank:83, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 10): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 84, finish warm up ...
rank_id = 84, input_tensor_shapes: [(2048, 2, 12288)]
rank:84,cuda fwd time: 177.7039337158203
rank:84, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.23,timestamp=4407533106.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.46,timestamp=4407533116.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4407533126.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.64,timestamp=4407533137.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.93,timestamp=4407533147.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.96,timestamp=4407533158.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.97,timestamp=4407533168.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.18,timestamp=4407533179.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.19,timestamp=4407533190.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.87,timestamp=4407533201.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4407533212.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.77,timestamp=4407533223.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.63,timestamp=4407533234.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.79,timestamp=4407533245.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.55,timestamp=4407533257.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.79,timestamp=4407533267.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 84, finish FWD profile ...
rank:84, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=12.12,timestamp=4407533289.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.59,timestamp=4407533313.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.54,timestamp=4407533333.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.19,timestamp=4407533357.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.86,timestamp=4407533376.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.33,timestamp=4407533400.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.92,timestamp=4407533420.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.11,timestamp=4407533444.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.82,timestamp=4407533463.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.33,timestamp=4407533488.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.92,timestamp=4407533507.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.54,timestamp=4407533531.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.89,timestamp=4407533551.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.61,timestamp=4407533575.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.83,timestamp=4407533595.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.44,timestamp=4407533619.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:84, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:84,optimizer_step time: 49.247230529785156
rank:84, finish optimizer.step profile ...
rank:84, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:84, trace log has been written to txt...
rank:84, finish release GPU memory ...
rank:84, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 10): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 85, finish warm up ...
rank_id = 85, input_tensor_shapes: [(2048, 2, 12288)]
rank:85,cuda fwd time: 171.35206604003906
rank:85, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.24,timestamp=4407535295.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407535306.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.28,timestamp=4407535316.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.66,timestamp=4407535326.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.71,timestamp=4407535337.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.98,timestamp=4407535347.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.93,timestamp=4407535358.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.08,timestamp=4407535369.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.06,timestamp=4407535380.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.75,timestamp=4407535390.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.6,timestamp=4407535401.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.74,timestamp=4407535412.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.53,timestamp=4407535423.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.75,timestamp=4407535434.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.17,timestamp=4407535445.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.69,timestamp=4407535456.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 85, finish FWD profile ...
rank:85, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.2,timestamp=4407535473.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.96,timestamp=4407535497.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.09,timestamp=4407535517.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.74,timestamp=4407535542.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.94,timestamp=4407535561.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.65,timestamp=4407535586.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.19,timestamp=4407535605.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.42,timestamp=4407535630.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.05,timestamp=4407535649.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.32,timestamp=4407535674.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.98,timestamp=4407535693.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.65,timestamp=4407535718.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.13,timestamp=4407535737.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.82,timestamp=4407535762.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.76,timestamp=4407535782.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.55,timestamp=4407535806.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:85, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:85,optimizer_step time: 49.21036911010742
rank:85, finish optimizer.step profile ...
rank:85, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:85, trace log has been written to txt...
rank:85, finish release GPU memory ...
rank:85, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 10): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 86, finish warm up ...
rank_id = 86, input_tensor_shapes: [(2048, 2, 12288)]
rank:86,cuda fwd time: 171.6643829345703
rank:86, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.07,timestamp=4407537484.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407537494.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4407537504.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.61,timestamp=4407537515.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.64,timestamp=4407537525.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.97,timestamp=4407537536.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.95,timestamp=4407537547.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.97,timestamp=4407537557.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.97,timestamp=4407537568.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.75,timestamp=4407537579.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.76,timestamp=4407537590.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.9,timestamp=4407537601.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.63,timestamp=4407537612.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.88,timestamp=4407537623.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4407537634.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.73,timestamp=4407537645.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 86, finish FWD profile ...
rank:86, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.45,timestamp=4407537661.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.93,timestamp=4407537686.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.39,timestamp=4407537706.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.74,timestamp=4407537730.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.91,timestamp=4407537750.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.3,timestamp=4407537774.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.99,timestamp=4407537793.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.5,timestamp=4407537818.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.13,timestamp=4407537838.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.71,timestamp=4407537862.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.17,timestamp=4407537882.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.6,timestamp=4407537906.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.61,timestamp=4407537926.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.54,timestamp=4407537950.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.8,timestamp=4407537970.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.43,timestamp=4407537994.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:86, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:86,optimizer_step time: 49.261566162109375
rank:86, finish optimizer.step profile ...
rank:86, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:86, trace log has been written to txt...
rank:86, finish release GPU memory ...
rank:86, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 10): 1812615168
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812615168 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffe00 recvbuff 0x7f4ccf3ffe00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffe00,7f4ccf3ffe00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 87, finish warm up ...
rank_id = 87, input_tensor_shapes: [(2048, 2, 12288)]
rank:87,cuda fwd time: 169.23443603515625
rank:87, fwd_subop num: 16, fwd_subop: ['trace_src_func=allreduce,duration=7.21,timestamp=4407539689.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.5,timestamp=4407539700.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4407539710.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.58,timestamp=4407539720.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4407539730.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.57,timestamp=4407539740.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.44,timestamp=4407539751.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.62,timestamp=4407539761.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.74,timestamp=4407539771.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.62,timestamp=4407539782.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4407539793.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.75,timestamp=4407539804.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.69,timestamp=4407539815.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.88,timestamp=4407539826.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.76,timestamp=4407539837.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.88,timestamp=4407539848.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 87, finish FWD profile ...
rank:87, bwd_subop num: 16, bwd_subop: ['trace_src_func=allreduce,duration=13.48,timestamp=4407539865.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.3,timestamp=4407539889.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.58,timestamp=4407539910.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.89,timestamp=4407539935.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.6,timestamp=4407539955.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.5,timestamp=4407539979.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.92,timestamp=4407539999.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.21,timestamp=4407540023.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.84,timestamp=4407540042.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.18,timestamp=4407540067.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=18.89,timestamp=4407540086.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.27,timestamp=4407540110.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.14,timestamp=4407540130.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.66,timestamp=4407540155.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=19.12,timestamp=4407540174.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.76,timestamp=4407540199.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:87, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3ffc00 recvbuff 0x7f4ccf3ffc00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3ffc00,7f4ccf3ffc00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:87,optimizer_step time: 49.263614654541016
rank:87, finish optimizer.step profile ...
rank:87, Before memory release - Allocated: 59855915520, Reserved: 79593209856
rank:87, trace log has been written to txt...
rank:87, finish release GPU memory ...
rank:87, After memory release - Allocated: 30250091520, Reserved: 45436895232
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f489c000000 recvbuff 0x7f489c000000 count 77266944 datatype 7 op 0 root 0 comm 0xe59efc0 [nranks=1] stream 0xa1b3c10
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f489c000000,7f489c000000,77266944,7,0,0,0xe59efc0,0xa1b3c10)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 11): 1889906688
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1889906688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f479a930e00 recvbuff 0x7f479a930e00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f479a930e00,7f479a930e00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 88, finish warm up ...
rank_id = 88, input_tensor_shapes: [(2048, 2, 12288)]
rank:88,cuda fwd time: 208.91293334960938
rank:88, fwd_subop num: 19, fwd_subop: ['trace_src_func=allreduce,duration=7.03,timestamp=4407542478.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.22,timestamp=4407542489.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.09,timestamp=4407542499.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.54,timestamp=4407542509.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.67,timestamp=4407542520.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.63,timestamp=4407542530.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.83,timestamp=4407542541.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.67,timestamp=4407542551.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.1,timestamp=4407542562.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.45,timestamp=4407542573.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.14,timestamp=4407542584.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.47,timestamp=4407542594.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.95,timestamp=4407542605.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.79,timestamp=4407542616.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.82,timestamp=4407542627.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.12,timestamp=4407542638.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=36.78,timestamp=4407542676.22,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.04,timestamp=4407542677.03,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.16,timestamp=4407542677.3,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 88, finish FWD profile ...
rank:88, bwd_subop num: 17, bwd_subop: ['trace_src_func=allreduce,duration=34.34,timestamp=4407542713.45,input__shape=[2048, 2, 12288],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=47.18,timestamp=4407542761.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.61,timestamp=4407542784.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.79,timestamp=4407542803.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.94,timestamp=4407542825.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.66,timestamp=4407542844.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.95,timestamp=4407542867.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.46,timestamp=4407542885.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.95,timestamp=4407542908.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.68,timestamp=4407542926.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.94,timestamp=4407542949.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.71,timestamp=4407542967.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.11,timestamp=4407542990.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.7,timestamp=4407543008.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.98,timestamp=4407543031.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.67,timestamp=4407543049.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.95,timestamp=4407543072.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:88, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f3c969ff800 recvbuff 0x7f3c969ff800 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f3c969ff800,7f3c969ff800,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:88,optimizer_step time: 41.28972625732422
rank:88, finish optimizer.step profile ...
rank:88, Before memory release - Allocated: 45570771968, Reserved: 79127642112
rank:88, trace log has been written to txt...
rank:88, finish release GPU memory ...
rank:88, After memory release - Allocated: 15547941888, Reserved: 37752930304
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffc00 recvbuff 0x7f4aec9ffc00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffc00,7f4aec9ffc00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffe00 recvbuff 0x7f4aec9ffe00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffe00,7f4aec9ffe00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4ccf3fca00 recvbuff 0x7f4ccf3fca00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4ccf3fca00,7f4ccf3fca00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4194000000 recvbuff 0x7f4194000000 count 77266944 datatype 7 op 0 root 0 comm 0xe59efc0 [nranks=1] stream 0xa1b3c10
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4194000000,7f4194000000,77266944,7,0,0,0xe59efc0,0xa1b3c10)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 11): 1889906688
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1889906688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9f5000 recvbuff 0x7f4aec9f5000 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9f5000,7f4aec9f5000,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 89, finish warm up ...
rank_id = 89, input_tensor_shapes: [(2048, 2, 12288)]
rank:89,cuda fwd time: 199.48133850097656
rank:89, fwd_subop num: 19, fwd_subop: ['trace_src_func=allreduce,duration=8.38,timestamp=4407545323.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.1,timestamp=4407545333.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.41,timestamp=4407545343.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.82,timestamp=4407545353.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.15,timestamp=4407545363.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4407545373.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.34,timestamp=4407545384.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4407545393.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.64,timestamp=4407545404.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4407545414.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.89,timestamp=4407545425.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4407545434.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.59,timestamp=4407545445.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4407545455.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.55,timestamp=4407545466.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4407545475.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=34.42,timestamp=4407545511.63,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4407545512.26,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.17,timestamp=4407545512.52,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 89, finish FWD profile ...
rank:89, bwd_subop num: 17, bwd_subop: ['trace_src_func=allreduce,duration=33.58,timestamp=4407545549.02,input__shape=[2048, 2, 12288],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=46.69,timestamp=4407545596.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.3,timestamp=4407545619.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.5,timestamp=4407545637.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.84,timestamp=4407545660.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.52,timestamp=4407545678.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.88,timestamp=4407545701.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.53,timestamp=4407545719.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.87,timestamp=4407545742.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.52,timestamp=4407545760.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.88,timestamp=4407545782.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.5,timestamp=4407545801.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.89,timestamp=4407545823.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.51,timestamp=4407545841.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.86,timestamp=4407545864.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.51,timestamp=4407545882.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.88,timestamp=4407545905.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:89, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f47d69ffa00 recvbuff 0x7f47d69ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f47d69ffa00,7f47d69ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:89,optimizer_step time: 41.174015045166016
rank:89, finish optimizer.step profile ...
rank:89, Before memory release - Allocated: 46189710336, Reserved: 64426606592
rank:89, trace log has been written to txt...
rank:89, finish release GPU memory ...
rank:89, After memory release - Allocated: 15547941888, Reserved: 45313163264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f41a66c0000 recvbuff 0x7f41a66c0000 count 77266944 datatype 7 op 0 root 0 comm 0xe59efc0 [nranks=1] stream 0xa1b3c10
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f41a66c0000,7f41a66c0000,77266944,7,0,0,0xe59efc0,0xa1b3c10)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 11): 1889906688
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1889906688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a79ffa00 recvbuff 0x7f44a79ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a79ffa00,7f44a79ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 90, finish warm up ...
rank_id = 90, input_tensor_shapes: [(2048, 2, 12288)]
rank:90,cuda fwd time: 199.41580200195312
rank:90, fwd_subop num: 19, fwd_subop: ['trace_src_func=allreduce,duration=8.37,timestamp=4407547649.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.1,timestamp=4407547659.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.46,timestamp=4407547669.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.8,timestamp=4407547679.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.13,timestamp=4407547689.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4407547699.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.1,timestamp=4407547709.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.28,timestamp=4407547719.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.6,timestamp=4407547730.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4407547740.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.59,timestamp=4407547750.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4407547760.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.53,timestamp=4407547771.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4407547781.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.5,timestamp=4407547791.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4407547801.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=34.39,timestamp=4407547837.48,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.04,timestamp=4407547838.12,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.16,timestamp=4407547838.38,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 90, finish FWD profile ...
rank:90, bwd_subop num: 17, bwd_subop: ['trace_src_func=allreduce,duration=33.58,timestamp=4407547873.92,input__shape=[2048, 2, 12288],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=46.68,timestamp=4407547921.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.3,timestamp=4407547944.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.48,timestamp=4407547962.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.84,timestamp=4407547985.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.44,timestamp=4407548003.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.8,timestamp=4407548026.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.51,timestamp=4407548044.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.87,timestamp=4407548067.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.5,timestamp=4407548085.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.89,timestamp=4407548107.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.48,timestamp=4407548125.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.86,timestamp=4407548148.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.52,timestamp=4407548166.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.88,timestamp=4407548189.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.51,timestamp=4407548207.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.86,timestamp=4407548230.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:90, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a79ff800 recvbuff 0x7f44a79ff800 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a79ff800,7f44a79ff800,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:90,optimizer_step time: 41.15865707397461
rank:90, finish optimizer.step profile ...
rank:90, Before memory release - Allocated: 46189710336, Reserved: 64426606592
rank:90, trace log has been written to txt...
rank:90, finish release GPU memory ...
rank:90, After memory release - Allocated: 15547941888, Reserved: 37652267008
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffc00 recvbuff 0x7f4aec9ffc00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffc00,7f4aec9ffc00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffe00 recvbuff 0x7f4aec9ffe00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffe00,7f4aec9ffe00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a79ff800 recvbuff 0x7f44a79ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a79ff800,7f44a79ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4194000000 recvbuff 0x7f4194000000 count 77266944 datatype 7 op 0 root 0 comm 0xe59efc0 [nranks=1] stream 0xa1b3c10
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4194000000,7f4194000000,77266944,7,0,0,0xe59efc0,0xa1b3c10)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 11): 1889906688
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1889906688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a79ffa00 recvbuff 0x7f44a79ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a79ffa00,7f44a79ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 91, finish warm up ...
rank_id = 91, input_tensor_shapes: [(2048, 2, 12288)]
rank:91,cuda fwd time: 203.1585235595703
rank:91, fwd_subop num: 19, fwd_subop: ['trace_src_func=allreduce,duration=7.12,timestamp=4407549954.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.18,timestamp=4407549964.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.24,timestamp=4407549974.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.23,timestamp=4407549984.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.28,timestamp=4407549994.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.23,timestamp=4407550004.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.88,timestamp=4407550014.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.21,timestamp=4407550024.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.85,timestamp=4407550034.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.86,timestamp=4407550044.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.79,timestamp=4407550054.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.88,timestamp=4407550064.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.79,timestamp=4407550075.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.9,timestamp=4407550084.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.37,timestamp=4407550095.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.15,timestamp=4407550105.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=34.34,timestamp=4407550141.75,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4407550147.0,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.17,timestamp=4407550147.26,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 91, finish FWD profile ...
rank:91, bwd_subop num: 17, bwd_subop: ['trace_src_func=allreduce,duration=33.58,timestamp=4407550183.8,input__shape=[2048, 2, 12288],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=46.64,timestamp=4407550231.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.26,timestamp=4407550254.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.44,timestamp=4407550272.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.85,timestamp=4407550295.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.52,timestamp=4407550313.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.87,timestamp=4407550336.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.49,timestamp=4407550354.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.87,timestamp=4407550376.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.51,timestamp=4407550395.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.85,timestamp=4407550417.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.51,timestamp=4407550435.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.83,timestamp=4407550458.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.51,timestamp=4407550476.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.86,timestamp=4407550499.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.5,timestamp=4407550517.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.86,timestamp=4407550540.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:91, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a79ff800 recvbuff 0x7f44a79ff800 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a79ff800,7f44a79ff800,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:91,optimizer_step time: 41.11769485473633
rank:91, finish optimizer.step profile ...
rank:91, Before memory release - Allocated: 46189710336, Reserved: 64407732224
rank:91, trace log has been written to txt...
rank:91, finish release GPU memory ...
rank:91, After memory release - Allocated: 15547941888, Reserved: 37652267008
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f41a66c0000 recvbuff 0x7f41a66c0000 count 77266944 datatype 7 op 0 root 0 comm 0xe59efc0 [nranks=1] stream 0xa1b3c10
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f41a66c0000,7f41a66c0000,77266944,7,0,0,0xe59efc0,0xa1b3c10)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 11): 1889906688
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1889906688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec8ed000 recvbuff 0x7f4aec8ed000 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec8ed000,7f4aec8ed000,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 92, finish warm up ...
rank_id = 92, input_tensor_shapes: [(2048, 2, 12288)]
rank:92,cuda fwd time: 197.501953125
rank:92, fwd_subop num: 19, fwd_subop: ['trace_src_func=allreduce,duration=7.12,timestamp=4407552281.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.19,timestamp=4407552291.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4407552301.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.24,timestamp=4407552311.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4407552321.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.24,timestamp=4407552331.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.91,timestamp=4407552341.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.24,timestamp=4407552351.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.91,timestamp=4407552362.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.94,timestamp=4407552371.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.95,timestamp=4407552382.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.94,timestamp=4407552392.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.94,timestamp=4407552402.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.95,timestamp=4407552412.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.55,timestamp=4407552422.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4407552432.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=34.47,timestamp=4407552468.37,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4407552468.96,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.17,timestamp=4407552469.21,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 92, finish FWD profile ...
rank:92, bwd_subop num: 17, bwd_subop: ['trace_src_func=allreduce,duration=33.58,timestamp=4407552504.72,input__shape=[2048, 2, 12288],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=45.48,timestamp=4407552552.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.25,timestamp=4407552575.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.48,timestamp=4407552593.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.82,timestamp=4407552616.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.51,timestamp=4407552634.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.84,timestamp=4407552656.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.52,timestamp=4407552675.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.85,timestamp=4407552697.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.52,timestamp=4407552715.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.87,timestamp=4407552738.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.52,timestamp=4407552756.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.86,timestamp=4407552779.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.49,timestamp=4407552797.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.86,timestamp=4407552820.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.51,timestamp=4407552838.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.89,timestamp=4407552861.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:92, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f45189ffa00 recvbuff 0x7f45189ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f45189ffa00,7f45189ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:92,optimizer_step time: 41.11155319213867
rank:92, finish optimizer.step profile ...
rank:92, Before memory release - Allocated: 46189710336, Reserved: 64407732224
rank:92, trace log has been written to txt...
rank:92, finish release GPU memory ...
rank:92, After memory release - Allocated: 15547941888, Reserved: 37652267008
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffc00 recvbuff 0x7f4aec9ffc00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffc00,7f4aec9ffc00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffe00 recvbuff 0x7f4aec9ffe00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffe00,7f4aec9ffe00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a79fe000 recvbuff 0x7f44a79fe000 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a79fe000,7f44a79fe000,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4194000000 recvbuff 0x7f4194000000 count 77266944 datatype 7 op 0 root 0 comm 0xe59efc0 [nranks=1] stream 0xa1b3c10
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4194000000,7f4194000000,77266944,7,0,0,0xe59efc0,0xa1b3c10)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 11): 1889906688
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1889906688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a79ff800 recvbuff 0x7f44a79ff800 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a79ff800,7f44a79ff800,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 93, finish warm up ...
rank_id = 93, input_tensor_shapes: [(2048, 2, 12288)]
rank:93,cuda fwd time: 197.49273681640625
rank:93, fwd_subop num: 19, fwd_subop: ['trace_src_func=allreduce,duration=7.14,timestamp=4407554575.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.17,timestamp=4407554584.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4407554595.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.23,timestamp=4407554604.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4407554615.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.22,timestamp=4407554624.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.98,timestamp=4407554635.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.21,timestamp=4407554644.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.94,timestamp=4407554655.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.95,timestamp=4407554665.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.94,timestamp=4407554675.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.93,timestamp=4407554685.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.92,timestamp=4407554695.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.95,timestamp=4407554705.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.46,timestamp=4407554716.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.43,timestamp=4407554725.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=34.44,timestamp=4407554761.45,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4407554762.04,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.17,timestamp=4407554762.29,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 93, finish FWD profile ...
rank:93, bwd_subop num: 17, bwd_subop: ['trace_src_func=allreduce,duration=33.58,timestamp=4407554797.82,input__shape=[2048, 2, 12288],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=46.66,timestamp=4407554845.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.39,timestamp=4407554868.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.52,timestamp=4407554886.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.85,timestamp=4407554909.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.5,timestamp=4407554927.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.88,timestamp=4407554950.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.5,timestamp=4407554968.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.86,timestamp=4407554990.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.52,timestamp=4407555009.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.83,timestamp=4407555031.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.39,timestamp=4407555049.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.79,timestamp=4407555072.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.5,timestamp=4407555090.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.75,timestamp=4407555113.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.49,timestamp=4407555131.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.84,timestamp=4407555154.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:93, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f45189ff800 recvbuff 0x7f45189ff800 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f45189ff800,7f45189ff800,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:93,optimizer_step time: 41.16172790527344
rank:93, finish optimizer.step profile ...
rank:93, Before memory release - Allocated: 46189710336, Reserved: 64407732224
rank:93, trace log has been written to txt...
rank:93, finish release GPU memory ...
rank:93, After memory release - Allocated: 15547941888, Reserved: 37652267008
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ff800 recvbuff 0x7f4aec9ff800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ff800,7f4aec9ff800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffa00 recvbuff 0x7f4aec9ffa00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffa00,7f4aec9ffa00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f41a66c0000 recvbuff 0x7f41a66c0000 count 77266944 datatype 7 op 0 root 0 comm 0xe59efc0 [nranks=1] stream 0xa1b3c10
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f41a66c0000,7f41a66c0000,77266944,7,0,0,0xe59efc0,0xa1b3c10)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 11): 1889906688
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1889906688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f479a8ed000 recvbuff 0x7f479a8ed000 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f479a8ed000,7f479a8ed000,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 94, finish warm up ...
rank_id = 94, input_tensor_shapes: [(2048, 2, 12288)]
rank:94,cuda fwd time: 197.18655395507812
rank:94, fwd_subop num: 19, fwd_subop: ['trace_src_func=allreduce,duration=7.15,timestamp=4407556942.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.16,timestamp=4407556952.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4407556962.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.21,timestamp=4407556972.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4407556982.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.23,timestamp=4407556992.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.98,timestamp=4407557002.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.21,timestamp=4407557012.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.91,timestamp=4407557022.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.93,timestamp=4407557032.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.95,timestamp=4407557043.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.94,timestamp=4407557052.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.92,timestamp=4407557063.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.95,timestamp=4407557073.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.56,timestamp=4407557083.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.43,timestamp=4407557093.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=34.46,timestamp=4407557128.89,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4407557129.46,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.17,timestamp=4407557129.72,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 94, finish FWD profile ...
rank:94, bwd_subop num: 17, bwd_subop: ['trace_src_func=allreduce,duration=33.58,timestamp=4407557165.25,input__shape=[2048, 2, 12288],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=46.67,timestamp=4407557212.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.36,timestamp=4407557235.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.43,timestamp=4407557253.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.85,timestamp=4407557276.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.5,timestamp=4407557294.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.86,timestamp=4407557317.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.51,timestamp=4407557335.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.88,timestamp=4407557358.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.51,timestamp=4407557376.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.87,timestamp=4407557399.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.47,timestamp=4407557417.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.89,timestamp=4407557439.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.5,timestamp=4407557457.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.87,timestamp=4407557480.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.5,timestamp=4407557498.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.87,timestamp=4407557521.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:94, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f479a8ed000 recvbuff 0x7f479a8ed000 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f479a8ed000,7f479a8ed000,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:94,optimizer_step time: 41.046016693115234
rank:94, finish optimizer.step profile ...
rank:94, Before memory release - Allocated: 46189710336, Reserved: 64407732224
rank:94, trace log has been written to txt...
rank:94, finish release GPU memory ...
rank:94, After memory release - Allocated: 15547941888, Reserved: 37652267008
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f4d533ee850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffc00 recvbuff 0x7f4aec9ffc00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffc00,7f4aec9ffc00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4aec9ffe00 recvbuff 0x7f4aec9ffe00 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4aec9ffe00,7f4aec9ffe00,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f44a79fc800 recvbuff 0x7f44a79fc800 count 1 datatype 1 op 0 root 0 comm 0xa19d050 [nranks=1] stream 0x99ad340
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f44a79fc800,7f44a79fc800,1,1,0,0,0xa19d050,0x99ad340)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f4194000000 recvbuff 0x7f4194000000 count 77266944 datatype 7 op 0 root 0 comm 0xe59efc0 [nranks=1] stream 0xa1b3c10
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f4194000000,7f4194000000,77266944,7,0,0,0xe59efc0,0xa1b3c10)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 11): 1889906688
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1889906688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f4d53449820>)
> learning rate decay style: cosine
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f479a927c00 recvbuff 0x7f479a927c00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f479a927c00,7f479a927c00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank_id = 95, finish warm up ...
rank_id = 95, input_tensor_shapes: [(2048, 2, 12288)]
rank:95,cuda fwd time: 200.60365295410156
rank:95, fwd_subop num: 19, fwd_subop: ['trace_src_func=allreduce,duration=7.1,timestamp=4407559239.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.16,timestamp=4407559249.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.23,timestamp=4407559260.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.21,timestamp=4407559269.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4407559279.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.22,timestamp=4407559289.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.95,timestamp=4407559299.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.23,timestamp=4407559309.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.69,timestamp=4407559320.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.91,timestamp=4407559329.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.87,timestamp=4407559340.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.91,timestamp=4407559350.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.9,timestamp=4407559360.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.94,timestamp=4407559370.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.48,timestamp=4407559380.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4407559390.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=34.45,timestamp=4407559426.34,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4407559430.1,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.17,timestamp=4407559430.36,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 95, finish FWD profile ...
rank:95, bwd_subop num: 17, bwd_subop: ['trace_src_func=allreduce,duration=33.57,timestamp=4407559466.66,input__shape=[2048, 2, 12288],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=46.67,timestamp=4407559514.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.4,timestamp=4407559537.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.42,timestamp=4407559555.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.84,timestamp=4407559577.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.51,timestamp=4407559596.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.85,timestamp=4407559618.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.49,timestamp=4407559636.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.87,timestamp=4407559659.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.52,timestamp=4407559677.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.87,timestamp=4407559700.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.5,timestamp=4407559718.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.87,timestamp=4407559741.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.51,timestamp=4407559759.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.87,timestamp=4407559782.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=17.51,timestamp=4407559800.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=21.86,timestamp=4407559823.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:95, finish BWD profile ...
proj187:3264676:3264676 NCCL CALL ncclGroupStart()
proj187:3264676:3264676 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f45189ffa00 recvbuff 0x7f45189ffa00 count 1 datatype 7 op 0 root 0 comm 0x1a6dea30 [nranks=1] stream 0xa1bcb90
proj187:3264676:3264676 NCCL CALL ncclAllReduce(7f45189ffa00,7f45189ffa00,1,7,0,0,0x1a6dea30,0xa1bcb90)
proj187:3264676:3264676 NCCL CALL ncclGroupEnd()
rank:95,optimizer_step time: 41.07878494262695
rank:95, finish optimizer.step profile ...
rank:95, Before memory release - Allocated: 46189710336, Reserved: 64407732224
rank:95, trace log has been written to txt...
rank:95, finish release GPU memory ...
rank:95, After memory release - Allocated: 15547941888, Reserved: 37652267008
