using world size: 1, data-parallel size: 1, context-parallel size: 1 tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
using torch.float32 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.999
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. True
  add_position_embedding .......................... True
  add_qkv_bias .................................... False
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  apply_rope_fusion ............................... True
  async_tensor_model_parallel_allreduce ........... True
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  auto_detect_ckpt_format ......................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  bias_swiglu_fusion .............................. True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  check_for_nan_in_loss_and_grad .................. True
  check_weight_hash_across_dp_replicas_interval ... None
  ckpt_fully_parallel_save ........................ False
  ckpt_step ....................................... None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  clone_scatter_output_in_embedding ............... True
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_size ........................... 1
  create_attention_mask_in_dataloader ............. True
  data_cache_path ................................. None
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 1
  data_path ....................................... ['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  ddp_bucket_size ................................. None
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  decoupled_lr .................................... None
  decoupled_min_lr ................................ None
  delay_grad_reduce ............................... True
  delay_param_gather .............................. False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  disable_straggler_on_startup .................... False
  dist_ckpt_format ................................ torch_dist
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 10
  do_trace ........................................ True
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  enable_one_logger ............................... False
  encoder_num_layers .............................. 96
  encoder_seq_length .............................. 2048
  end_weight_decay ................................ 0.01
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_model_parallel_size ...................... 1
  fake_dp ......................................... 1
  fake_gpus_per_node .............................. 8
  fake_local_rank ................................. 0
  fake_pp ......................................... 16
  fake_tp ......................................... 6
  fake_world_size ................................. 96
  fake_wrank ...................................... 0
  ffn_hidden_size ................................. 49152
  finetune ........................................ False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 2
  gradient_accumulation_fusion .................... True
  group_query_attention ........................... False
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 12288
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  is_scaling_mode ................................. True
  iter_per_epoch .................................. 1250
  kv_channels ..................................... 128
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 100
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_progress .................................... False
  log_straggler ................................... False
  log_throughput .................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.00015
  lr_decay_iters .................................. 320000
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. 0.01
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  main_tokenizer_type ............................. GPT2BPETokenizer
  make_vocab_size_divisible_by .................... 128
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 2048
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/gpt2-merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_bin_files .................................. True
  mock_data ....................................... False
  moe_aux_loss_coeff .............................. 0.0
  moe_grouped_gemm ................................ False
  moe_input_jitter_eps ............................ None
  moe_per_layer_logging ........................... False
  moe_router_load_balancing_type .................. aux_loss
  moe_router_topk ................................. 2
  moe_token_dispatcher_type ....................... allgather
  moe_token_dropping .............................. False
  moe_z_loss_coeff ................................ None
  nccl_communicator_config_path ................... None
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... LayerNorm
  nsight_start .................................... 10
  num_attention_heads ............................. 96
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... None
  num_layers ...................................... 96
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 1
  num_workers ..................................... 2
  one_logger_entity ............................... hwinf_dcm
  one_logger_project .............................. e2e-tracking
  one_logger_run_name ............................. None
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  overlap_param_gather ............................ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.float32
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... learned_absolute
  pretrained_checkpoint ........................... None
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  qk_layernorm .................................... False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... None
  recompute_method ................................ None
  recompute_num_layers ............................ None
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_attention_gate ............................ 1
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_project_dir ............................... None
  retro_verify_neighbor_count ..................... True
  rotary_interleaved .............................. False
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 2048
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  spec ............................................ None
  split ........................................... 949,50,1
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.01
  straggler_ctrlr_port ............................ 65535
  straggler_minmax_count .......................... 1
  swiglu .......................................... False
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  test_mode ....................................... False
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. None
  tokenizer_type .................................. GPT2BPETokenizer
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. False
  tp_comm_overlap_ag .............................. True
  tp_comm_overlap_cfg ............................. None
  tp_comm_overlap_rs .............................. True
  tp_comm_overlap_rs_dgrad ........................ False
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  trace_start ..................................... 10
  train_data_path ................................. None
  train_iters ..................................... 10
  train_samples ................................... None
  transformer_impl ................................ transformer_engine
  transformer_pipeline_model_parallel_size ........ 1
  untie_embeddings_and_output_weights ............. False
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_dist_ckpt ................................... False
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. False
  use_mcore_models ................................ True
  use_one_sent_docs ............................... False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  use_tp_pp_dp_mapping ............................ False
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/gpt2-vocab.json
  vocab_size ...................................... 3200
  wandb_exp_name .................................. 
  wandb_project ................................... 
  wandb_save_dir .................................. 
  weight_decay .................................... 0.01
  weight_decay_incr_style ......................... constant
  world_size ...................................... 1
  yaml_cfg ........................................ None
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 1
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initialized tensor model parallel with size 1
> initialized pipeline model parallel with size 1
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/datasets'
make: Nothing to be done for 'default'.
make: Leaving directory '/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.059 seconds
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
> compiling and loading fused kernels ...
proj187:3256792:3256792 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ens81f0
proj187:3256792:3256792 [0] NCCL INFO NCCL_SOCKET_IFNAME set to ens81f0
proj187:3256792:3256792 [0] NCCL INFO Bootstrap : Using ens81f0:192.168.50.187<0>
proj187:3256792:3256792 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
proj187:3256792:3256792 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
proj187:3256792:3256792 NCCL CALL ncclGetUniqueId(0x9ea2541a94d2c468)
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.18.6+cuda12.1
proj187:3256792:3256792 [0] NCCL INFO init.cc:1584 Cuda Host Alloc Size 4 pointer 0x7f2407400000
proj187:3256792:3257144 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
proj187:3256792:3257144 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ens81f0
proj187:3256792:3257144 [0] NCCL INFO NET/Socket : Using [0]ens81f0:192.168.50.187<0>
proj187:3256792:3257144 [0] NCCL INFO Using network Socket
proj187:3256792:3257144 [0] NCCL INFO comm 0x9a487d0 rank 0 nranks 1 cudaDev 0 nvmlDev 1 busId 2a000 commId 0x9ea2541a94d2c468 - Init START
proj187:3256792:3257144 [0] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'ens81f0'
proj187:3256792:3257144 [0] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
proj187:3256792:3257144 [0] NCCL INFO CPU/0 (1/1/2)
proj187:3256792:3257144 [0] NCCL INFO + PCI[24.0] - PCI/21000 (1000c01010000000)
proj187:3256792:3257144 [0] NCCL INFO               + PCI[24.0] - PCI/28000 (1000c01010de13b8)
proj187:3256792:3257144 [0] NCCL INFO                             + PCI[24.0] - GPU/2A000 (0)
proj187:3256792:3257144 [0] NCCL INFO                                           + NVL[160.0] - NVS/0
proj187:3256792:3257144 [0] NCCL INFO + PCI[3.0] - NIC/17000
proj187:3256792:3257144 [0] NCCL INFO ==========================================
proj187:3256792:3257144 [0] NCCL INFO GPU/2A000 :GPU/2A000 (0/5000.000000/LOC) NVS/0 (1/160.000000/NVL) CPU/0 (3/24.000000/PHB) 
proj187:3256792:3257144 [0] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
proj187:3256792:3257144 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 40.000000/40.000000, type LOC/PIX, sameChannels 1
proj187:3256792:3257144 [0] NCCL INFO  0 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO  1 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO  2 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO  3 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO  4 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO  5 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO  6 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO  7 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO  8 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO  9 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO 10 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO 11 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO 12 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO 13 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO 14 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO 15 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 40.000000/40.000000, type LOC/PIX, sameChannels 1
proj187:3256792:3257144 [0] NCCL INFO  0 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO  1 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO  2 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO  3 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO  4 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO  5 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO  6 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO  7 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO  8 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO  9 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO 10 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO 11 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO 12 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO 13 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO 14 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO 15 : GPU/0
proj187:3256792:3257144 [0] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257144 [0] NCCL INFO Channel 00/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 01/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 02/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 03/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 04/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 05/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 06/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 07/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 08/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 09/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 10/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 11/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 12/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 13/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 14/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 15/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 16/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 17/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 18/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 19/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 20/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 21/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 22/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 23/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 24/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 25/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 26/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 27/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 28/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 29/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 30/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Channel 31/32 :    0
proj187:3256792:3257144 [0] NCCL INFO Ring 00 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 01 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 02 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 03 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 04 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 05 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 06 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 07 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 08 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 09 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 10 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 11 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 12 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 13 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 14 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 15 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 16 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 17 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 18 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 19 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 20 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 21 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 22 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 23 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 24 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 25 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 26 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 27 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 28 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 29 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 30 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Ring 31 : 0 -> 0 -> 0
proj187:3256792:3257144 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
proj187:3256792:3257144 [0] NCCL INFO P2P Chunksize set to 131072
proj187:3256792:3257144 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c00000
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c00200
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c00400
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c00600
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c00800
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c00a00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c00c00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c00e00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c01000
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c01200
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c01400
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c01600
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c01800
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c01a00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c01c00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c01e00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c02000
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c02200
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c02400
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c02600
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c02800
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c02a00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c02c00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c02e00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c03000
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c03200
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c03400
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c03600
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c03800
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c03a00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c03c00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c03e00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c04000
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c04200
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c04400
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c04600
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c04800
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c04a00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c04c00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c04e00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c05000
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c05200
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c05400
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c05600
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c05800
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c05a00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c05c00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c05e00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c06000
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c06200
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c06400
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c06600
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c06800
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c06a00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c06c00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c06e00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c07000
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c07200
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c07400
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c07600
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c07800
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c07a00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c07c00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c07e00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c08000
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c08200
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c08400
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c08600
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c08800
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c08a00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c08c00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c08e00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c09000
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c09200
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c09400
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c09600
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c09800
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c09a00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c09c00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c09e00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c0a000
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c0a200
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c0a400
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c0a600
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c0a800
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c0aa00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c0ac00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c0ae00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c0b000
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c0b200
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c0b400
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c0b600
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c0b800
proj187:3256792:3257144 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c0ba00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c0bc00
proj187:3256792:3257144 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c0be00
proj187:3256792:3257144 [0] NCCL INFO Connected all rings
proj187:3256792:3257144 [0] NCCL INFO Connected all trees
proj187:3256792:3257144 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
proj187:3256792:3257145 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x7f23ec002f20
proj187:3256792:3257145 [0] NCCL INFO Allocated 4194660 bytes of shared memory in /dev/shm/nccl-8HD2Rj
proj187:3256792:3257145 [0] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
proj187:3256792:3257145 [0] NCCL INFO proxyProgressAsync opId=0x7f23da18cba0 op.type=1 op.reqBuff=0x7f23ec000bb0 op.respSize=16 done
proj187:3256792:3257144 [0] NCCL INFO ncclPollProxyResponse Received new opId=0x7f23da18cba0
proj187:3256792:3257144 [0] NCCL INFO recvOpId=0x7f23da18cba0 matches expected opId=0x7f23da18cba0
proj187:3256792:3257145 [0] NCCL INFO Received and initiated operation=Init res=0
proj187:3256792:3257144 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x7f23ec003160
proj187:3256792:3257145 [0] NCCL INFO transport/net.cc:446 Cuda Alloc Size 67108864 pointer 0x7f23cc000000
proj187:3256792:3257145 [0] NCCL INFO proxyProgressAsync opId=0x7f23da18cba0 op.type=2 op.reqBuff=0x7f23ec005cc0 op.respSize=0 done
proj187:3256792:3257144 [0] NCCL INFO ncclPollProxyResponse Received new opId=0x7f23da18cba0
proj187:3256792:3257145 [0] NCCL INFO Received and initiated operation=SharedInit res=0
proj187:3256792:3257144 [0] NCCL INFO recvOpId=0x7f23da18cba0 matches expected opId=0x7f23da18cba0
proj187:3256792:3257144 [0] NCCL INFO init.cc:387 Cuda Alloc Size 7728 pointer 0x7f23d7c0c000
proj187:3256792:3257144 [0] NCCL INFO init.cc:412 Cuda Host Alloc Size 33554432 pointer 0x7f23ca000000
proj187:3256792:3257144 [0] NCCL INFO init.cc:418 Cuda Host Alloc Size 128 pointer 0x7f2407400200
proj187:3256792:3257144 NCCL CALL ncclCommInitRank(0x9a487d0, 1, 0x9ea2541a94d2c468, 0, 0)
proj187:3256792:3257144 [0] NCCL INFO comm 0x9a487d0 rank 0 nranks 1 cudaDev 0 nvmlDev 1 busId 2a000 commId 0x9ea2541a94d2c468 - Init COMPLETE
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f2407200200 recvbuff 0x7f2407200200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f2407200200,7f2407200200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f2407200000 recvbuff 0x7f2407200000 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f2407200000,7f2407200000,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
>>> done with compiling and loading fused kernels. Compilation time: 0.884 seconds
/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/training/initialize.py:405: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400412039/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f2407200000 recvbuff 0x7f2407200000 count 1 datatype 8 op 3 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f2407200000,7f2407200000,1,8,3,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
time to initialize megatron (seconds): 2.101
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f2407200200 recvbuff 0x7f2407200200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f2407200200,7f2407200200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
[after megatron is initialized] datetime: 2024-09-22 16:27:14 
mpu_info:MPUInfo:
	dp_size=1
	tp_size=6
	pp_size=16
	mp_size=96
	world_size=96
	dp_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95]]
	pp_groups=[[0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66, 72, 78, 84, 90], [1, 7, 13, 19, 25, 31, 37, 43, 49, 55, 61, 67, 73, 79, 85, 91], [2, 8, 14, 20, 26, 32, 38, 44, 50, 56, 62, 68, 74, 80, 86, 92], [3, 9, 15, 21, 27, 33, 39, 45, 51, 57, 63, 69, 75, 81, 87, 93], [4, 10, 16, 22, 28, 34, 40, 46, 52, 58, 64, 70, 76, 82, 88, 94], [5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83, 89, 95]]
	tp_groups=[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35], [36, 37, 38, 39, 40, 41], [42, 43, 44, 45, 46, 47], [48, 49, 50, 51, 52, 53], [54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65], [66, 67, 68, 69, 70, 71], [72, 73, 74, 75, 76, 77], [78, 79, 80, 81, 82, 83], [84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95]]
	mp_groups=[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]]
	ep_groups=[[0, 90], [1, 91], [2, 92], [3, 93], [4, 94], [5, 95]]
	pep_groups=[[0], [1], [2], [3], [4], [5]]
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f2407200400 recvbuff 0x7f2407200400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f2407200400,7f2407200400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f2407200200 recvbuff 0x7f2407200200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f2407200200,7f2407200200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f2407200400 recvbuff 0x7f2407200400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f2407200400,7f2407200400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3256792:3256792 NCCL CALL ncclGetUniqueId(0x88763314a789493c)
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO init.cc:1584 Cuda Host Alloc Size 4 pointer 0x7f2407470400
proj187:3256792:3257400 [0] NCCL INFO Using network Socket
proj187:3256792:3257400 [0] NCCL INFO comm 0xd317670 rank 0 nranks 1 cudaDev 0 nvmlDev 1 busId 2a000 commId 0x88763314a789493c - Init START
proj187:3256792:3257400 [0] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'ens81f0'
proj187:3256792:3257400 [0] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
proj187:3256792:3257400 [0] NCCL INFO CPU/0 (1/1/2)
proj187:3256792:3257400 [0] NCCL INFO + PCI[24.0] - PCI/21000 (1000c01010000000)
proj187:3256792:3257400 [0] NCCL INFO               + PCI[24.0] - PCI/28000 (1000c01010de13b8)
proj187:3256792:3257400 [0] NCCL INFO                             + PCI[24.0] - GPU/2A000 (0)
proj187:3256792:3257400 [0] NCCL INFO                                           + NVL[160.0] - NVS/0
proj187:3256792:3257400 [0] NCCL INFO + PCI[3.0] - NIC/17000
proj187:3256792:3257400 [0] NCCL INFO ==========================================
proj187:3256792:3257400 [0] NCCL INFO GPU/2A000 :GPU/2A000 (0/5000.000000/LOC) NVS/0 (1/160.000000/NVL) CPU/0 (3/24.000000/PHB) 
proj187:3256792:3257400 [0] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
proj187:3256792:3257400 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 40.000000/40.000000, type LOC/PIX, sameChannels 1
proj187:3256792:3257400 [0] NCCL INFO  0 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO  1 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO  2 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO  3 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO  4 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO  5 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO  6 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO  7 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO  8 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO  9 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO 10 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO 11 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO 12 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO 13 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO 14 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO 15 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 40.000000/40.000000, type LOC/PIX, sameChannels 1
proj187:3256792:3257400 [0] NCCL INFO  0 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO  1 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO  2 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO  3 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO  4 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO  5 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO  6 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO  7 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO  8 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO  9 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO 10 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO 11 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO 12 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO 13 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO 14 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO 15 : GPU/0
proj187:3256792:3257400 [0] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257400 [0] NCCL INFO Channel 00/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 01/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 02/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 03/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 04/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 05/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 06/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 07/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 08/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 09/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 10/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 11/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 12/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 13/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 14/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 15/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 16/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 17/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 18/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 19/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 20/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 21/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 22/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 23/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 24/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 25/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 26/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 27/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 28/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 29/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 30/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Channel 31/32 :    0
proj187:3256792:3257400 [0] NCCL INFO Ring 00 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 01 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 02 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 03 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 04 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 05 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 06 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 07 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 08 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 09 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 10 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 11 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 12 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 13 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 14 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 15 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 16 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 17 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 18 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 19 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 20 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 21 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 22 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 23 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 24 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 25 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 26 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 27 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 28 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 29 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 30 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Ring 31 : 0 -> 0 -> 0
proj187:3256792:3257400 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
proj187:3256792:3257400 [0] NCCL INFO P2P Chunksize set to 131072
proj187:3256792:3257400 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c0e000
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c0e200
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c0e400
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c0e600
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c0e800
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c0ea00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c0ec00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c0ee00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c0f000
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c0f200
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c0f400
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c0f600
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c0f800
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c0fa00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c0fc00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c0fe00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c10000
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c10200
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c10400
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c10600
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c10800
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c10a00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c10c00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c10e00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c11000
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c11200
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c11400
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c11600
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c11800
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c11a00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c11c00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c11e00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c12000
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c12200
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c12400
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c12600
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c12800
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c12a00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c12c00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c12e00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c13000
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c13200
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c13400
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c13600
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c13800
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c13a00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c13c00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c13e00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c14000
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c14200
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c14400
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c14600
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c14800
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c14a00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c14c00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c14e00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c15000
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c15200
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c15400
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c15600
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c15800
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c15a00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c15c00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c15e00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c16000
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c16200
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c16400
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c16600
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c16800
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c16a00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c16c00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c16e00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c17000
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c17200
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c17400
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c17600
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c17800
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c17a00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c17c00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c17e00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c18000
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c18200
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c18400
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c18600
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c18800
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c18a00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c18c00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c18e00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c19000
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c19200
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c19400
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c19600
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c19800
proj187:3256792:3257400 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c19a00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c19c00
proj187:3256792:3257400 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c19e00
proj187:3256792:3257400 [0] NCCL INFO Connected all rings
proj187:3256792:3257400 [0] NCCL INFO Connected all trees
proj187:3256792:3257400 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
proj187:3256792:3257401 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x7f23a4002f20
proj187:3256792:3257401 [0] NCCL INFO Allocated 4194660 bytes of shared memory in /dev/shm/nccl-ghaSTo
proj187:3256792:3257401 [0] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
proj187:3256792:3257401 [0] NCCL INFO proxyProgressAsync opId=0x7f21b8058c40 op.type=1 op.reqBuff=0x7f23a4000bb0 op.respSize=16 done
proj187:3256792:3257400 [0] NCCL INFO ncclPollProxyResponse Received new opId=0x7f21b8058c40
proj187:3256792:3257400 [0] NCCL INFO recvOpId=0x7f21b8058c40 matches expected opId=0x7f21b8058c40
proj187:3256792:3257401 [0] NCCL INFO Received and initiated operation=Init res=0
proj187:3256792:3257400 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x7f23a4003160
proj187:3256792:3257401 [0] NCCL INFO transport/net.cc:446 Cuda Alloc Size 67108864 pointer 0x7f21ac000000
proj187:3256792:3257401 [0] NCCL INFO proxyProgressAsync opId=0x7f21b8058c40 op.type=2 op.reqBuff=0x7f23a4005cc0 op.respSize=0 done
proj187:3256792:3257400 [0] NCCL INFO ncclPollProxyResponse Received new opId=0x7f21b8058c40
proj187:3256792:3257401 [0] NCCL INFO Received and initiated operation=SharedInit res=0
proj187:3256792:3257400 [0] NCCL INFO recvOpId=0x7f21b8058c40 matches expected opId=0x7f21b8058c40
proj187:3256792:3257400 [0] NCCL INFO init.cc:387 Cuda Alloc Size 7728 pointer 0x7f23d7c1a000
proj187:3256792:3257400 [0] NCCL INFO init.cc:412 Cuda Host Alloc Size 33554432 pointer 0x7f2405000000
proj187:3256792:3257400 [0] NCCL INFO init.cc:418 Cuda Host Alloc Size 128 pointer 0x7f2407470600
proj187:3256792:3257400 NCCL CALL ncclCommInitRank(0xd317670, 1, 0x88763314a789493c, 0, 0)
proj187:3256792:3257400 [0] NCCL INFO comm 0xd317670 rank 0 nranks 1 cudaDev 0 nvmlDev 1 busId 2a000 commId 0x88763314a789493c - Init COMPLETE
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f2378000000 recvbuff 0x7f2378000000 count 103022592 datatype 7 op 0 root 0 comm 0xd317670 [nranks=1] stream 0x9a5f390
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f2378000000,7f2378000000,103022592,7,0,0,0xd317670,0x9a5f390)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1940656128
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1940656128 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGetUniqueId(0xef1e73e112c2fb5c)
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO init.cc:1584 Cuda Host Alloc Size 4 pointer 0x7f2407470800
proj187:3256792:3257511 [0] NCCL INFO Using network Socket
proj187:3256792:3257511 [0] NCCL INFO comm 0x19f6c600 rank 0 nranks 1 cudaDev 0 nvmlDev 1 busId 2a000 commId 0xef1e73e112c2fb5c - Init START
proj187:3256792:3257511 [0] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'ens81f0'
proj187:3256792:3257511 [0] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
proj187:3256792:3257511 [0] NCCL INFO CPU/0 (1/1/2)
proj187:3256792:3257511 [0] NCCL INFO + PCI[24.0] - PCI/21000 (1000c01010000000)
proj187:3256792:3257511 [0] NCCL INFO               + PCI[24.0] - PCI/28000 (1000c01010de13b8)
proj187:3256792:3257511 [0] NCCL INFO                             + PCI[24.0] - GPU/2A000 (0)
proj187:3256792:3257511 [0] NCCL INFO                                           + NVL[160.0] - NVS/0
proj187:3256792:3257511 [0] NCCL INFO + PCI[3.0] - NIC/17000
proj187:3256792:3257511 [0] NCCL INFO ==========================================
proj187:3256792:3257511 [0] NCCL INFO GPU/2A000 :GPU/2A000 (0/5000.000000/LOC) NVS/0 (1/160.000000/NVL) CPU/0 (3/24.000000/PHB) 
proj187:3256792:3257511 [0] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
proj187:3256792:3257511 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 40.000000/40.000000, type LOC/PIX, sameChannels 1
proj187:3256792:3257511 [0] NCCL INFO  0 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO  1 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO  2 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO  3 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO  4 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO  5 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO  6 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO  7 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO  8 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO  9 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO 10 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO 11 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO 12 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO 13 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO 14 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO 15 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 40.000000/40.000000, type LOC/PIX, sameChannels 1
proj187:3256792:3257511 [0] NCCL INFO  0 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO  1 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO  2 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO  3 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO  4 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO  5 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO  6 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO  7 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO  8 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO  9 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO 10 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO 11 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO 12 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO 13 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO 14 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO 15 : GPU/0
proj187:3256792:3257511 [0] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
proj187:3256792:3257511 [0] NCCL INFO Channel 00/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 01/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 02/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 03/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 04/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 05/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 06/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 07/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 08/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 09/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 10/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 11/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 12/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 13/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 14/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 15/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 16/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 17/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 18/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 19/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 20/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 21/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 22/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 23/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 24/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 25/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 26/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 27/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 28/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 29/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 30/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Channel 31/32 :    0
proj187:3256792:3257511 [0] NCCL INFO Ring 00 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 01 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 02 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 03 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 04 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 05 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 06 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 07 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 08 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 09 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 10 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 11 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 12 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 13 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 14 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 15 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 16 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 17 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 18 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 19 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 20 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 21 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 22 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 23 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 24 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 25 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 26 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 27 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 28 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 29 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 30 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Ring 31 : 0 -> 0 -> 0
proj187:3256792:3257511 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
proj187:3256792:3257511 [0] NCCL INFO P2P Chunksize set to 131072
proj187:3256792:3257511 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c5c800
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c5ca00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c5cc00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c5ce00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c5d000
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c5d200
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c5d400
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c5d600
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c5d800
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c5da00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c5dc00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c5de00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c5e000
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c5e200
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c5e400
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c5e600
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c5e800
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c5ea00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c5ec00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c5ee00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c5f000
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c5f200
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c5f400
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c5f600
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c5f800
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c5fa00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c5fc00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c5fe00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c60000
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c60200
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c60400
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c60600
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c60800
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c60a00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c60c00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c60e00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c61000
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c61200
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c61400
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c61600
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c61800
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c61a00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c61c00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c61e00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c62000
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c62200
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c62400
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c62600
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c62800
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c62a00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c62c00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c62e00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c63000
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c63200
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c63400
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c63600
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c63800
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c63a00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c63c00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c63e00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c64000
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c64200
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c64400
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c64600
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c64800
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c64a00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c64c00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c64e00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c65000
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c65200
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c65400
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c65600
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c65800
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c65a00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c65c00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c65e00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c66000
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c66200
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c66400
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c66600
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c66800
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c66a00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c66c00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c66e00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c67000
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c67200
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c67400
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c67600
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c67800
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c67a00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c67c00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c67e00
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c68000
proj187:3256792:3257511 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f23d7c68200
proj187:3256792:3257511 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f23d7c68400
proj187:3256792:3257511 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f23d7c68600
proj187:3256792:3257511 [0] NCCL INFO Connected all rings
proj187:3256792:3257511 [0] NCCL INFO Connected all trees
proj187:3256792:3257511 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
proj187:3256792:3257512 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x7f1bbc002f20
proj187:3256792:3257512 [0] NCCL INFO Allocated 4194660 bytes of shared memory in /dev/shm/nccl-iePgrU
proj187:3256792:3257512 [0] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
proj187:3256792:3257512 [0] NCCL INFO proxyProgressAsync opId=0x7f1bb0058c40 op.type=1 op.reqBuff=0x7f1bbc000bb0 op.respSize=16 done
proj187:3256792:3257511 [0] NCCL INFO ncclPollProxyResponse Received new opId=0x7f1bb0058c40
proj187:3256792:3257511 [0] NCCL INFO recvOpId=0x7f1bb0058c40 matches expected opId=0x7f1bb0058c40
proj187:3256792:3257512 [0] NCCL INFO Received and initiated operation=Init res=0
proj187:3256792:3257511 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x7f1bbc003160
proj187:3256792:3257512 [0] NCCL INFO transport/net.cc:446 Cuda Alloc Size 67108864 pointer 0x7f1bac000000
proj187:3256792:3257512 [0] NCCL INFO proxyProgressAsync opId=0x7f1bb0058c40 op.type=2 op.reqBuff=0x7f1bbc005cc0 op.respSize=0 done
proj187:3256792:3257511 [0] NCCL INFO ncclPollProxyResponse Received new opId=0x7f1bb0058c40
proj187:3256792:3257512 [0] NCCL INFO Received and initiated operation=SharedInit res=0
proj187:3256792:3257511 [0] NCCL INFO recvOpId=0x7f1bb0058c40 matches expected opId=0x7f1bb0058c40
proj187:3256792:3257511 [0] NCCL INFO init.cc:387 Cuda Alloc Size 7728 pointer 0x7f23d7c68800
proj187:3256792:3257511 [0] NCCL INFO init.cc:412 Cuda Host Alloc Size 33554432 pointer 0x7f1be2000000
proj187:3256792:3257511 [0] NCCL INFO init.cc:418 Cuda Host Alloc Size 128 pointer 0x7f2407470a00
proj187:3256792:3257511 NCCL CALL ncclCommInitRank(0x19f6c600, 1, 0xef1e73e112c2fb5c, 0, 0)
proj187:3256792:3257511 [0] NCCL INFO comm 0x19f6c600 rank 0 nranks 1 cudaDev 0 nvmlDev 1 busId 2a000 commId 0xef1e73e112c2fb5c - Init COMPLETE
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073ffe00 recvbuff 0x7f24073ffe00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073ffe00,7f24073ffe00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 0, finish warm up ...
rank_id = 0, input_tensor_shapes: []
rank:0,cuda fwd time: 167.10426330566406
rank:0, fwd_subop num: 13, fwd_subop: ['trace_src_func=_reduce,duration=0.48,timestamp=4407068120.47,input__shape=[2, 2048, 12288],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.24,timestamp=4407068135.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.31,timestamp=4407068148.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.08,timestamp=4407068161.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.51,timestamp=4407068174.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.64,timestamp=4407068188.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4407068202.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.1,timestamp=4407068216.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4407068230.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.82,timestamp=4407068244.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.84,timestamp=4407068257.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.18,timestamp=4407068272.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.78,timestamp=4407068285.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 0, finish FWD profile ...
rank:0, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.89,timestamp=4407068307.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.67,timestamp=4407068339.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.8,timestamp=4407068364.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.62,timestamp=4407068396.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.73,timestamp=4407068422.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.36,timestamp=4407068453.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.5,timestamp=4407068478.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.5,timestamp=4407068510.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.62,timestamp=4407068535.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.64,timestamp=4407068567.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.58,timestamp=4407068593.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.95,timestamp=4407068625.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:0, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fea00 recvbuff 0x7f24073fea00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fea00,7f24073fea00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:0,optimizer_step time: 52.63052749633789
rank:0, finish optimizer.step profile ...
rank:0, Before memory release - Allocated: 31279335936, Reserved: 47846522880
rank:0, trace log has been written to txt...
rank:0, finish release GPU memory ...
rank:0, After memory release - Allocated: 15552758784, Reserved: 15852371968
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f2407200200 recvbuff 0x7f2407200200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f2407200200,7f2407200200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073f8400 recvbuff 0x7f24073f8400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073f8400,7f24073f8400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f2407200200 recvbuff 0x7f2407200200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f2407200200,7f2407200200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f1f98000000 recvbuff 0x7f1f98000000 count 103022592 datatype 7 op 0 root 0 comm 0xd317670 [nranks=1] stream 0x9a5f390
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f1f98000000,7f1f98000000,103022592,7,0,0,0xd317670,0x9a5f390)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1940656128
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1940656128 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fea00 recvbuff 0x7f24073fea00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fea00,7f24073fea00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 1, finish warm up ...
rank_id = 1, input_tensor_shapes: []
rank:1,cuda fwd time: 167.53663635253906
rank:1, fwd_subop num: 13, fwd_subop: ['trace_src_func=_reduce,duration=0.46,timestamp=4407069998.95,input__shape=[2, 2048, 12288],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.9,timestamp=4407070013.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.39,timestamp=4407070026.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.48,timestamp=4407070040.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.68,timestamp=4407070053.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.44,timestamp=4407070067.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4407070081.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.86,timestamp=4407070095.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.12,timestamp=4407070109.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.0,timestamp=4407070123.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4407070137.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.77,timestamp=4407070151.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.6,timestamp=4407070164.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 1, finish FWD profile ...
rank:1, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.0,timestamp=4407070186.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.0,timestamp=4407070218.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.42,timestamp=4407070244.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.59,timestamp=4407070276.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.57,timestamp=4407070301.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.77,timestamp=4407070333.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.59,timestamp=4407070358.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.59,timestamp=4407070390.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.88,timestamp=4407070415.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.73,timestamp=4407070447.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.74,timestamp=4407070473.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.56,timestamp=4407070504.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:1, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fea00 recvbuff 0x7f24073fea00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fea00,7f24073fea00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:1,optimizer_step time: 52.785152435302734
rank:1, finish optimizer.step profile ...
rank:1, Before memory release - Allocated: 46806665728, Reserved: 63898124288
rank:1, trace log has been written to txt...
rank:1, finish release GPU memory ...
rank:1, After memory release - Allocated: 15552758784, Reserved: 31279022080
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe800 recvbuff 0x7f24073fe800 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe800,7f24073fe800,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe600 recvbuff 0x7f24073fe600 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe600,7f24073fe600,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe800 recvbuff 0x7f24073fe800 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe800,7f24073fe800,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f2378000000 recvbuff 0x7f2378000000 count 103022592 datatype 7 op 0 root 0 comm 0xd317670 [nranks=1] stream 0x9a5f390
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f2378000000,7f2378000000,103022592,7,0,0,0xd317670,0x9a5f390)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 1940656128
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1940656128 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fee00 recvbuff 0x7f24073fee00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fee00,7f24073fee00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 2, finish warm up ...
rank_id = 2, input_tensor_shapes: []
rank:2,cuda fwd time: 167.88479614257812
rank:2, fwd_subop num: 13, fwd_subop: ['trace_src_func=_reduce,duration=0.48,timestamp=4407071785.61,input__shape=[2, 2048, 12288],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.43,timestamp=4407071800.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.48,timestamp=4407071813.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.38,timestamp=4407071826.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.62,timestamp=4407071840.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.6,timestamp=4407071854.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4407071867.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.18,timestamp=4407071882.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.24,timestamp=4407071895.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.05,timestamp=4407071910.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4407071923.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.91,timestamp=4407071938.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.71,timestamp=4407071951.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 2, finish FWD profile ...
rank:2, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.16,timestamp=4407071973.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.13,timestamp=4407072005.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.23,timestamp=4407072031.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.62,timestamp=4407072062.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.97,timestamp=4407072088.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.67,timestamp=4407072119.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.74,timestamp=4407072145.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.95,timestamp=4407072176.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.73,timestamp=4407072202.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.9,timestamp=4407072234.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.95,timestamp=4407072259.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.93,timestamp=4407072291.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:2, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:2,optimizer_step time: 52.87526321411133
rank:2, finish optimizer.step profile ...
rank:2, Before memory release - Allocated: 46806665728, Reserved: 63898124288
rank:2, trace log has been written to txt...
rank:2, finish release GPU memory ...
rank:2, After memory release - Allocated: 15552758784, Reserved: 31279022080
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fea00 recvbuff 0x7f24073fea00 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fea00,7f24073fea00,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fee00 recvbuff 0x7f24073fee00 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fee00,7f24073fee00,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f1f98000000 recvbuff 0x7f1f98000000 count 103022592 datatype 7 op 0 root 0 comm 0xd317670 [nranks=1] stream 0x9a5f390
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f1f98000000,7f1f98000000,103022592,7,0,0,0xd317670,0x9a5f390)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 1940656128
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1940656128 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fee00 recvbuff 0x7f24073fee00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fee00,7f24073fee00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 3, finish warm up ...
rank_id = 3, input_tensor_shapes: []
rank:3,cuda fwd time: 168.52479553222656
rank:3, fwd_subop num: 13, fwd_subop: ['trace_src_func=_reduce,duration=0.48,timestamp=4407073614.55,input__shape=[2, 2048, 12288],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.43,timestamp=4407073629.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.44,timestamp=4407073642.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.18,timestamp=4407073655.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.85,timestamp=4407073669.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.01,timestamp=4407073683.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.29,timestamp=4407073697.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.21,timestamp=4407073711.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.05,timestamp=4407073725.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.21,timestamp=4407073739.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.27,timestamp=4407073753.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.14,timestamp=4407073767.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.62,timestamp=4407073781.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 3, finish FWD profile ...
rank:3, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.96,timestamp=4407073802.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.42,timestamp=4407073834.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.8,timestamp=4407073860.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.97,timestamp=4407073892.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.89,timestamp=4407073917.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.74,timestamp=4407073949.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.83,timestamp=4407073974.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.9,timestamp=4407074006.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.93,timestamp=4407074032.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.87,timestamp=4407074063.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.67,timestamp=4407074089.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.75,timestamp=4407074120.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:3, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fee00 recvbuff 0x7f24073fee00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fee00,7f24073fee00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:3,optimizer_step time: 52.67967987060547
rank:3, finish optimizer.step profile ...
rank:3, Before memory release - Allocated: 46806665728, Reserved: 63898124288
rank:3, trace log has been written to txt...
rank:3, finish release GPU memory ...
rank:3, After memory release - Allocated: 15552758784, Reserved: 31279022080
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f2378000000 recvbuff 0x7f2378000000 count 103022592 datatype 7 op 0 root 0 comm 0xd317670 [nranks=1] stream 0x9a5f390
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f2378000000,7f2378000000,103022592,7,0,0,0xd317670,0x9a5f390)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 0): 1940656128
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1940656128 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fea00 recvbuff 0x7f24073fea00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fea00,7f24073fea00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 4, finish warm up ...
rank_id = 4, input_tensor_shapes: []
rank:4,cuda fwd time: 172.84608459472656
rank:4, fwd_subop num: 13, fwd_subop: ['trace_src_func=_reduce,duration=0.48,timestamp=4407075354.58,input__shape=[2, 2048, 12288],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.5,timestamp=4407075369.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.5,timestamp=4407075382.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.08,timestamp=4407075395.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.7,timestamp=4407075409.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.86,timestamp=4407075423.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.28,timestamp=4407075436.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.2,timestamp=4407075451.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.29,timestamp=4407075465.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.18,timestamp=4407075479.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.3,timestamp=4407075493.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.14,timestamp=4407075511.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.72,timestamp=4407075525.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 4, finish FWD profile ...
rank:4, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.68,timestamp=4407075547.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.82,timestamp=4407075578.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.54,timestamp=4407075603.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.57,timestamp=4407075635.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.85,timestamp=4407075661.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.96,timestamp=4407075692.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.14,timestamp=4407075718.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.9,timestamp=4407075750.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.88,timestamp=4407075775.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.87,timestamp=4407075807.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.87,timestamp=4407075833.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.92,timestamp=4407075865.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:4, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fea00 recvbuff 0x7f24073fea00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fea00,7f24073fea00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:4,optimizer_step time: 52.7902717590332
rank:4, finish optimizer.step profile ...
rank:4, Before memory release - Allocated: 46806665728, Reserved: 63898124288
rank:4, trace log has been written to txt...
rank:4, finish release GPU memory ...
rank:4, After memory release - Allocated: 15552758784, Reserved: 31279022080
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe600 recvbuff 0x7f24073fe600 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe600,7f24073fe600,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe800 recvbuff 0x7f24073fe800 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe800,7f24073fe800,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fea00 recvbuff 0x7f24073fea00 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fea00,7f24073fea00,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f1f98000000 recvbuff 0x7f1f98000000 count 103022592 datatype 7 op 0 root 0 comm 0xd317670 [nranks=1] stream 0x9a5f390
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f1f98000000,7f1f98000000,103022592,7,0,0,0xd317670,0x9a5f390)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 0): 1940656128
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1940656128 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 5, finish warm up ...
rank_id = 5, input_tensor_shapes: []
rank:5,cuda fwd time: 167.6953582763672
rank:5, fwd_subop num: 13, fwd_subop: ['trace_src_func=_reduce,duration=0.48,timestamp=4407077218.14,input__shape=[2, 2048, 12288],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.57,timestamp=4407077232.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.49,timestamp=4407077245.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.13,timestamp=4407077259.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.77,timestamp=4407077272.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.95,timestamp=4407077286.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.21,timestamp=4407077300.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.04,timestamp=4407077314.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.09,timestamp=4407077328.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.89,timestamp=4407077342.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4407077356.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.13,timestamp=4407077370.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.67,timestamp=4407077384.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 5, finish FWD profile ...
rank:5, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.03,timestamp=4407077405.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.5,timestamp=4407077438.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.16,timestamp=4407077463.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.84,timestamp=4407077495.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.86,timestamp=4407077521.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.04,timestamp=4407077552.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.18,timestamp=4407077578.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.67,timestamp=4407077610.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.6,timestamp=4407077635.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.69,timestamp=4407077667.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.63,timestamp=4407077692.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.72,timestamp=4407077723.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:5, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:5,optimizer_step time: 52.64998245239258
rank:5, finish optimizer.step profile ...
rank:5, Before memory release - Allocated: 46806665728, Reserved: 63898124288
rank:5, trace log has been written to txt...
rank:5, finish release GPU memory ...
rank:5, After memory release - Allocated: 15552758784, Reserved: 31279022080
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 6, finish warm up ...
rank_id = 6, input_tensor_shapes: [(2048, 2, 12288)]
rank:6,cuda fwd time: 165.80709838867188
rank:6, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.84,timestamp=4407079409.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.37,timestamp=4407079422.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.38,timestamp=4407079436.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.76,timestamp=4407079449.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.86,timestamp=4407079463.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4407079477.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.85,timestamp=4407079491.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.25,timestamp=4407079504.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.97,timestamp=4407079519.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.27,timestamp=4407079533.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.33,timestamp=4407079547.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.57,timestamp=4407079561.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 6, finish FWD profile ...
rank:6, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.49,timestamp=4407079582.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.17,timestamp=4407079614.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.8,timestamp=4407079639.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.84,timestamp=4407079671.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.61,timestamp=4407079696.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.92,timestamp=4407079728.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.91,timestamp=4407079754.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.95,timestamp=4407079786.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.88,timestamp=4407079811.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.72,timestamp=4407079843.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.79,timestamp=4407079868.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.78,timestamp=4407079900.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:6, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:6,optimizer_step time: 49.27180862426758
rank:6, finish optimizer.step profile ...
rank:6, Before memory release - Allocated: 45156223488, Reserved: 61649977344
rank:6, trace log has been written to txt...
rank:6, finish release GPU memory ...
rank:6, After memory release - Allocated: 30455153664, Reserved: 31167873024
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 7, finish warm up ...
rank_id = 7, input_tensor_shapes: [(2048, 2, 12288)]
rank:7,cuda fwd time: 162.42381286621094
rank:7, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=8.82,timestamp=4407081486.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.32,timestamp=4407081499.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.08,timestamp=4407081512.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.61,timestamp=4407081526.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.55,timestamp=4407081539.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.61,timestamp=4407081553.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.57,timestamp=4407081566.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.61,timestamp=4407081579.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.56,timestamp=4407081593.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.75,timestamp=4407081606.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.37,timestamp=4407081621.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4407081634.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 7, finish FWD profile ...
rank:7, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.11,timestamp=4407081656.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.3,timestamp=4407081688.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.1,timestamp=4407081714.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.92,timestamp=4407081746.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.66,timestamp=4407081772.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.99,timestamp=4407081804.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.67,timestamp=4407081829.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.86,timestamp=4407081861.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.7,timestamp=4407081886.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.94,timestamp=4407081918.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.99,timestamp=4407081944.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.99,timestamp=4407081976.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:7, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:7,optimizer_step time: 49.2042236328125
rank:7, finish optimizer.step profile ...
rank:7, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:7, trace log has been written to txt...
rank:7, finish release GPU memory ...
rank:7, After memory release - Allocated: 30455153664, Reserved: 45569015808
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 8, finish warm up ...
rank_id = 8, input_tensor_shapes: [(2048, 2, 12288)]
rank:8,cuda fwd time: 164.71347045898438
rank:8, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.82,timestamp=4407083562.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.36,timestamp=4407083575.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.33,timestamp=4407083588.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.72,timestamp=4407083602.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.61,timestamp=4407083616.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.91,timestamp=4407083629.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.77,timestamp=4407083643.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.99,timestamp=4407083657.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.95,timestamp=4407083671.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4407083685.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.42,timestamp=4407083699.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.73,timestamp=4407083713.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 8, finish FWD profile ...
rank:8, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.74,timestamp=4407083734.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.48,timestamp=4407083766.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=26.06,timestamp=4407083793.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.38,timestamp=4407083825.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.67,timestamp=4407083850.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.65,timestamp=4407083882.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.85,timestamp=4407083908.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.89,timestamp=4407083939.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.13,timestamp=4407083965.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.9,timestamp=4407083997.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.83,timestamp=4407084022.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.54,timestamp=4407084054.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:8, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:8,optimizer_step time: 49.04857635498047
rank:8, finish optimizer.step profile ...
rank:8, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:8, trace log has been written to txt...
rank:8, finish release GPU memory ...
rank:8, After memory release - Allocated: 30455153664, Reserved: 45569015808
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 9, finish warm up ...
rank_id = 9, input_tensor_shapes: [(2048, 2, 12288)]
rank:9,cuda fwd time: 165.6934356689453
rank:9, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.44,timestamp=4407085720.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.31,timestamp=4407085733.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.78,timestamp=4407085746.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.47,timestamp=4407085759.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.09,timestamp=4407085773.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.19,timestamp=4407085787.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.89,timestamp=4407085801.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.2,timestamp=4407085815.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.84,timestamp=4407085830.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4407085843.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.94,timestamp=4407085858.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.22,timestamp=4407085871.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 9, finish FWD profile ...
rank:9, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.12,timestamp=4407085893.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.46,timestamp=4407085925.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.01,timestamp=4407085951.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.96,timestamp=4407085983.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.99,timestamp=4407086008.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.32,timestamp=4407086040.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.73,timestamp=4407086065.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.67,timestamp=4407086097.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.74,timestamp=4407086122.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.1,timestamp=4407086154.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.18,timestamp=4407086180.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.87,timestamp=4407086212.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:9, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:9,optimizer_step time: 49.34860610961914
rank:9, finish optimizer.step profile ...
rank:9, Before memory release - Allocated: 60058618368, Reserved: 77114376192
rank:9, trace log has been written to txt...
rank:9, finish release GPU memory ...
rank:9, After memory release - Allocated: 30455153664, Reserved: 45770342400
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 1): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 10, finish warm up ...
rank_id = 10, input_tensor_shapes: [(2048, 2, 12288)]
rank:10,cuda fwd time: 164.68173217773438
rank:10, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.82,timestamp=4407087758.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.38,timestamp=4407087771.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.34,timestamp=4407087785.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.87,timestamp=4407087798.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.54,timestamp=4407087812.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4407087826.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.76,timestamp=4407087840.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.87,timestamp=4407087853.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.73,timestamp=4407087867.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.88,timestamp=4407087880.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.48,timestamp=4407087895.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.21,timestamp=4407087909.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 10, finish FWD profile ...
rank:10, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.93,timestamp=4407087930.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.56,timestamp=4407087962.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.55,timestamp=4407087987.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.9,timestamp=4407088019.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.52,timestamp=4407088045.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.17,timestamp=4407088077.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.25,timestamp=4407088103.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.85,timestamp=4407088135.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.83,timestamp=4407088160.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.78,timestamp=4407088192.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.86,timestamp=4407088217.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.01,timestamp=4407088249.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:10, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:10,optimizer_step time: 49.073150634765625
rank:10, finish optimizer.step profile ...
rank:10, Before memory release - Allocated: 60058618368, Reserved: 77114376192
rank:10, trace log has been written to txt...
rank:10, finish release GPU memory ...
rank:10, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 1): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 11, finish warm up ...
rank_id = 11, input_tensor_shapes: [(2048, 2, 12288)]
rank:11,cuda fwd time: 165.32786560058594
rank:11, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.83,timestamp=4407089841.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.41,timestamp=4407089854.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.33,timestamp=4407089868.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.63,timestamp=4407089881.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.84,timestamp=4407089895.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.28,timestamp=4407089909.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.2,timestamp=4407089923.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.11,timestamp=4407089937.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.86,timestamp=4407089951.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4407089964.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.47,timestamp=4407089979.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4407089993.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 11, finish FWD profile ...
rank:11, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.88,timestamp=4407090014.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.98,timestamp=4407090046.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.45,timestamp=4407090072.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.08,timestamp=4407090104.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.22,timestamp=4407090130.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.88,timestamp=4407090162.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.85,timestamp=4407090187.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.5,timestamp=4407090219.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.76,timestamp=4407090244.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.99,timestamp=4407090276.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.74,timestamp=4407090301.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.02,timestamp=4407090333.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:11, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:11,optimizer_step time: 49.20627212524414
rank:11, finish optimizer.step profile ...
rank:11, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:11, trace log has been written to txt...
rank:11, finish release GPU memory ...
rank:11, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 12, finish warm up ...
rank_id = 12, input_tensor_shapes: [(2048, 2, 12288)]
rank:12,cuda fwd time: 166.11737060546875
rank:12, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.47,timestamp=4407091993.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.31,timestamp=4407092006.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.81,timestamp=4407092019.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4407092033.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.43,timestamp=4407092047.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.09,timestamp=4407092060.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.75,timestamp=4407092075.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.07,timestamp=4407092088.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.71,timestamp=4407092103.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4407092116.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.07,timestamp=4407092131.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.36,timestamp=4407092145.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 12, finish FWD profile ...
rank:12, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.82,timestamp=4407092167.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.24,timestamp=4407092199.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.09,timestamp=4407092225.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.83,timestamp=4407092256.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.06,timestamp=4407092282.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.87,timestamp=4407092314.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.88,timestamp=4407092339.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.78,timestamp=4407092371.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.91,timestamp=4407092396.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.89,timestamp=4407092428.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.63,timestamp=4407092453.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.41,timestamp=4407092485.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:12, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:12,optimizer_step time: 49.305599212646484
rank:12, finish optimizer.step profile ...
rank:12, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:12, trace log has been written to txt...
rank:12, finish release GPU memory ...
rank:12, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 13, finish warm up ...
rank_id = 13, input_tensor_shapes: [(2048, 2, 12288)]
rank:13,cuda fwd time: 165.05856323242188
rank:13, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.79,timestamp=4407094061.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.39,timestamp=4407094074.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.15,timestamp=4407094088.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4407094101.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.58,timestamp=4407094115.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.19,timestamp=4407094128.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.12,timestamp=4407094143.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.21,timestamp=4407094157.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.02,timestamp=4407094171.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.99,timestamp=4407094184.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.42,timestamp=4407094199.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4407094212.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 13, finish FWD profile ...
rank:13, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.9,timestamp=4407094234.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.1,timestamp=4407094265.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.81,timestamp=4407094291.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.09,timestamp=4407094322.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.7,timestamp=4407094347.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.12,timestamp=4407094378.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.88,timestamp=4407094404.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.01,timestamp=4407094436.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.76,timestamp=4407094461.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.16,timestamp=4407094493.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.98,timestamp=4407094519.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.86,timestamp=4407094550.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:13, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:13,optimizer_step time: 49.23596954345703
rank:13, finish optimizer.step profile ...
rank:13, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:13, trace log has been written to txt...
rank:13, finish release GPU memory ...
rank:13, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 2): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 14, finish warm up ...
rank_id = 14, input_tensor_shapes: [(2048, 2, 12288)]
rank:14,cuda fwd time: 165.78048706054688
rank:14, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.4,timestamp=4407096217.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.3,timestamp=4407096230.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.92,timestamp=4407096244.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4407096257.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.7,timestamp=4407096271.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.2,timestamp=4407096285.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.73,timestamp=4407096299.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4407096313.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.55,timestamp=4407096327.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.9,timestamp=4407096341.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.95,timestamp=4407096355.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4407096368.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 14, finish FWD profile ...
rank:14, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.61,timestamp=4407096390.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.95,timestamp=4407096422.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.9,timestamp=4407096448.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.05,timestamp=4407096480.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.15,timestamp=4407096505.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.82,timestamp=4407096537.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.03,timestamp=4407096563.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.88,timestamp=4407096595.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.87,timestamp=4407096620.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.83,timestamp=4407096652.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.87,timestamp=4407096677.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.88,timestamp=4407096709.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:14, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:14,optimizer_step time: 49.22470474243164
rank:14, finish optimizer.step profile ...
rank:14, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:14, trace log has been written to txt...
rank:14, finish release GPU memory ...
rank:14, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 2): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 15, finish warm up ...
rank_id = 15, input_tensor_shapes: [(2048, 2, 12288)]
rank:15,cuda fwd time: 162.4145965576172
rank:15, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.68,timestamp=4407098325.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.4,timestamp=4407098338.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.19,timestamp=4407098352.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.5,timestamp=4407098365.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.25,timestamp=4407098378.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.48,timestamp=4407098391.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.27,timestamp=4407098405.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.48,timestamp=4407098418.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.93,timestamp=4407098432.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.12,timestamp=4407098445.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.53,timestamp=4407098460.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4407098474.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 15, finish FWD profile ...
rank:15, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.95,timestamp=4407098495.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.18,timestamp=4407098527.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.82,timestamp=4407098552.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.05,timestamp=4407098583.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.91,timestamp=4407098609.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.28,timestamp=4407098640.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.83,timestamp=4407098666.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.87,timestamp=4407098697.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.71,timestamp=4407098723.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.96,timestamp=4407098755.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.9,timestamp=4407098780.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.07,timestamp=4407098812.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:15, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:15,optimizer_step time: 49.15507125854492
rank:15, finish optimizer.step profile ...
rank:15, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:15, trace log has been written to txt...
rank:15, finish release GPU memory ...
rank:15, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 2): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 16, finish warm up ...
rank_id = 16, input_tensor_shapes: [(2048, 2, 12288)]
rank:16,cuda fwd time: 165.40057373046875
rank:16, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.81,timestamp=4407100375.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.37,timestamp=4407100388.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.32,timestamp=4407100401.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.85,timestamp=4407100415.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.86,timestamp=4407100429.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.99,timestamp=4407100442.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.83,timestamp=4407100456.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4407100470.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.01,timestamp=4407100484.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.29,timestamp=4407100498.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.61,timestamp=4407100512.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4407100526.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 16, finish FWD profile ...
rank:16, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.85,timestamp=4407100548.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.52,timestamp=4407100580.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=26.18,timestamp=4407100607.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.56,timestamp=4407100639.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.58,timestamp=4407100665.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.8,timestamp=4407100697.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.91,timestamp=4407100723.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.95,timestamp=4407100755.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.12,timestamp=4407100780.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.38,timestamp=4407100812.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.97,timestamp=4407100837.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.6,timestamp=4407100869.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:16, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:16,optimizer_step time: 49.11103820800781
rank:16, finish optimizer.step profile ...
rank:16, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:16, trace log has been written to txt...
rank:16, finish release GPU memory ...
rank:16, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 2): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 17, finish warm up ...
rank_id = 17, input_tensor_shapes: [(2048, 2, 12288)]
rank:17,cuda fwd time: 162.9818878173828
rank:17, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.36,timestamp=4407102561.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.33,timestamp=4407102574.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.93,timestamp=4407102588.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.54,timestamp=4407102601.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.13,timestamp=4407102615.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4407102628.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.2,timestamp=4407102641.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4407102655.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.19,timestamp=4407102668.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.62,timestamp=4407102682.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.12,timestamp=4407102696.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4407102710.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 17, finish FWD profile ...
rank:17, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.29,timestamp=4407102732.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.08,timestamp=4407102764.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.92,timestamp=4407102790.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.99,timestamp=4407102822.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.76,timestamp=4407102848.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.1,timestamp=4407102880.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.97,timestamp=4407102906.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.91,timestamp=4407102938.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.87,timestamp=4407102963.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.86,timestamp=4407102995.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.7,timestamp=4407103021.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.86,timestamp=4407103052.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:17, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:17,optimizer_step time: 49.19193649291992
rank:17, finish optimizer.step profile ...
rank:17, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:17, trace log has been written to txt...
rank:17, finish release GPU memory ...
rank:17, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 18, finish warm up ...
rank_id = 18, input_tensor_shapes: [(2048, 2, 12288)]
rank:18,cuda fwd time: 165.39852905273438
rank:18, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.78,timestamp=4407104725.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.37,timestamp=4407104738.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.37,timestamp=4407104752.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4407104765.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.48,timestamp=4407104779.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.73,timestamp=4407104792.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.84,timestamp=4407104806.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.26,timestamp=4407104820.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.13,timestamp=4407104835.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.27,timestamp=4407104848.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.48,timestamp=4407104863.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4407104877.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 18, finish FWD profile ...
rank:18, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.02,timestamp=4407104898.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.99,timestamp=4407104930.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.4,timestamp=4407104956.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.51,timestamp=4407104987.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.09,timestamp=4407105013.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.9,timestamp=4407105045.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.64,timestamp=4407105070.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.84,timestamp=4407105102.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.1,timestamp=4407105128.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.81,timestamp=4407105160.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.07,timestamp=4407105185.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.69,timestamp=4407105217.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:18, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:18,optimizer_step time: 49.17555236816406
rank:18, finish optimizer.step profile ...
rank:18, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:18, trace log has been written to txt...
rank:18, finish release GPU memory ...
rank:18, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 19, finish warm up ...
rank_id = 19, input_tensor_shapes: [(2048, 2, 12288)]
rank:19,cuda fwd time: 163.09759521484375
rank:19, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.75,timestamp=4407106798.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.37,timestamp=4407106811.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.19,timestamp=4407106825.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.6,timestamp=4407106838.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.24,timestamp=4407106852.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4407106865.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.77,timestamp=4407106879.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.84,timestamp=4407106892.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.76,timestamp=4407106906.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.84,timestamp=4407106920.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.22,timestamp=4407106934.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.8,timestamp=4407106947.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 19, finish FWD profile ...
rank:19, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.69,timestamp=4407106969.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.18,timestamp=4407107001.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.73,timestamp=4407107026.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.89,timestamp=4407107058.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.5,timestamp=4407107083.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.92,timestamp=4407107114.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.54,timestamp=4407107139.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.55,timestamp=4407107170.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.36,timestamp=4407107196.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.12,timestamp=4407107228.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.05,timestamp=4407107254.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.09,timestamp=4407107286.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:19, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:19,optimizer_step time: 49.340415954589844
rank:19, finish optimizer.step profile ...
rank:19, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:19, trace log has been written to txt...
rank:19, finish release GPU memory ...
rank:19, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 3): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 20, finish warm up ...
rank_id = 20, input_tensor_shapes: [(2048, 2, 12288)]
rank:20,cuda fwd time: 166.2003173828125
rank:20, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.84,timestamp=4407108861.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.4,timestamp=4407108875.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.34,timestamp=4407108888.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.84,timestamp=4407108902.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.91,timestamp=4407108916.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.27,timestamp=4407108930.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.96,timestamp=4407108944.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.09,timestamp=4407108958.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.05,timestamp=4407108972.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.3,timestamp=4407108986.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.7,timestamp=4407109000.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4407109014.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 20, finish FWD profile ...
rank:20, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.82,timestamp=4407109035.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.34,timestamp=4407109067.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.62,timestamp=4407109092.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.61,timestamp=4407109124.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.23,timestamp=4407109150.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.72,timestamp=4407109181.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.06,timestamp=4407109207.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.1,timestamp=4407109239.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.05,timestamp=4407109265.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.86,timestamp=4407109296.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.97,timestamp=4407109322.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.15,timestamp=4407109354.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:20, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:20,optimizer_step time: 49.17350387573242
rank:20, finish optimizer.step profile ...
rank:20, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:20, trace log has been written to txt...
rank:20, finish release GPU memory ...
rank:20, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 3): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 21, finish warm up ...
rank_id = 21, input_tensor_shapes: [(2048, 2, 12288)]
rank:21,cuda fwd time: 164.46054077148438
rank:21, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.8,timestamp=4407110900.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.39,timestamp=4407110913.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.38,timestamp=4407110927.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4407110940.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.5,timestamp=4407110954.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4407110967.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.8,timestamp=4407110981.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.26,timestamp=4407110995.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.19,timestamp=4407111009.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.1,timestamp=4407111023.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.28,timestamp=4407111037.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4407111051.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 21, finish FWD profile ...
rank:21, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.75,timestamp=4407111072.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.13,timestamp=4407111104.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.0,timestamp=4407111130.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.4,timestamp=4407111161.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.03,timestamp=4407111187.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.75,timestamp=4407111219.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.98,timestamp=4407111244.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.11,timestamp=4407111276.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.51,timestamp=4407111302.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.8,timestamp=4407111334.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.83,timestamp=4407111360.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.96,timestamp=4407111391.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:21, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:21,optimizer_step time: 49.16019058227539
rank:21, finish optimizer.step profile ...
rank:21, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:21, trace log has been written to txt...
rank:21, finish release GPU memory ...
rank:21, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 3): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 22, finish warm up ...
rank_id = 22, input_tensor_shapes: [(2048, 2, 12288)]
rank:22,cuda fwd time: 165.82041931152344
rank:22, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.67,timestamp=4407112975.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.35,timestamp=4407112989.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.17,timestamp=4407113002.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.51,timestamp=4407113015.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.67,timestamp=4407113029.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.27,timestamp=4407113043.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.13,timestamp=4407113057.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.18,timestamp=4407113071.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.02,timestamp=4407113085.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.11,timestamp=4407113099.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.55,timestamp=4407113113.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.09,timestamp=4407113127.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 22, finish FWD profile ...
rank:22, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.34,timestamp=4407113149.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.47,timestamp=4407113181.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.76,timestamp=4407113208.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.1,timestamp=4407113240.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.78,timestamp=4407113265.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.29,timestamp=4407113297.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.58,timestamp=4407113322.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.82,timestamp=4407113354.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.96,timestamp=4407113380.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.03,timestamp=4407113412.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.98,timestamp=4407113437.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.72,timestamp=4407113469.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:22, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:22,optimizer_step time: 49.181697845458984
rank:22, finish optimizer.step profile ...
rank:22, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:22, trace log has been written to txt...
rank:22, finish release GPU memory ...
rank:22, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 3): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 23, finish warm up ...
rank_id = 23, input_tensor_shapes: [(2048, 2, 12288)]
rank:23,cuda fwd time: 167.38406372070312
rank:23, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.52,timestamp=4407115020.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.38,timestamp=4407115034.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.65,timestamp=4407115049.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.54,timestamp=4407115062.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.49,timestamp=4407115076.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4407115090.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.09,timestamp=4407115104.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.23,timestamp=4407115118.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.04,timestamp=4407115132.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.99,timestamp=4407115146.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.38,timestamp=4407115160.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.95,timestamp=4407115174.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 23, finish FWD profile ...
rank:23, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.71,timestamp=4407115196.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.71,timestamp=4407115228.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.67,timestamp=4407115254.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.77,timestamp=4407115286.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.81,timestamp=4407115311.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.79,timestamp=4407115343.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.84,timestamp=4407115369.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.38,timestamp=4407115400.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.34,timestamp=4407115426.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.86,timestamp=4407115458.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.94,timestamp=4407115483.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.01,timestamp=4407115515.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:23, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:23,optimizer_step time: 49.21753692626953
rank:23, finish optimizer.step profile ...
rank:23, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:23, trace log has been written to txt...
rank:23, finish release GPU memory ...
rank:23, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 24, finish warm up ...
rank_id = 24, input_tensor_shapes: [(2048, 2, 12288)]
rank:24,cuda fwd time: 165.23776245117188
rank:24, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.86,timestamp=4407117083.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.39,timestamp=4407117096.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.43,timestamp=4407117109.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.7,timestamp=4407117123.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.71,timestamp=4407117137.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.77,timestamp=4407117150.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.82,timestamp=4407117164.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.28,timestamp=4407117178.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.22,timestamp=4407117192.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.21,timestamp=4407117206.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.48,timestamp=4407117220.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.19,timestamp=4407117234.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 24, finish FWD profile ...
rank:24, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.91,timestamp=4407117255.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.06,timestamp=4407117287.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.71,timestamp=4407117313.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.4,timestamp=4407117345.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.67,timestamp=4407117370.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.04,timestamp=4407117402.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.91,timestamp=4407117428.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.07,timestamp=4407117460.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.23,timestamp=4407117485.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.43,timestamp=4407117517.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.86,timestamp=4407117543.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.86,timestamp=4407117575.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:24, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:24,optimizer_step time: 49.20934295654297
rank:24, finish optimizer.step profile ...
rank:24, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:24, trace log has been written to txt...
rank:24, finish release GPU memory ...
rank:24, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 25, finish warm up ...
rank_id = 25, input_tensor_shapes: [(2048, 2, 12288)]
rank:25,cuda fwd time: 164.91725158691406
rank:25, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.8,timestamp=4407119514.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.4,timestamp=4407119527.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.36,timestamp=4407119541.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.62,timestamp=4407119554.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.58,timestamp=4407119568.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4407119581.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.09,timestamp=4407119595.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.28,timestamp=4407119609.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.11,timestamp=4407119624.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4407119637.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.4,timestamp=4407119651.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.69,timestamp=4407119665.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 25, finish FWD profile ...
rank:25, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.68,timestamp=4407119686.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=16.81,timestamp=4407119718.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.58,timestamp=4407119743.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.67,timestamp=4407119775.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.87,timestamp=4407119801.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.01,timestamp=4407119832.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.48,timestamp=4407119858.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.6,timestamp=4407119890.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.09,timestamp=4407119916.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.83,timestamp=4407119947.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.79,timestamp=4407119973.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.87,timestamp=4407120005.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:25, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:25,optimizer_step time: 49.314815521240234
rank:25, finish optimizer.step profile ...
rank:25, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:25, trace log has been written to txt...
rank:25, finish release GPU memory ...
rank:25, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 26, finish warm up ...
rank_id = 26, input_tensor_shapes: [(2048, 2, 12288)]
rank:26,cuda fwd time: 165.791748046875
rank:26, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.45,timestamp=4407121709.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.35,timestamp=4407121722.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.8,timestamp=4407121735.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.48,timestamp=4407121748.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.33,timestamp=4407121762.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.05,timestamp=4407121776.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.84,timestamp=4407121790.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4407121804.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.68,timestamp=4407121818.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4407121832.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.2,timestamp=4407121846.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.85,timestamp=4407121860.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 26, finish FWD profile ...
rank:26, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.09,timestamp=4407121882.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.29,timestamp=4407121914.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.75,timestamp=4407121941.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.46,timestamp=4407121973.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.88,timestamp=4407121998.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.78,timestamp=4407122030.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.01,timestamp=4407122056.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.87,timestamp=4407122088.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.0,timestamp=4407122113.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.04,timestamp=4407122145.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.46,timestamp=4407122171.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.55,timestamp=4407122203.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:26, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:26,optimizer_step time: 49.1673583984375
rank:26, finish optimizer.step profile ...
rank:26, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:26, trace log has been written to txt...
rank:26, finish release GPU memory ...
rank:26, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 27, finish warm up ...
rank_id = 27, input_tensor_shapes: [(2048, 2, 12288)]
rank:27,cuda fwd time: 163.0074920654297
rank:27, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.82,timestamp=4407123831.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.41,timestamp=4407123844.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.42,timestamp=4407123858.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.68,timestamp=4407123871.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.68,timestamp=4407123885.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4407123898.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.7,timestamp=4407123912.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4407123925.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.61,timestamp=4407123939.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.75,timestamp=4407123953.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.69,timestamp=4407123967.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4407123980.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 27, finish FWD profile ...
rank:27, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.96,timestamp=4407124002.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.31,timestamp=4407124034.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.67,timestamp=4407124059.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.52,timestamp=4407124091.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.25,timestamp=4407124116.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.81,timestamp=4407124147.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.5,timestamp=4407124172.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.74,timestamp=4407124204.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.08,timestamp=4407124229.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.86,timestamp=4407124261.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.74,timestamp=4407124287.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.54,timestamp=4407124319.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:27, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:27,optimizer_step time: 49.42950439453125
rank:27, finish optimizer.step profile ...
rank:27, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:27, trace log has been written to txt...
rank:27, finish release GPU memory ...
rank:27, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 4): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 28, finish warm up ...
rank_id = 28, input_tensor_shapes: [(2048, 2, 12288)]
rank:28,cuda fwd time: 167.49363708496094
rank:28, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.43,timestamp=4407126021.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.35,timestamp=4407126034.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.0,timestamp=4407126048.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4407126061.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.98,timestamp=4407126075.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.76,timestamp=4407126088.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.62,timestamp=4407126103.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.2,timestamp=4407126116.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.86,timestamp=4407126131.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.21,timestamp=4407126145.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.65,timestamp=4407126159.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4407126173.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 28, finish FWD profile ...
rank:28, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.93,timestamp=4407126198.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.76,timestamp=4407126229.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.08,timestamp=4407126255.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.84,timestamp=4407126287.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.97,timestamp=4407126313.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.85,timestamp=4407126344.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.92,timestamp=4407126370.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.04,timestamp=4407126402.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.43,timestamp=4407126427.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.57,timestamp=4407126459.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.16,timestamp=4407126485.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.99,timestamp=4407126517.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:28, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:28,optimizer_step time: 49.185791015625
rank:28, finish optimizer.step profile ...
rank:28, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:28, trace log has been written to txt...
rank:28, finish release GPU memory ...
rank:28, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 4): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 29, finish warm up ...
rank_id = 29, input_tensor_shapes: [(2048, 2, 12288)]
rank:29,cuda fwd time: 166.35699462890625
rank:29, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.52,timestamp=4407128537.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.35,timestamp=4407128550.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.85,timestamp=4407128563.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.53,timestamp=4407128576.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.41,timestamp=4407128590.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.25,timestamp=4407128604.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.06,timestamp=4407128619.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.32,timestamp=4407128633.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.9,timestamp=4407128647.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.09,timestamp=4407128661.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.21,timestamp=4407128675.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4407128689.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 29, finish FWD profile ...
rank:29, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.12,timestamp=4407128711.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.39,timestamp=4407128743.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.56,timestamp=4407128768.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.02,timestamp=4407128800.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.86,timestamp=4407128825.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.98,timestamp=4407128857.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.88,timestamp=4407128882.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.96,timestamp=4407128914.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.44,timestamp=4407128940.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.55,timestamp=4407128972.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.89,timestamp=4407128997.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.04,timestamp=4407129029.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:29, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:29,optimizer_step time: 49.282047271728516
rank:29, finish optimizer.step profile ...
rank:29, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:29, trace log has been written to txt...
rank:29, finish release GPU memory ...
rank:29, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 30, finish warm up ...
rank_id = 30, input_tensor_shapes: [(2048, 2, 12288)]
rank:30,cuda fwd time: 169.08082580566406
rank:30, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.48,timestamp=4407130711.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.35,timestamp=4407130724.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.07,timestamp=4407130737.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.8,timestamp=4407130751.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.78,timestamp=4407130765.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4407130779.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.79,timestamp=4407130793.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4407130807.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.78,timestamp=4407130821.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.09,timestamp=4407130835.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.21,timestamp=4407130852.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.54,timestamp=4407130866.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 30, finish FWD profile ...
rank:30, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=16.88,timestamp=4407130887.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.1,timestamp=4407130919.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.57,timestamp=4407130944.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.85,timestamp=4407130976.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.03,timestamp=4407131002.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.16,timestamp=4407131033.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.76,timestamp=4407131058.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.16,timestamp=4407131090.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.76,timestamp=4407131116.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.73,timestamp=4407131147.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.81,timestamp=4407131173.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.89,timestamp=4407131205.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:30, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:30,optimizer_step time: 49.21139144897461
rank:30, finish optimizer.step profile ...
rank:30, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:30, trace log has been written to txt...
rank:30, finish release GPU memory ...
rank:30, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 5): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 31, finish warm up ...
rank_id = 31, input_tensor_shapes: [(2048, 2, 12288)]
rank:31,cuda fwd time: 165.2346954345703
rank:31, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.85,timestamp=4407132771.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.4,timestamp=4407132784.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.26,timestamp=4407132798.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4407132811.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.56,timestamp=4407132825.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4407132838.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.15,timestamp=4407132853.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.27,timestamp=4407132867.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.21,timestamp=4407132881.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.05,timestamp=4407132895.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.37,timestamp=4407132909.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4407132923.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 31, finish FWD profile ...
rank:31, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.14,timestamp=4407132944.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.08,timestamp=4407132976.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.75,timestamp=4407133002.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.81,timestamp=4407133034.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.83,timestamp=4407133060.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.25,timestamp=4407133093.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.98,timestamp=4407133118.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.03,timestamp=4407133150.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.7,timestamp=4407133175.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.5,timestamp=4407133207.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.72,timestamp=4407133233.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.79,timestamp=4407133265.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:31, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:31,optimizer_step time: 49.15507125854492
rank:31, finish optimizer.step profile ...
rank:31, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:31, trace log has been written to txt...
rank:31, finish release GPU memory ...
rank:31, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 5): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 32, finish warm up ...
rank_id = 32, input_tensor_shapes: [(2048, 2, 12288)]
rank:32,cuda fwd time: 165.71084594726562
rank:32, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.46,timestamp=4407135036.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.34,timestamp=4407135049.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.14,timestamp=4407135063.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.71,timestamp=4407135076.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.38,timestamp=4407135090.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4407135104.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.51,timestamp=4407135118.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.05,timestamp=4407135132.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.82,timestamp=4407135146.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.19,timestamp=4407135160.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.11,timestamp=4407135174.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4407135188.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 32, finish FWD profile ...
rank:32, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.61,timestamp=4407135209.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.29,timestamp=4407135241.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=26.05,timestamp=4407135268.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.43,timestamp=4407135301.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.88,timestamp=4407135327.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.82,timestamp=4407135359.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.67,timestamp=4407135384.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.03,timestamp=4407135417.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.12,timestamp=4407135442.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.76,timestamp=4407135474.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.82,timestamp=4407135500.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.79,timestamp=4407135531.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:32, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:32,optimizer_step time: 49.18988800048828
rank:32, finish optimizer.step profile ...
rank:32, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:32, trace log has been written to txt...
rank:32, finish release GPU memory ...
rank:32, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 5): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 33, finish warm up ...
rank_id = 33, input_tensor_shapes: [(2048, 2, 12288)]
rank:33,cuda fwd time: 163.5819549560547
rank:33, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.49,timestamp=4407137230.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.36,timestamp=4407137243.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.12,timestamp=4407137257.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4407137270.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.23,timestamp=4407137284.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.61,timestamp=4407137297.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.23,timestamp=4407137311.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.6,timestamp=4407137324.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.2,timestamp=4407137338.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.76,timestamp=4407137351.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.2,timestamp=4407137366.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.73,timestamp=4407137379.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 33, finish FWD profile ...
rank:33, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.96,timestamp=4407137401.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.07,timestamp=4407137433.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.51,timestamp=4407137458.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.22,timestamp=4407137490.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.97,timestamp=4407137515.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.39,timestamp=4407137546.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.1,timestamp=4407137572.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.07,timestamp=4407137604.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.15,timestamp=4407137630.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.86,timestamp=4407137662.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.99,timestamp=4407137687.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.87,timestamp=4407137719.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:33, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:33,optimizer_step time: 49.28819274902344
rank:33, finish optimizer.step profile ...
rank:33, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:33, trace log has been written to txt...
rank:33, finish release GPU memory ...
rank:33, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 5): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 34, finish warm up ...
rank_id = 34, input_tensor_shapes: [(2048, 2, 12288)]
rank:34,cuda fwd time: 168.1817626953125
rank:34, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.86,timestamp=4407139294.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.41,timestamp=4407139307.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.14,timestamp=4407139321.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.49,timestamp=4407139334.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.23,timestamp=4407139347.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.49,timestamp=4407139360.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.04,timestamp=4407139380.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.62,timestamp=4407139393.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.91,timestamp=4407139407.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4407139421.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.5,timestamp=4407139435.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.72,timestamp=4407139448.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 34, finish FWD profile ...
rank:34, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.69,timestamp=4407139470.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.6,timestamp=4407139502.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.7,timestamp=4407139529.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.19,timestamp=4407139561.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.67,timestamp=4407139587.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.16,timestamp=4407139619.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.97,timestamp=4407139645.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.0,timestamp=4407139677.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.95,timestamp=4407139702.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.89,timestamp=4407139734.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.13,timestamp=4407139760.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.87,timestamp=4407139791.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:34, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:34,optimizer_step time: 49.21343994140625
rank:34, finish optimizer.step profile ...
rank:34, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:34, trace log has been written to txt...
rank:34, finish release GPU memory ...
rank:34, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 5): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 35, finish warm up ...
rank_id = 35, input_tensor_shapes: [(2048, 2, 12288)]
rank:35,cuda fwd time: 169.21087646484375
rank:35, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.82,timestamp=4407141340.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.4,timestamp=4407141353.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.22,timestamp=4407141367.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.49,timestamp=4407141380.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.14,timestamp=4407141399.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.8,timestamp=4407141412.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.57,timestamp=4407141426.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.84,timestamp=4407141440.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.95,timestamp=4407141454.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.25,timestamp=4407141468.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.68,timestamp=4407141482.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.78,timestamp=4407141496.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 35, finish FWD profile ...
rank:35, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.94,timestamp=4407141517.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.26,timestamp=4407141549.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.3,timestamp=4407141575.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.97,timestamp=4407141607.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.28,timestamp=4407141633.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.96,timestamp=4407141665.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.12,timestamp=4407141690.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.81,timestamp=4407141722.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.08,timestamp=4407141748.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.87,timestamp=4407141780.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.04,timestamp=4407141805.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.81,timestamp=4407141837.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:35, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:35,optimizer_step time: 49.141761779785156
rank:35, finish optimizer.step profile ...
rank:35, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:35, trace log has been written to txt...
rank:35, finish release GPU memory ...
rank:35, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 36, finish warm up ...
rank_id = 36, input_tensor_shapes: [(2048, 2, 12288)]
rank:36,cuda fwd time: 164.35609436035156
rank:36, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.87,timestamp=4407143379.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.4,timestamp=4407143392.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.22,timestamp=4407143405.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.49,timestamp=4407143418.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.06,timestamp=4407143432.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4407143446.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.9,timestamp=4407143460.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.27,timestamp=4407143474.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.18,timestamp=4407143488.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4407143502.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.4,timestamp=4407143516.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.71,timestamp=4407143529.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 36, finish FWD profile ...
rank:36, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.7,timestamp=4407143550.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.7,timestamp=4407143582.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.93,timestamp=4407143607.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.46,timestamp=4407143639.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.97,timestamp=4407143664.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.16,timestamp=4407143696.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.17,timestamp=4407143722.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.95,timestamp=4407143754.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.12,timestamp=4407143779.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.78,timestamp=4407143811.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.09,timestamp=4407143837.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.85,timestamp=4407143869.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:36, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:36,optimizer_step time: 49.06291198730469
rank:36, finish optimizer.step profile ...
rank:36, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:36, trace log has been written to txt...
rank:36, finish release GPU memory ...
rank:36, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 6): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 37, finish warm up ...
rank_id = 37, input_tensor_shapes: [(2048, 2, 12288)]
rank:37,cuda fwd time: 165.73440551757812
rank:37, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.89,timestamp=4407145427.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.42,timestamp=4407145440.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.11,timestamp=4407145454.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.67,timestamp=4407145467.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.61,timestamp=4407145481.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4407145495.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.07,timestamp=4407145509.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.29,timestamp=4407145523.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.21,timestamp=4407145537.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.19,timestamp=4407145551.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.37,timestamp=4407145565.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.0,timestamp=4407145579.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 37, finish FWD profile ...
rank:37, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.16,timestamp=4407145601.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.4,timestamp=4407145633.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.53,timestamp=4407145659.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.62,timestamp=4407145691.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.7,timestamp=4407145716.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.9,timestamp=4407145748.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.94,timestamp=4407145773.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.02,timestamp=4407145805.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.99,timestamp=4407145831.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.98,timestamp=4407145862.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.87,timestamp=4407145888.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.84,timestamp=4407145920.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:37, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:37,optimizer_step time: 49.15507125854492
rank:37, finish optimizer.step profile ...
rank:37, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:37, trace log has been written to txt...
rank:37, finish release GPU memory ...
rank:37, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 6): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 38, finish warm up ...
rank_id = 38, input_tensor_shapes: [(2048, 2, 12288)]
rank:38,cuda fwd time: 164.45542907714844
rank:38, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=5.92,timestamp=4407147953.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.34,timestamp=4407147967.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.12,timestamp=4407147980.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.6,timestamp=4407147994.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.42,timestamp=4407148008.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.8,timestamp=4407148021.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.4,timestamp=4407148035.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.78,timestamp=4407148048.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.06,timestamp=4407148062.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.75,timestamp=4407148076.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.05,timestamp=4407148090.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.6,timestamp=4407148104.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 38, finish FWD profile ...
rank:38, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=17.31,timestamp=4407148125.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.14,timestamp=4407148157.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.72,timestamp=4407148183.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.69,timestamp=4407148214.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.54,timestamp=4407148240.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.99,timestamp=4407148272.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.04,timestamp=4407148298.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.88,timestamp=4407148330.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.13,timestamp=4407148356.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.05,timestamp=4407148387.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.16,timestamp=4407148413.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.81,timestamp=4407148445.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:38, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:38,optimizer_step time: 49.21855926513672
rank:38, finish optimizer.step profile ...
rank:38, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:38, trace log has been written to txt...
rank:38, finish release GPU memory ...
rank:38, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 6): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 39, finish warm up ...
rank_id = 39, input_tensor_shapes: [(2048, 2, 12288)]
rank:39,cuda fwd time: 164.96231079101562
rank:39, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.8,timestamp=4407150112.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.38,timestamp=4407150125.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.2,timestamp=4407150139.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.65,timestamp=4407150152.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.76,timestamp=4407150166.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4407150180.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.97,timestamp=4407150194.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.08,timestamp=4407150208.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.87,timestamp=4407150222.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4407150236.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.39,timestamp=4407150250.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.64,timestamp=4407150263.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 39, finish FWD profile ...
rank:39, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.71,timestamp=4407150284.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.87,timestamp=4407150316.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.81,timestamp=4407150341.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.0,timestamp=4407150373.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.15,timestamp=4407150399.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.9,timestamp=4407150431.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.05,timestamp=4407150457.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.89,timestamp=4407150488.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.93,timestamp=4407150514.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.01,timestamp=4407150546.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.73,timestamp=4407150571.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.11,timestamp=4407150603.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:39, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:39,optimizer_step time: 49.19398498535156
rank:39, finish optimizer.step profile ...
rank:39, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:39, trace log has been written to txt...
rank:39, finish release GPU memory ...
rank:39, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 6): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 40, finish warm up ...
rank_id = 40, input_tensor_shapes: [(2048, 2, 12288)]
rank:40,cuda fwd time: 163.57171630859375
rank:40, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.82,timestamp=4407152192.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.41,timestamp=4407152205.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.32,timestamp=4407152219.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.66,timestamp=4407152232.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.76,timestamp=4407152246.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.84,timestamp=4407152259.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.77,timestamp=4407152273.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4407152287.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.75,timestamp=4407152301.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.84,timestamp=4407152314.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.1,timestamp=4407152328.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4407152342.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 40, finish FWD profile ...
rank:40, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.84,timestamp=4407152363.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.19,timestamp=4407152395.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.85,timestamp=4407152420.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.21,timestamp=4407152452.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.85,timestamp=4407152478.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.96,timestamp=4407152510.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.17,timestamp=4407152536.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.0,timestamp=4407152567.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.13,timestamp=4407152593.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.92,timestamp=4407152625.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.09,timestamp=4407152651.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.8,timestamp=4407152682.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:40, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:40,optimizer_step time: 49.1591682434082
rank:40, finish optimizer.step profile ...
rank:40, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:40, trace log has been written to txt...
rank:40, finish release GPU memory ...
rank:40, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 6): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 41, finish warm up ...
rank_id = 41, input_tensor_shapes: [(2048, 2, 12288)]
rank:41,cuda fwd time: 166.866943359375
rank:41, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.81,timestamp=4407154251.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.42,timestamp=4407154264.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.3,timestamp=4407154278.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.9,timestamp=4407154291.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.08,timestamp=4407154305.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.32,timestamp=4407154319.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.22,timestamp=4407154334.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4407154347.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.08,timestamp=4407154362.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.43,timestamp=4407154376.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.85,timestamp=4407154390.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4407154404.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 41, finish FWD profile ...
rank:41, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.99,timestamp=4407154425.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.41,timestamp=4407154457.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.63,timestamp=4407154483.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.94,timestamp=4407154515.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.5,timestamp=4407154541.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.95,timestamp=4407154573.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.8,timestamp=4407154598.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.9,timestamp=4407154630.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.11,timestamp=4407154655.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.78,timestamp=4407154687.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.83,timestamp=4407154713.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.86,timestamp=4407154744.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:41, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:41,optimizer_step time: 49.26464080810547
rank:41, finish optimizer.step profile ...
rank:41, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:41, trace log has been written to txt...
rank:41, finish release GPU memory ...
rank:41, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 42, finish warm up ...
rank_id = 42, input_tensor_shapes: [(2048, 2, 12288)]
rank:42,cuda fwd time: 165.49171447753906
rank:42, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.8,timestamp=4407156326.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.42,timestamp=4407156339.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.5,timestamp=4407156352.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.68,timestamp=4407156366.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.46,timestamp=4407156379.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.71,timestamp=4407156393.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.92,timestamp=4407156407.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.46,timestamp=4407156421.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.36,timestamp=4407156435.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.29,timestamp=4407156449.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.49,timestamp=4407156463.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4407156477.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 42, finish FWD profile ...
rank:42, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.78,timestamp=4407156499.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.64,timestamp=4407156531.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=26.11,timestamp=4407156558.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.28,timestamp=4407156590.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.72,timestamp=4407156616.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.93,timestamp=4407156647.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.73,timestamp=4407156673.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.94,timestamp=4407156705.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.92,timestamp=4407156730.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.97,timestamp=4407156762.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.09,timestamp=4407156788.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.01,timestamp=4407156820.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:42, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:42,optimizer_step time: 49.15507125854492
rank:42, finish optimizer.step profile ...
rank:42, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:42, trace log has been written to txt...
rank:42, finish release GPU memory ...
rank:42, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 7): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 43, finish warm up ...
rank_id = 43, input_tensor_shapes: [(2048, 2, 12288)]
rank:43,cuda fwd time: 165.97605895996094
rank:43, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.76,timestamp=4407158398.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.4,timestamp=4407158411.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.14,timestamp=4407158425.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4407158438.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.56,timestamp=4407158452.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.29,timestamp=4407158466.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.29,timestamp=4407158480.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.35,timestamp=4407158494.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.14,timestamp=4407158508.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.12,timestamp=4407158522.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.45,timestamp=4407158536.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.96,timestamp=4407158550.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 43, finish FWD profile ...
rank:43, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.12,timestamp=4407158572.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.25,timestamp=4407158604.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.16,timestamp=4407158629.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.94,timestamp=4407158661.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.05,timestamp=4407158687.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.85,timestamp=4407158719.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.02,timestamp=4407158744.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.91,timestamp=4407158776.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.92,timestamp=4407158802.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.99,timestamp=4407158834.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.91,timestamp=4407158859.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.07,timestamp=4407158891.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:43, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:43,optimizer_step time: 49.29228973388672
rank:43, finish optimizer.step profile ...
rank:43, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:43, trace log has been written to txt...
rank:43, finish release GPU memory ...
rank:43, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 7): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 44, finish warm up ...
rank_id = 44, input_tensor_shapes: [(2048, 2, 12288)]
rank:44,cuda fwd time: 168.69273376464844
rank:44, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.58,timestamp=4407160464.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.42,timestamp=4407160477.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.16,timestamp=4407160490.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.6,timestamp=4407160503.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.76,timestamp=4407160517.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.29,timestamp=4407160531.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.4,timestamp=4407160550.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4407160563.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.73,timestamp=4407160577.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4407160591.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.24,timestamp=4407160605.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4407160618.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 44, finish FWD profile ...
rank:44, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.6,timestamp=4407160639.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.44,timestamp=4407160671.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.2,timestamp=4407160697.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.85,timestamp=4407160729.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.13,timestamp=4407160755.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.88,timestamp=4407160787.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.89,timestamp=4407160812.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.69,timestamp=4407160844.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.98,timestamp=4407160869.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.03,timestamp=4407160901.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.73,timestamp=4407160927.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.0,timestamp=4407160959.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:44, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:44,optimizer_step time: 49.2861442565918
rank:44, finish optimizer.step profile ...
rank:44, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:44, trace log has been written to txt...
rank:44, finish release GPU memory ...
rank:44, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 7): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 45, finish warm up ...
rank_id = 45, input_tensor_shapes: [(2048, 2, 12288)]
rank:45,cuda fwd time: 167.7004852294922
rank:45, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.88,timestamp=4407162502.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.41,timestamp=4407162515.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.37,timestamp=4407162529.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.73,timestamp=4407162542.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.84,timestamp=4407162556.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.91,timestamp=4407162569.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.92,timestamp=4407162584.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.28,timestamp=4407162597.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.24,timestamp=4407162612.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.32,timestamp=4407162626.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.63,timestamp=4407162640.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4407162654.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 45, finish FWD profile ...
rank:45, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.39,timestamp=4407162676.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.95,timestamp=4407162708.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.84,timestamp=4407162734.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.9,timestamp=4407162765.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.9,timestamp=4407162791.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.01,timestamp=4407162823.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.96,timestamp=4407162848.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.13,timestamp=4407162880.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.2,timestamp=4407162906.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.89,timestamp=4407162938.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.21,timestamp=4407162964.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.87,timestamp=4407162996.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:45, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:45,optimizer_step time: 49.22060775756836
rank:45, finish optimizer.step profile ...
rank:45, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:45, trace log has been written to txt...
rank:45, finish release GPU memory ...
rank:45, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 7): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 46, finish warm up ...
rank_id = 46, input_tensor_shapes: [(2048, 2, 12288)]
rank:46,cuda fwd time: 166.5464324951172
rank:46, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.88,timestamp=4407164536.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.42,timestamp=4407164549.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.89,timestamp=4407164563.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4407164576.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.44,timestamp=4407164590.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4407164603.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.26,timestamp=4407164618.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.43,timestamp=4407164632.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.35,timestamp=4407164646.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.45,timestamp=4407164660.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.75,timestamp=4407164675.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.17,timestamp=4407164689.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 46, finish FWD profile ...
rank:46, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.38,timestamp=4407164710.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.37,timestamp=4407164742.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.08,timestamp=4407164768.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.83,timestamp=4407164800.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.83,timestamp=4407164825.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.74,timestamp=4407164857.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.93,timestamp=4407164883.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.17,timestamp=4407164915.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.9,timestamp=4407164940.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.0,timestamp=4407164972.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.94,timestamp=4407164998.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.0,timestamp=4407165030.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:46, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:46,optimizer_step time: 49.37830352783203
rank:46, finish optimizer.step profile ...
rank:46, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:46, trace log has been written to txt...
rank:46, finish release GPU memory ...
rank:46, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 7): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 47, finish warm up ...
rank_id = 47, input_tensor_shapes: [(2048, 2, 12288)]
rank:47,cuda fwd time: 168.92518615722656
rank:47, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.93,timestamp=4407166572.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.45,timestamp=4407166585.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.16,timestamp=4407166599.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.99,timestamp=4407166613.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.28,timestamp=4407166627.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.23,timestamp=4407166641.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.05,timestamp=4407166655.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4407166669.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.06,timestamp=4407166683.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4407166697.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.37,timestamp=4407166714.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.68,timestamp=4407166727.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 47, finish FWD profile ...
rank:47, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.77,timestamp=4407166749.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.36,timestamp=4407166781.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.57,timestamp=4407166806.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.65,timestamp=4407166837.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.66,timestamp=4407166863.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.87,timestamp=4407166895.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.13,timestamp=4407166920.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.03,timestamp=4407166952.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.15,timestamp=4407166978.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.86,timestamp=4407167010.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.98,timestamp=4407167035.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.01,timestamp=4407167067.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:47, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:47,optimizer_step time: 49.320960998535156
rank:47, finish optimizer.step profile ...
rank:47, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:47, trace log has been written to txt...
rank:47, finish release GPU memory ...
rank:47, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 8): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 48, finish warm up ...
rank_id = 48, input_tensor_shapes: [(2048, 2, 12288)]
rank:48,cuda fwd time: 170.79603576660156
rank:48, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.84,timestamp=4407168619.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.41,timestamp=4407168632.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.21,timestamp=4407168646.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.71,timestamp=4407168659.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.86,timestamp=4407168673.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4407168687.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.24,timestamp=4407168707.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4407168720.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.07,timestamp=4407168735.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4407168748.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.45,timestamp=4407168763.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.73,timestamp=4407168776.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 48, finish FWD profile ...
rank:48, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.01,timestamp=4407168797.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.69,timestamp=4407168830.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.84,timestamp=4407168855.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.81,timestamp=4407168887.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.83,timestamp=4407168914.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.24,timestamp=4407168946.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.99,timestamp=4407168971.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.9,timestamp=4407169003.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.17,timestamp=4407169029.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.88,timestamp=4407169061.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.04,timestamp=4407169086.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.24,timestamp=4407169118.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:48, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:48,optimizer_step time: 49.14995193481445
rank:48, finish optimizer.step profile ...
rank:48, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:48, trace log has been written to txt...
rank:48, finish release GPU memory ...
rank:48, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 8): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 49, finish warm up ...
rank_id = 49, input_tensor_shapes: [(2048, 2, 12288)]
rank:49,cuda fwd time: 171.16876220703125
rank:49, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.87,timestamp=4407170661.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.43,timestamp=4407170674.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.09,timestamp=4407170688.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4407170701.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.46,timestamp=4407170715.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.99,timestamp=4407170728.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.05,timestamp=4407170742.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.3,timestamp=4407170756.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.15,timestamp=4407170771.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.07,timestamp=4407170784.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.49,timestamp=4407170799.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.43,timestamp=4407170812.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 49, finish FWD profile ...
rank:49, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.29,timestamp=4407170841.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.01,timestamp=4407170873.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.85,timestamp=4407170898.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.99,timestamp=4407170930.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.58,timestamp=4407170957.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.02,timestamp=4407170989.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.8,timestamp=4407171014.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.8,timestamp=4407171046.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.82,timestamp=4407171071.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.97,timestamp=4407171103.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.92,timestamp=4407171129.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.96,timestamp=4407171161.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:49, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:49,optimizer_step time: 49.179649353027344
rank:49, finish optimizer.step profile ...
rank:49, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:49, trace log has been written to txt...
rank:49, finish release GPU memory ...
rank:49, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 8): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 50, finish warm up ...
rank_id = 50, input_tensor_shapes: [(2048, 2, 12288)]
rank:50,cuda fwd time: 163.0187530517578
rank:50, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=7.11,timestamp=4407172867.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.36,timestamp=4407172881.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.97,timestamp=4407172894.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.61,timestamp=4407172907.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.3,timestamp=4407172921.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.65,timestamp=4407172935.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.34,timestamp=4407172948.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.68,timestamp=4407172962.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.27,timestamp=4407172976.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.69,timestamp=4407172989.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.74,timestamp=4407173003.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.2,timestamp=4407173016.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 50, finish FWD profile ...
rank:50, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.51,timestamp=4407173038.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.39,timestamp=4407173070.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.72,timestamp=4407173096.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.91,timestamp=4407173127.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.78,timestamp=4407173153.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.17,timestamp=4407173184.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.03,timestamp=4407173210.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.08,timestamp=4407173242.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.15,timestamp=4407173267.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.04,timestamp=4407173299.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.14,timestamp=4407173325.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.87,timestamp=4407173357.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:50, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:50,optimizer_step time: 49.2492790222168
rank:50, finish optimizer.step profile ...
rank:50, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:50, trace log has been written to txt...
rank:50, finish release GPU memory ...
rank:50, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 8): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 51, finish warm up ...
rank_id = 51, input_tensor_shapes: [(2048, 2, 12288)]
rank:51,cuda fwd time: 162.54360961914062
rank:51, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.85,timestamp=4407174905.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.4,timestamp=4407174918.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.2,timestamp=4407174931.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.64,timestamp=4407174945.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.89,timestamp=4407174958.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.72,timestamp=4407174972.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.55,timestamp=4407174986.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.73,timestamp=4407174999.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.48,timestamp=4407175013.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4407175026.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.99,timestamp=4407175040.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.63,timestamp=4407175053.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 51, finish FWD profile ...
rank:51, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.95,timestamp=4407175075.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.59,timestamp=4407175107.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.59,timestamp=4407175132.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.91,timestamp=4407175164.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.85,timestamp=4407175190.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.14,timestamp=4407175221.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.89,timestamp=4407175246.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.03,timestamp=4407175278.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.11,timestamp=4407175304.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.29,timestamp=4407175336.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.03,timestamp=4407175362.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.86,timestamp=4407175394.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:51, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:51,optimizer_step time: 49.15302276611328
rank:51, finish optimizer.step profile ...
rank:51, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:51, trace log has been written to txt...
rank:51, finish release GPU memory ...
rank:51, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 8): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 52, finish warm up ...
rank_id = 52, input_tensor_shapes: [(2048, 2, 12288)]
rank:52,cuda fwd time: 166.3795166015625
rank:52, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.63,timestamp=4407176962.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.42,timestamp=4407176975.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.15,timestamp=4407176988.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.61,timestamp=4407177002.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.15,timestamp=4407177016.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.33,timestamp=4407177029.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.58,timestamp=4407177044.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.45,timestamp=4407177058.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.93,timestamp=4407177072.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.11,timestamp=4407177086.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.57,timestamp=4407177100.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.9,timestamp=4407177114.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 52, finish FWD profile ...
rank:52, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.11,timestamp=4407177136.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.44,timestamp=4407177168.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.07,timestamp=4407177194.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.97,timestamp=4407177226.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.83,timestamp=4407177252.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.04,timestamp=4407177284.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.06,timestamp=4407177310.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.04,timestamp=4407177342.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.79,timestamp=4407177367.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.01,timestamp=4407177399.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.82,timestamp=4407177424.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.11,timestamp=4407177456.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:52, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:52,optimizer_step time: 49.287166595458984
rank:52, finish optimizer.step profile ...
rank:52, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:52, trace log has been written to txt...
rank:52, finish release GPU memory ...
rank:52, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 8): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 53, finish warm up ...
rank_id = 53, input_tensor_shapes: [(2048, 2, 12288)]
rank:53,cuda fwd time: 166.67237854003906
rank:53, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.82,timestamp=4407179031.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.45,timestamp=4407179044.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.14,timestamp=4407179057.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.49,timestamp=4407179070.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.2,timestamp=4407179084.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.5,timestamp=4407179097.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.08,timestamp=4407179114.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.18,timestamp=4407179128.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.87,timestamp=4407179142.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.99,timestamp=4407179155.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.44,timestamp=4407179170.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4407179183.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 53, finish FWD profile ...
rank:53, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=17.91,timestamp=4407179205.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.44,timestamp=4407179237.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.47,timestamp=4407179264.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.1,timestamp=4407179296.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.75,timestamp=4407179321.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.66,timestamp=4407179353.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.79,timestamp=4407179378.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.74,timestamp=4407179410.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.89,timestamp=4407179435.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.14,timestamp=4407179467.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.97,timestamp=4407179493.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.88,timestamp=4407179525.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:53, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:53,optimizer_step time: 49.20115280151367
rank:53, finish optimizer.step profile ...
rank:53, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:53, trace log has been written to txt...
rank:53, finish release GPU memory ...
rank:53, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 9): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 54, finish warm up ...
rank_id = 54, input_tensor_shapes: [(2048, 2, 12288)]
rank:54,cuda fwd time: 174.93606567382812
rank:54, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=6.58,timestamp=4407181130.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.42,timestamp=4407181143.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.2,timestamp=4407181157.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.73,timestamp=4407181170.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.84,timestamp=4407181184.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.19,timestamp=4407181198.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.35,timestamp=4407181213.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4407181226.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.96,timestamp=4407181241.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.28,timestamp=4407181254.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.67,timestamp=4407181269.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=6.4,timestamp=4407181288.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 54, finish FWD profile ...
rank:54, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.14,timestamp=4407181310.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.15,timestamp=4407181342.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.68,timestamp=4407181367.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.89,timestamp=4407181399.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.11,timestamp=4407181424.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.13,timestamp=4407181456.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.25,timestamp=4407181482.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.81,timestamp=4407181514.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.88,timestamp=4407181540.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.88,timestamp=4407181571.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.81,timestamp=4407181597.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.93,timestamp=4407181629.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:54, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:54,optimizer_step time: 49.154048919677734
rank:54, finish optimizer.step profile ...
rank:54, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:54, trace log has been written to txt...
rank:54, finish release GPU memory ...
rank:54, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 9): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 55, finish warm up ...
rank_id = 55, input_tensor_shapes: [(2048, 2, 12288)]
rank:55,cuda fwd time: 164.92544555664062
rank:55, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.71,timestamp=4407183193.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.38,timestamp=4407183206.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.22,timestamp=4407183220.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4407183233.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.43,timestamp=4407183247.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.95,timestamp=4407183260.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.91,timestamp=4407183275.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.1,timestamp=4407183288.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.67,timestamp=4407183302.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.1,timestamp=4407183316.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.47,timestamp=4407183330.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.79,timestamp=4407183344.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 55, finish FWD profile ...
rank:55, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=17.68,timestamp=4407183366.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.21,timestamp=4407183398.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.15,timestamp=4407183424.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.91,timestamp=4407183455.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.88,timestamp=4407183481.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.89,timestamp=4407183513.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.76,timestamp=4407183538.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.94,timestamp=4407183570.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.93,timestamp=4407183595.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.23,timestamp=4407183628.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.18,timestamp=4407183653.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.14,timestamp=4407183685.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:55, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:55,optimizer_step time: 49.22265625
rank:55, finish optimizer.step profile ...
rank:55, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:55, trace log has been written to txt...
rank:55, finish release GPU memory ...
rank:55, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 9): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 56, finish warm up ...
rank_id = 56, input_tensor_shapes: [(2048, 2, 12288)]
rank:56,cuda fwd time: 165.32479858398438
rank:56, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.56,timestamp=4407185281.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.42,timestamp=4407185294.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.18,timestamp=4407185308.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4407185321.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.69,timestamp=4407185335.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4407185348.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.83,timestamp=4407185363.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.29,timestamp=4407185377.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.13,timestamp=4407185391.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.05,timestamp=4407185405.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.13,timestamp=4407185419.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4407185433.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 56, finish FWD profile ...
rank:56, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.38,timestamp=4407185455.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.26,timestamp=4407185487.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.01,timestamp=4407185513.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.41,timestamp=4407185544.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.05,timestamp=4407185570.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.44,timestamp=4407185601.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.92,timestamp=4407185627.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.99,timestamp=4407185659.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.91,timestamp=4407185684.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.32,timestamp=4407185716.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.32,timestamp=4407185742.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.18,timestamp=4407185774.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:56, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:56,optimizer_step time: 49.13663864135742
rank:56, finish optimizer.step profile ...
rank:56, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:56, trace log has been written to txt...
rank:56, finish release GPU memory ...
rank:56, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 9): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 57, finish warm up ...
rank_id = 57, input_tensor_shapes: [(2048, 2, 12288)]
rank:57,cuda fwd time: 164.54861450195312
rank:57, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.86,timestamp=4407187346.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.43,timestamp=4407187359.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.19,timestamp=4407187372.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.5,timestamp=4407187385.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.19,timestamp=4407187399.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4407187412.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.76,timestamp=4407187426.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.29,timestamp=4407187440.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.25,timestamp=4407187454.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.36,timestamp=4407187468.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.63,timestamp=4407187483.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.7,timestamp=4407187496.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 57, finish FWD profile ...
rank:57, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.74,timestamp=4407187517.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.31,timestamp=4407187549.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.85,timestamp=4407187576.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.27,timestamp=4407187608.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.41,timestamp=4407187634.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.8,timestamp=4407187666.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.77,timestamp=4407187691.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.12,timestamp=4407187723.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.11,timestamp=4407187749.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.85,timestamp=4407187781.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.07,timestamp=4407187806.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.98,timestamp=4407187838.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:57, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:57,optimizer_step time: 49.17657470703125
rank:57, finish optimizer.step profile ...
rank:57, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:57, trace log has been written to txt...
rank:57, finish release GPU memory ...
rank:57, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 9): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 58, finish warm up ...
rank_id = 58, input_tensor_shapes: [(2048, 2, 12288)]
rank:58,cuda fwd time: 166.60479736328125
rank:58, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.89,timestamp=4407189447.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.43,timestamp=4407189460.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.44,timestamp=4407189474.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4407189488.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.72,timestamp=4407189502.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.26,timestamp=4407189516.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.04,timestamp=4407189530.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4407189544.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.07,timestamp=4407189558.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.29,timestamp=4407189572.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.67,timestamp=4407189586.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4407189600.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 58, finish FWD profile ...
rank:58, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.58,timestamp=4407189622.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.2,timestamp=4407189654.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.6,timestamp=4407189679.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.93,timestamp=4407189711.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.22,timestamp=4407189737.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.1,timestamp=4407189769.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.16,timestamp=4407189795.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.01,timestamp=4407189827.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.08,timestamp=4407189853.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.13,timestamp=4407189885.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.02,timestamp=4407189910.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.83,timestamp=4407189942.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:58, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:58,optimizer_step time: 49.03731155395508
rank:58, finish optimizer.step profile ...
rank:58, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:58, trace log has been written to txt...
rank:58, finish release GPU memory ...
rank:58, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 9): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 59, finish warm up ...
rank_id = 59, input_tensor_shapes: [(2048, 2, 12288)]
rank:59,cuda fwd time: 165.6985626220703
rank:59, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.83,timestamp=4407191493.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.44,timestamp=4407191507.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.3,timestamp=4407191520.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4407191534.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.17,timestamp=4407191548.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.3,timestamp=4407191562.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.17,timestamp=4407191576.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.08,timestamp=4407191590.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.55,timestamp=4407191604.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.99,timestamp=4407191618.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.4,timestamp=4407191632.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.7,timestamp=4407191645.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 59, finish FWD profile ...
rank:59, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.59,timestamp=4407191667.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.26,timestamp=4407191699.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.12,timestamp=4407191725.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.98,timestamp=4407191756.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.92,timestamp=4407191782.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.85,timestamp=4407191814.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.14,timestamp=4407191840.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.88,timestamp=4407191871.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.84,timestamp=4407191897.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.96,timestamp=4407191929.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.85,timestamp=4407191954.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.88,timestamp=4407191986.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:59, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:59,optimizer_step time: 49.19398498535156
rank:59, finish optimizer.step profile ...
rank:59, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:59, trace log has been written to txt...
rank:59, finish release GPU memory ...
rank:59, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 10): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 60, finish warm up ...
rank_id = 60, input_tensor_shapes: [(2048, 2, 12288)]
rank:60,cuda fwd time: 177.89439392089844
rank:60, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.85,timestamp=4407193531.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.44,timestamp=4407193544.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.48,timestamp=4407193558.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.66,timestamp=4407193571.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.7,timestamp=4407193585.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.0,timestamp=4407193599.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.04,timestamp=4407193613.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.29,timestamp=4407193627.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.19,timestamp=4407193641.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.27,timestamp=4407193655.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.8,timestamp=4407193669.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4407193683.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 60, finish FWD profile ...
rank:60, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=17.05,timestamp=4407193717.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.25,timestamp=4407193748.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.54,timestamp=4407193773.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.94,timestamp=4407193805.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.1,timestamp=4407193830.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.96,timestamp=4407193862.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.05,timestamp=4407193888.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.85,timestamp=4407193920.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.95,timestamp=4407193945.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.02,timestamp=4407193977.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.06,timestamp=4407194003.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.29,timestamp=4407194035.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:60, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:60,optimizer_step time: 49.15507125854492
rank:60, finish optimizer.step profile ...
rank:60, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:60, trace log has been written to txt...
rank:60, finish release GPU memory ...
rank:60, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 10): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 61, finish warm up ...
rank_id = 61, input_tensor_shapes: [(2048, 2, 12288)]
rank:61,cuda fwd time: 167.00006103515625
rank:61, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.84,timestamp=4407195593.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.42,timestamp=4407195606.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.29,timestamp=4407195620.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4407195633.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.93,timestamp=4407195647.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4407195661.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.03,timestamp=4407195675.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4407195689.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.35,timestamp=4407195703.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4407195717.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.37,timestamp=4407195731.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.04,timestamp=4407195745.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 61, finish FWD profile ...
rank:61, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.77,timestamp=4407195769.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.4,timestamp=4407195801.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.56,timestamp=4407195826.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.91,timestamp=4407195858.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.01,timestamp=4407195884.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.11,timestamp=4407195916.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.16,timestamp=4407195942.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.87,timestamp=4407195974.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.06,timestamp=4407195999.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.88,timestamp=4407196031.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.77,timestamp=4407196056.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.65,timestamp=4407196088.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:61, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:61,optimizer_step time: 49.15097427368164
rank:61, finish optimizer.step profile ...
rank:61, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:61, trace log has been written to txt...
rank:61, finish release GPU memory ...
rank:61, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 10): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 62, finish warm up ...
rank_id = 62, input_tensor_shapes: [(2048, 2, 12288)]
rank:62,cuda fwd time: 165.54495239257812
rank:62, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.83,timestamp=4407197634.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.42,timestamp=4407197647.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.19,timestamp=4407197661.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.64,timestamp=4407197674.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.79,timestamp=4407197688.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.3,timestamp=4407197702.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.15,timestamp=4407197716.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.11,timestamp=4407197730.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.97,timestamp=4407197744.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4407197758.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.61,timestamp=4407197772.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4407197786.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 62, finish FWD profile ...
rank:62, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=17.28,timestamp=4407197808.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.4,timestamp=4407197840.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.12,timestamp=4407197866.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.99,timestamp=4407197897.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.6,timestamp=4407197924.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.02,timestamp=4407197956.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.84,timestamp=4407197981.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.74,timestamp=4407198013.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.84,timestamp=4407198038.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.0,timestamp=4407198070.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.91,timestamp=4407198095.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.02,timestamp=4407198127.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:62, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:62,optimizer_step time: 49.256446838378906
rank:62, finish optimizer.step profile ...
rank:62, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:62, trace log has been written to txt...
rank:62, finish release GPU memory ...
rank:62, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 10): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 63, finish warm up ...
rank_id = 63, input_tensor_shapes: [(2048, 2, 12288)]
rank:63,cuda fwd time: 166.0641326904297
rank:63, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.81,timestamp=4407199666.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.4,timestamp=4407199680.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.4,timestamp=4407199693.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.85,timestamp=4407199707.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.12,timestamp=4407199721.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.23,timestamp=4407199735.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.1,timestamp=4407199749.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4407199762.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.1,timestamp=4407199777.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.28,timestamp=4407199791.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.63,timestamp=4407199805.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4407199819.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 63, finish FWD profile ...
rank:63, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=17.24,timestamp=4407199840.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.56,timestamp=4407199872.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.78,timestamp=4407199898.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.63,timestamp=4407199929.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.24,timestamp=4407199955.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.61,timestamp=4407199987.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.78,timestamp=4407200012.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.78,timestamp=4407200044.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.08,timestamp=4407200070.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.81,timestamp=4407200102.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.04,timestamp=4407200128.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.86,timestamp=4407200159.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:63, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:63,optimizer_step time: 49.14790344238281
rank:63, finish optimizer.step profile ...
rank:63, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:63, trace log has been written to txt...
rank:63, finish release GPU memory ...
rank:63, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 10): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 64, finish warm up ...
rank_id = 64, input_tensor_shapes: [(2048, 2, 12288)]
rank:64,cuda fwd time: 165.4343719482422
rank:64, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.66,timestamp=4407201682.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.42,timestamp=4407201695.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.26,timestamp=4407201708.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4407201722.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.49,timestamp=4407201735.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4407201749.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.14,timestamp=4407201763.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.29,timestamp=4407201777.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.21,timestamp=4407201792.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.09,timestamp=4407201805.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.52,timestamp=4407201819.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4407201833.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 64, finish FWD profile ...
rank:64, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.36,timestamp=4407201855.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.32,timestamp=4407201887.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.05,timestamp=4407201913.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.1,timestamp=4407201945.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.82,timestamp=4407201972.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.18,timestamp=4407202004.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.04,timestamp=4407202029.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.97,timestamp=4407202061.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.89,timestamp=4407202087.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.01,timestamp=4407202119.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.92,timestamp=4407202144.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.27,timestamp=4407202176.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:64, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:64,optimizer_step time: 49.2677116394043
rank:64, finish optimizer.step profile ...
rank:64, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:64, trace log has been written to txt...
rank:64, finish release GPU memory ...
rank:64, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 10): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 65, finish warm up ...
rank_id = 65, input_tensor_shapes: [(2048, 2, 12288)]
rank:65,cuda fwd time: 165.8603515625
rank:65, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.86,timestamp=4407203718.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.39,timestamp=4407203731.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.39,timestamp=4407203744.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4407203758.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.03,timestamp=4407203772.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4407203786.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.05,timestamp=4407203800.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4407203814.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.0,timestamp=4407203828.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.29,timestamp=4407203842.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.65,timestamp=4407203856.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.8,timestamp=4407203870.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 65, finish FWD profile ...
rank:65, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=16.95,timestamp=4407203892.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.28,timestamp=4407203924.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.83,timestamp=4407203949.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.67,timestamp=4407203981.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.24,timestamp=4407204007.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.64,timestamp=4407204038.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.79,timestamp=4407204064.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.02,timestamp=4407204095.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.14,timestamp=4407204121.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.9,timestamp=4407204153.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.05,timestamp=4407204179.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.91,timestamp=4407204210.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:65, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:65,optimizer_step time: 49.18988800048828
rank:65, finish optimizer.step profile ...
rank:65, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:65, trace log has been written to txt...
rank:65, finish release GPU memory ...
rank:65, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 11): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 66, finish warm up ...
rank_id = 66, input_tensor_shapes: [(2048, 2, 12288)]
rank:66,cuda fwd time: 166.4358367919922
rank:66, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.86,timestamp=4407205788.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.42,timestamp=4407205802.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.57,timestamp=4407205815.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.8,timestamp=4407205829.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.95,timestamp=4407205843.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.89,timestamp=4407205856.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.97,timestamp=4407205870.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.43,timestamp=4407205884.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.32,timestamp=4407205899.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.36,timestamp=4407205913.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.69,timestamp=4407205927.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4407205941.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 66, finish FWD profile ...
rank:66, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.75,timestamp=4407205963.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.38,timestamp=4407205995.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.82,timestamp=4407206021.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.17,timestamp=4407206054.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.73,timestamp=4407206079.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.87,timestamp=4407206111.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.75,timestamp=4407206136.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.07,timestamp=4407206168.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.12,timestamp=4407206194.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.8,timestamp=4407206226.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.78,timestamp=4407206251.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.99,timestamp=4407206283.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:66, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:66,optimizer_step time: 49.23392105102539
rank:66, finish optimizer.step profile ...
rank:66, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:66, trace log has been written to txt...
rank:66, finish release GPU memory ...
rank:66, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 11): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 67, finish warm up ...
rank_id = 67, input_tensor_shapes: [(2048, 2, 12288)]
rank:67,cuda fwd time: 164.89369201660156
rank:67, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.87,timestamp=4407207823.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.41,timestamp=4407207836.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.11,timestamp=4407207849.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.49,timestamp=4407207862.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.45,timestamp=4407207876.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.99,timestamp=4407207889.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.06,timestamp=4407207904.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.29,timestamp=4407207918.1,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.19,timestamp=4407207932.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4407207946.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.45,timestamp=4407207960.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4407207974.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 67, finish FWD profile ...
rank:67, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=17.49,timestamp=4407207996.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.38,timestamp=4407208028.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.8,timestamp=4407208054.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.24,timestamp=4407208087.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.81,timestamp=4407208112.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.51,timestamp=4407208144.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.85,timestamp=4407208169.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.07,timestamp=4407208201.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.92,timestamp=4407208226.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.84,timestamp=4407208258.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.91,timestamp=4407208284.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.07,timestamp=4407208316.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:67, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:67,optimizer_step time: 49.181697845458984
rank:67, finish optimizer.step profile ...
rank:67, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:67, trace log has been written to txt...
rank:67, finish release GPU memory ...
rank:67, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 11): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 68, finish warm up ...
rank_id = 68, input_tensor_shapes: [(2048, 2, 12288)]
rank:68,cuda fwd time: 165.39852905273438
rank:68, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=7.43,timestamp=4407209891.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.41,timestamp=4407209904.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.17,timestamp=4407209917.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4407209930.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.68,timestamp=4407209944.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.29,timestamp=4407209958.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.11,timestamp=4407209973.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.21,timestamp=4407209986.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.92,timestamp=4407210001.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4407210014.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.37,timestamp=4407210028.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4407210042.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 68, finish FWD profile ...
rank:68, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=17.64,timestamp=4407210065.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.38,timestamp=4407210097.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.93,timestamp=4407210122.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.86,timestamp=4407210154.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.14,timestamp=4407210180.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.84,timestamp=4407210212.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.89,timestamp=4407210237.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.93,timestamp=4407210269.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.92,timestamp=4407210294.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.02,timestamp=4407210326.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.95,timestamp=4407210352.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.15,timestamp=4407210384.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:68, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:68,optimizer_step time: 49.21446228027344
rank:68, finish optimizer.step profile ...
rank:68, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:68, trace log has been written to txt...
rank:68, finish release GPU memory ...
rank:68, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 11): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 69, finish warm up ...
rank_id = 69, input_tensor_shapes: [(2048, 2, 12288)]
rank:69,cuda fwd time: 166.4583740234375
rank:69, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.84,timestamp=4407211927.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.4,timestamp=4407211940.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.47,timestamp=4407211954.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.72,timestamp=4407211967.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.98,timestamp=4407211982.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4407211995.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.09,timestamp=4407212009.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.33,timestamp=4407212023.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.24,timestamp=4407212038.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.38,timestamp=4407212052.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.67,timestamp=4407212066.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.99,timestamp=4407212080.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 69, finish FWD profile ...
rank:69, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.13,timestamp=4407212102.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.38,timestamp=4407212134.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.86,timestamp=4407212159.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.85,timestamp=4407212191.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.65,timestamp=4407212216.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.97,timestamp=4407212248.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.01,timestamp=4407212274.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.31,timestamp=4407212306.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.18,timestamp=4407212332.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.02,timestamp=4407212364.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.08,timestamp=4407212390.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.84,timestamp=4407212421.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:69, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:69,optimizer_step time: 49.20012664794922
rank:69, finish optimizer.step profile ...
rank:69, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:69, trace log has been written to txt...
rank:69, finish release GPU memory ...
rank:69, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 11): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 70, finish warm up ...
rank_id = 70, input_tensor_shapes: [(2048, 2, 12288)]
rank:70,cuda fwd time: 165.4640655517578
rank:70, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.81,timestamp=4407213966.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.41,timestamp=4407213979.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.14,timestamp=4407213993.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4407214006.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.82,timestamp=4407214020.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.18,timestamp=4407214034.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.17,timestamp=4407214048.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.1,timestamp=4407214062.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.6,timestamp=4407214076.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.12,timestamp=4407214090.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.52,timestamp=4407214104.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4407214118.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 70, finish FWD profile ...
rank:70, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=17.21,timestamp=4407214140.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.08,timestamp=4407214172.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.89,timestamp=4407214197.8,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.84,timestamp=4407214229.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.87,timestamp=4407214255.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.89,timestamp=4407214286.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.09,timestamp=4407214312.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.8,timestamp=4407214344.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.9,timestamp=4407214369.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.92,timestamp=4407214401.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.75,timestamp=4407214426.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.98,timestamp=4407214458.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:70, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:70,optimizer_step time: 49.179649353027344
rank:70, finish optimizer.step profile ...
rank:70, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:70, trace log has been written to txt...
rank:70, finish release GPU memory ...
rank:70, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 11): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 71, finish warm up ...
rank_id = 71, input_tensor_shapes: [(2048, 2, 12288)]
rank:71,cuda fwd time: 165.62789916992188
rank:71, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.87,timestamp=4407215996.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.4,timestamp=4407216010.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.33,timestamp=4407216023.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4407216037.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.0,timestamp=4407216051.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.09,timestamp=4407216065.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.64,timestamp=4407216079.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4407216092.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.92,timestamp=4407216107.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4407216120.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.5,timestamp=4407216134.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4407216148.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 71, finish FWD profile ...
rank:71, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.57,timestamp=4407216170.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.18,timestamp=4407216202.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.47,timestamp=4407216227.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.84,timestamp=4407216259.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.17,timestamp=4407216285.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.91,timestamp=4407216317.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.84,timestamp=4407216342.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.99,timestamp=4407216374.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.14,timestamp=4407216400.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.89,timestamp=4407216432.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.03,timestamp=4407216457.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.89,timestamp=4407216489.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:71, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:71,optimizer_step time: 49.27180862426758
rank:71, finish optimizer.step profile ...
rank:71, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:71, trace log has been written to txt...
rank:71, finish release GPU memory ...
rank:71, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 12): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 72, finish warm up ...
rank_id = 72, input_tensor_shapes: [(2048, 2, 12288)]
rank:72,cuda fwd time: 167.81004333496094
rank:72, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.82,timestamp=4407218038.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.4,timestamp=4407218051.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.88,timestamp=4407218065.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4407218078.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.71,timestamp=4407218092.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.3,timestamp=4407218106.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.21,timestamp=4407218120.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.12,timestamp=4407218134.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.88,timestamp=4407218148.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.99,timestamp=4407218161.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.25,timestamp=4407218176.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.68,timestamp=4407218189.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 72, finish FWD profile ...
rank:72, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.42,timestamp=4407218214.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.84,timestamp=4407218246.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.84,timestamp=4407218271.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.89,timestamp=4407218303.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.1,timestamp=4407218329.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.98,timestamp=4407218361.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.09,timestamp=4407218386.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.82,timestamp=4407218418.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.92,timestamp=4407218444.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.89,timestamp=4407218475.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.94,timestamp=4407218501.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.07,timestamp=4407218533.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:72, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:72,optimizer_step time: 49.1591682434082
rank:72, finish optimizer.step profile ...
rank:72, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:72, trace log has been written to txt...
rank:72, finish release GPU memory ...
rank:72, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 12): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 73, finish warm up ...
rank_id = 73, input_tensor_shapes: [(2048, 2, 12288)]
rank:73,cuda fwd time: 165.02989196777344
rank:73, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.84,timestamp=4407220092.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.41,timestamp=4407220106.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.42,timestamp=4407220119.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.72,timestamp=4407220133.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.79,timestamp=4407220147.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4407220160.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.89,timestamp=4407220175.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.0,timestamp=4407220188.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.93,timestamp=4407220202.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.0,timestamp=4407220216.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.31,timestamp=4407220230.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.67,timestamp=4407220244.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 73, finish FWD profile ...
rank:73, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=17.78,timestamp=4407220265.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.62,timestamp=4407220298.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.9,timestamp=4407220323.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.81,timestamp=4407220355.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.81,timestamp=4407220380.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.08,timestamp=4407220411.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.64,timestamp=4407220437.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.8,timestamp=4407220468.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.13,timestamp=4407220494.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.85,timestamp=4407220526.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.9,timestamp=4407220551.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.02,timestamp=4407220583.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:73, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:73,optimizer_step time: 49.24313735961914
rank:73, finish optimizer.step profile ...
rank:73, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:73, trace log has been written to txt...
rank:73, finish release GPU memory ...
rank:73, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 12): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 74, finish warm up ...
rank_id = 74, input_tensor_shapes: [(2048, 2, 12288)]
rank:74,cuda fwd time: 165.0831298828125
rank:74, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.85,timestamp=4407222149.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.4,timestamp=4407222162.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.95,timestamp=4407222176.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4407222189.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.53,timestamp=4407222202.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.99,timestamp=4407222216.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.1,timestamp=4407222230.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.3,timestamp=4407222244.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.16,timestamp=4407222258.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.05,timestamp=4407222272.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.53,timestamp=4407222286.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.16,timestamp=4407222300.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 74, finish FWD profile ...
rank:74, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.22,timestamp=4407222323.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.3,timestamp=4407222355.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.06,timestamp=4407222380.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.94,timestamp=4407222412.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.84,timestamp=4407222439.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.35,timestamp=4407222471.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.08,timestamp=4407222497.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.1,timestamp=4407222529.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.74,timestamp=4407222554.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.99,timestamp=4407222586.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.25,timestamp=4407222612.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.9,timestamp=4407222643.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:74, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:74,optimizer_step time: 49.22060775756836
rank:74, finish optimizer.step profile ...
rank:74, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:74, trace log has been written to txt...
rank:74, finish release GPU memory ...
rank:74, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 12): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 75, finish warm up ...
rank_id = 75, input_tensor_shapes: [(2048, 2, 12288)]
rank:75,cuda fwd time: 163.28089904785156
rank:75, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.83,timestamp=4407224220.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.43,timestamp=4407224233.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.18,timestamp=4407224247.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.48,timestamp=4407224260.29,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.27,timestamp=4407224273.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.49,timestamp=4407224286.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.93,timestamp=4407224300.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4407224314.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.06,timestamp=4407224328.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4407224342.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.49,timestamp=4407224356.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.0,timestamp=4407224370.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 75, finish FWD profile ...
rank:75, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.75,timestamp=4407224391.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.59,timestamp=4407224424.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.52,timestamp=4407224450.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.09,timestamp=4407224482.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.76,timestamp=4407224507.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.78,timestamp=4407224539.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.85,timestamp=4407224564.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.95,timestamp=4407224596.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.61,timestamp=4407224621.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.02,timestamp=4407224653.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.21,timestamp=4407224679.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.85,timestamp=4407224711.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:75, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:75,optimizer_step time: 49.23904037475586
rank:75, finish optimizer.step profile ...
rank:75, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:75, trace log has been written to txt...
rank:75, finish release GPU memory ...
rank:75, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 12): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 76, finish warm up ...
rank_id = 76, input_tensor_shapes: [(2048, 2, 12288)]
rank:76,cuda fwd time: 164.20045471191406
rank:76, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.8,timestamp=4407226251.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.41,timestamp=4407226264.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.21,timestamp=4407226277.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.47,timestamp=4407226290.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.22,timestamp=4407226304.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.53,timestamp=4407226317.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.01,timestamp=4407226331.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.38,timestamp=4407226345.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.17,timestamp=4407226359.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.0,timestamp=4407226373.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.44,timestamp=4407226387.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4407226401.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 76, finish FWD profile ...
rank:76, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.04,timestamp=4407226423.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.26,timestamp=4407226455.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.6,timestamp=4407226481.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.16,timestamp=4407226513.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.16,timestamp=4407226539.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.59,timestamp=4407226570.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.9,timestamp=4407226596.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.8,timestamp=4407226628.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.74,timestamp=4407226653.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.03,timestamp=4407226685.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.91,timestamp=4407226710.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.95,timestamp=4407226742.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:76, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:76,optimizer_step time: 49.19705581665039
rank:76, finish optimizer.step profile ...
rank:76, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:76, trace log has been written to txt...
rank:76, finish release GPU memory ...
rank:76, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 12): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 77, finish warm up ...
rank_id = 77, input_tensor_shapes: [(2048, 2, 12288)]
rank:77,cuda fwd time: 162.8968963623047
rank:77, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.69,timestamp=4407228270.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.44,timestamp=4407228283.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.26,timestamp=4407228297.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.51,timestamp=4407228310.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.26,timestamp=4407228323.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.53,timestamp=4407228336.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.35,timestamp=4407228350.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4407228363.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.97,timestamp=4407228377.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.3,timestamp=4407228391.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.64,timestamp=4407228405.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.17,timestamp=4407228419.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 77, finish FWD profile ...
rank:77, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=17.54,timestamp=4407228441.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.47,timestamp=4407228473.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=26.08,timestamp=4407228500.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.46,timestamp=4407228532.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=26.07,timestamp=4407228559.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.17,timestamp=4407228591.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.16,timestamp=4407228617.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.88,timestamp=4407228649.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.11,timestamp=4407228674.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.99,timestamp=4407228706.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.18,timestamp=4407228732.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.84,timestamp=4407228764.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:77, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:77,optimizer_step time: 49.181697845458984
rank:77, finish optimizer.step profile ...
rank:77, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:77, trace log has been written to txt...
rank:77, finish release GPU memory ...
rank:77, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 13): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 78, finish warm up ...
rank_id = 78, input_tensor_shapes: [(2048, 2, 12288)]
rank:78,cuda fwd time: 165.49171447753906
rank:78, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.82,timestamp=4407230383.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.41,timestamp=4407230396.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.38,timestamp=4407230410.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.64,timestamp=4407230423.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.69,timestamp=4407230437.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.75,timestamp=4407230450.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.8,timestamp=4407230464.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.29,timestamp=4407230478.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.54,timestamp=4407230493.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.26,timestamp=4407230506.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.99,timestamp=4407230521.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4407230535.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 78, finish FWD profile ...
rank:78, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.12,timestamp=4407230557.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.41,timestamp=4407230589.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.82,timestamp=4407230615.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.87,timestamp=4407230647.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.96,timestamp=4407230672.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.04,timestamp=4407230704.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.78,timestamp=4407230729.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.97,timestamp=4407230761.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.17,timestamp=4407230787.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.85,timestamp=4407230819.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.85,timestamp=4407230844.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.98,timestamp=4407230876.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:78, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:78,optimizer_step time: 49.20627212524414
rank:78, finish optimizer.step profile ...
rank:78, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:78, trace log has been written to txt...
rank:78, finish release GPU memory ...
rank:78, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 13): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 79, finish warm up ...
rank_id = 79, input_tensor_shapes: [(2048, 2, 12288)]
rank:79,cuda fwd time: 165.8726348876953
rank:79, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.6,timestamp=4407232408.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.37,timestamp=4407232422.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.4,timestamp=4407232435.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.7,timestamp=4407232449.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.73,timestamp=4407232463.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4407232476.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.81,timestamp=4407232490.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4407232504.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.18,timestamp=4407232518.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.28,timestamp=4407232532.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.43,timestamp=4407232546.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.99,timestamp=4407232560.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 79, finish FWD profile ...
rank:79, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.16,timestamp=4407232583.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.27,timestamp=4407232615.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.66,timestamp=4407232640.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.84,timestamp=4407232672.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.92,timestamp=4407232697.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.92,timestamp=4407232729.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.89,timestamp=4407232755.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.83,timestamp=4407232786.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.16,timestamp=4407232812.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.89,timestamp=4407232844.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.84,timestamp=4407232869.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.82,timestamp=4407232901.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:79, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:79,optimizer_step time: 49.09056091308594
rank:79, finish optimizer.step profile ...
rank:79, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:79, trace log has been written to txt...
rank:79, finish release GPU memory ...
rank:79, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 13): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 80, finish warm up ...
rank_id = 80, input_tensor_shapes: [(2048, 2, 12288)]
rank:80,cuda fwd time: 166.0088348388672
rank:80, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=6.61,timestamp=4407234419.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.4,timestamp=4407234432.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.54,timestamp=4407234446.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.78,timestamp=4407234459.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.89,timestamp=4407234473.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.9,timestamp=4407234486.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.87,timestamp=4407234500.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.3,timestamp=4407234514.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.25,timestamp=4407234529.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.22,timestamp=4407234542.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.99,timestamp=4407234557.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4407234570.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 80, finish FWD profile ...
rank:80, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=17.32,timestamp=4407234592.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.95,timestamp=4407234624.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.96,timestamp=4407234649.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.99,timestamp=4407234681.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.54,timestamp=4407234706.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.92,timestamp=4407234738.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.19,timestamp=4407234764.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.0,timestamp=4407234796.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.19,timestamp=4407234822.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.98,timestamp=4407234853.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.09,timestamp=4407234879.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.08,timestamp=4407234911.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:80, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:80,optimizer_step time: 49.1407356262207
rank:80, finish optimizer.step profile ...
rank:80, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:80, trace log has been written to txt...
rank:80, finish release GPU memory ...
rank:80, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 13): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 81, finish warm up ...
rank_id = 81, input_tensor_shapes: [(2048, 2, 12288)]
rank:81,cuda fwd time: 165.34425354003906
rank:81, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.49,timestamp=4407236587.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.38,timestamp=4407236600.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.03,timestamp=4407236614.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.54,timestamp=4407236627.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.87,timestamp=4407236641.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4407236654.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.71,timestamp=4407236668.95,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4407236682.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.0,timestamp=4407236696.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.29,timestamp=4407236710.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.59,timestamp=4407236725.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.75,timestamp=4407236738.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 81, finish FWD profile ...
rank:81, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=17.0,timestamp=4407236760.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.33,timestamp=4407236792.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.8,timestamp=4407236819.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.28,timestamp=4407236851.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.42,timestamp=4407236877.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.78,timestamp=4407236909.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.88,timestamp=4407236934.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.78,timestamp=4407236966.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.88,timestamp=4407236991.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.96,timestamp=4407237023.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.17,timestamp=4407237049.46,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.91,timestamp=4407237081.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:81, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:81,optimizer_step time: 49.19193649291992
rank:81, finish optimizer.step profile ...
rank:81, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:81, trace log has been written to txt...
rank:81, finish release GPU memory ...
rank:81, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 13): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 82, finish warm up ...
rank_id = 82, input_tensor_shapes: [(2048, 2, 12288)]
rank:82,cuda fwd time: 165.7487335205078
rank:82, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=7.19,timestamp=4407238884.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.37,timestamp=4407238897.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.05,timestamp=4407238911.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4407238924.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.24,timestamp=4407238938.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4407238951.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.56,timestamp=4407238965.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.25,timestamp=4407238979.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.7,timestamp=4407238994.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4407239007.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.16,timestamp=4407239022.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.03,timestamp=4407239035.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 82, finish FWD profile ...
rank:82, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.69,timestamp=4407239057.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.65,timestamp=4407239088.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.83,timestamp=4407239114.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.85,timestamp=4407239145.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.81,timestamp=4407239171.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.96,timestamp=4407239203.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.91,timestamp=4407239228.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.89,timestamp=4407239260.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.96,timestamp=4407239286.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.98,timestamp=4407239318.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.39,timestamp=4407239344.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.06,timestamp=4407239375.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:82, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:82,optimizer_step time: 49.28102493286133
rank:82, finish optimizer.step profile ...
rank:82, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:82, trace log has been written to txt...
rank:82, finish release GPU memory ...
rank:82, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 13): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 83, finish warm up ...
rank_id = 83, input_tensor_shapes: [(2048, 2, 12288)]
rank:83,cuda fwd time: 165.00941467285156
rank:83, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=7.82,timestamp=4407241034.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.38,timestamp=4407241047.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.49,timestamp=4407241061.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.6,timestamp=4407241074.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.46,timestamp=4407241088.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.8,timestamp=4407241101.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.65,timestamp=4407241115.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4407241129.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.88,timestamp=4407241144.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.9,timestamp=4407241157.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.81,timestamp=4407241172.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.64,timestamp=4407241185.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 83, finish FWD profile ...
rank:83, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.81,timestamp=4407241207.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.01,timestamp=4407241239.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.69,timestamp=4407241264.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.15,timestamp=4407241295.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.74,timestamp=4407241320.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.72,timestamp=4407241352.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.96,timestamp=4407241377.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.99,timestamp=4407241409.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.16,timestamp=4407241435.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.05,timestamp=4407241467.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.13,timestamp=4407241493.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.08,timestamp=4407241525.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:83, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:83,optimizer_step time: 49.279998779296875
rank:83, finish optimizer.step profile ...
rank:83, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:83, trace log has been written to txt...
rank:83, finish release GPU memory ...
rank:83, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 14): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 84, finish warm up ...
rank_id = 84, input_tensor_shapes: [(2048, 2, 12288)]
rank:84,cuda fwd time: 166.0528564453125
rank:84, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=4.54,timestamp=4407243146.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.39,timestamp=4407243159.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.53,timestamp=4407243173.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.76,timestamp=4407243187.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.94,timestamp=4407243201.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.88,timestamp=4407243214.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.8,timestamp=4407243228.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.07,timestamp=4407243242.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.01,timestamp=4407243257.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4407243270.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.07,timestamp=4407243285.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4407243299.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 84, finish FWD profile ...
rank:84, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=17.44,timestamp=4407243321.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.98,timestamp=4407243352.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.86,timestamp=4407243378.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.79,timestamp=4407243410.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.56,timestamp=4407243435.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.83,timestamp=4407243467.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.13,timestamp=4407243492.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.96,timestamp=4407243524.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.18,timestamp=4407243550.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.79,timestamp=4407243582.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.14,timestamp=4407243607.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.05,timestamp=4407243639.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:84, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:84,optimizer_step time: 49.0618896484375
rank:84, finish optimizer.step profile ...
rank:84, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:84, trace log has been written to txt...
rank:84, finish release GPU memory ...
rank:84, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 14): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 85, finish warm up ...
rank_id = 85, input_tensor_shapes: [(2048, 2, 12288)]
rank:85,cuda fwd time: 166.90687561035156
rank:85, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.29,timestamp=4407245331.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.3,timestamp=4407245344.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.83,timestamp=4407245358.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.73,timestamp=4407245371.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.47,timestamp=4407245385.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4407245399.57,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.68,timestamp=4407245413.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4407245427.51,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.67,timestamp=4407245441.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.36,timestamp=4407245455.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.4,timestamp=4407245470.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.99,timestamp=4407245484.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 85, finish FWD profile ...
rank:85, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.8,timestamp=4407245507.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.95,timestamp=4407245538.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.17,timestamp=4407245564.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.65,timestamp=4407245596.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.16,timestamp=4407245621.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.99,timestamp=4407245653.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.11,timestamp=4407245679.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.59,timestamp=4407245710.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.87,timestamp=4407245736.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.03,timestamp=4407245768.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.75,timestamp=4407245793.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.01,timestamp=4407245825.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:85, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:85,optimizer_step time: 49.24006271362305
rank:85, finish optimizer.step profile ...
rank:85, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:85, trace log has been written to txt...
rank:85, finish release GPU memory ...
rank:85, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 14): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 86, finish warm up ...
rank_id = 86, input_tensor_shapes: [(2048, 2, 12288)]
rank:86,cuda fwd time: 165.4599609375
rank:86, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.82,timestamp=4407247385.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.43,timestamp=4407247398.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.17,timestamp=4407247411.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4407247425.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.8,timestamp=4407247439.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.12,timestamp=4407247452.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.16,timestamp=4407247467.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.1,timestamp=4407247480.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.89,timestamp=4407247494.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.28,timestamp=4407247508.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.7,timestamp=4407247523.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4407247536.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 86, finish FWD profile ...
rank:86, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.91,timestamp=4407247559.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.94,timestamp=4407247590.64,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.34,timestamp=4407247616.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.81,timestamp=4407247648.31,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.07,timestamp=4407247673.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.02,timestamp=4407247705.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.18,timestamp=4407247731.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.84,timestamp=4407247763.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.87,timestamp=4407247788.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.91,timestamp=4407247820.65,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.92,timestamp=4407247846.14,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.03,timestamp=4407247878.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:86, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:86,optimizer_step time: 49.163265228271484
rank:86, finish optimizer.step profile ...
rank:86, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:86, trace log has been written to txt...
rank:86, finish release GPU memory ...
rank:86, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 14): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 87, finish warm up ...
rank_id = 87, input_tensor_shapes: [(2048, 2, 12288)]
rank:87,cuda fwd time: 167.46701049804688
rank:87, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=6.59,timestamp=4407249430.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.39,timestamp=4407249443.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.15,timestamp=4407249456.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4407249470.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.67,timestamp=4407249483.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.29,timestamp=4407249497.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.29,timestamp=4407249512.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.4,timestamp=4407249526.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.19,timestamp=4407249540.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4407249554.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4407249568.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4407249582.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 87, finish FWD profile ...
rank:87, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=19.12,timestamp=4407249604.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.43,timestamp=4407249636.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.61,timestamp=4407249661.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.88,timestamp=4407249693.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.1,timestamp=4407249718.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.94,timestamp=4407249750.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.87,timestamp=4407249776.23,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.96,timestamp=4407249808.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.87,timestamp=4407249833.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.98,timestamp=4407249865.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.95,timestamp=4407249890.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.12,timestamp=4407249922.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:87, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:87,optimizer_step time: 49.28819274902344
rank:87, finish optimizer.step profile ...
rank:87, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:87, trace log has been written to txt...
rank:87, finish release GPU memory ...
rank:87, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 14): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 88, finish warm up ...
rank_id = 88, input_tensor_shapes: [(2048, 2, 12288)]
rank:88,cuda fwd time: 165.34527587890625
rank:88, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=9.8,timestamp=4407251466.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.4,timestamp=4407251479.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.38,timestamp=4407251493.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.73,timestamp=4407251506.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.64,timestamp=4407251520.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.88,timestamp=4407251534.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.71,timestamp=4407251548.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4407251562.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.99,timestamp=4407251576.42,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.24,timestamp=4407251590.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.53,timestamp=4407251604.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4407251618.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 88, finish FWD profile ...
rank:88, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=17.2,timestamp=4407251640.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.92,timestamp=4407251671.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.53,timestamp=4407251697.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.86,timestamp=4407251728.86,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.86,timestamp=4407251754.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.81,timestamp=4407251786.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.84,timestamp=4407251811.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.84,timestamp=4407251843.22,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.09,timestamp=4407251868.88,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.08,timestamp=4407251900.85,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.09,timestamp=4407251926.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.83,timestamp=4407251958.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:88, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:88,optimizer_step time: 49.137664794921875
rank:88, finish optimizer.step profile ...
rank:88, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:88, trace log has been written to txt...
rank:88, finish release GPU memory ...
rank:88, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 14): 1812467712
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1812467712 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 89, finish warm up ...
rank_id = 89, input_tensor_shapes: [(2048, 2, 12288)]
rank:89,cuda fwd time: 166.11123657226562
rank:89, fwd_subop num: 12, fwd_subop: ['trace_src_func=allreduce,duration=4.5,timestamp=4407253673.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.4,timestamp=4407253686.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.4,timestamp=4407253700.44,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.76,timestamp=4407253713.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.86,timestamp=4407253727.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.89,timestamp=4407253741.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.87,timestamp=4407253755.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.3,timestamp=4407253769.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.24,timestamp=4407253783.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.43,timestamp=4407253797.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.73,timestamp=4407253812.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4407253825.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 89, finish FWD profile ...
rank:89, bwd_subop num: 12, bwd_subop: ['trace_src_func=allreduce,duration=18.95,timestamp=4407253847.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.82,timestamp=4407253878.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.16,timestamp=4407253904.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=31.06,timestamp=4407253936.71,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.64,timestamp=4407253961.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.9,timestamp=4407253993.79,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=25.18,timestamp=4407254019.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.87,timestamp=4407254051.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.84,timestamp=4407254076.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.84,timestamp=4407254108.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=24.88,timestamp=4407254133.93,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=30.81,timestamp=4407254165.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:89, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:89,optimizer_step time: 49.25030517578125
rank:89, finish optimizer.step profile ...
rank:89, Before memory release - Allocated: 60058618368, Reserved: 76778831872
rank:89, trace log has been written to txt...
rank:89, finish release GPU memory ...
rank:89, After memory release - Allocated: 30455153664, Reserved: 45669679104
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe400 recvbuff 0x7f24073fe400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe400,7f24073fe400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f1fd6000000 recvbuff 0x7f1fd6000000 count 103022592 datatype 7 op 0 root 0 comm 0xd317670 [nranks=1] stream 0x9a5f390
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f1fd6000000,7f1fd6000000,103022592,7,0,0,0xd317670,0x9a5f390)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 15): 1915514880
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1915514880 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f1fb0bf6800 recvbuff 0x7f1fb0bf6800 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f1fb0bf6800,7f1fb0bf6800,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 90, finish warm up ...
rank_id = 90, input_tensor_shapes: [(2048, 2, 12288)]
rank:90,cuda fwd time: 218.14764404296875
rank:90, fwd_subop num: 15, fwd_subop: ['trace_src_func=allreduce,duration=9.59,timestamp=4407256121.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.09,timestamp=4407256134.94,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.7,timestamp=4407256148.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4407256162.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.37,timestamp=4407256176.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.78,timestamp=4407256189.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.99,timestamp=4407256203.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.84,timestamp=4407256217.43,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.67,timestamp=4407256232.18,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.5,timestamp=4407256245.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.43,timestamp=4407256260.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.42,timestamp=4407256274.33,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=49.68,timestamp=4407256325.22,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.04,timestamp=4407256325.96,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.23,timestamp=4407256326.29,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 90, finish FWD profile ...
rank:90, bwd_subop num: 13, bwd_subop: ['trace_src_func=allreduce,duration=45.72,timestamp=4407256375.18,input__shape=[2048, 2, 12288],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=62.78,timestamp=4407256439.06,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.03,timestamp=4407256468.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.25,timestamp=4407256492.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.67,timestamp=4407256522.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.2,timestamp=4407256545.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.68,timestamp=4407256575.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.22,timestamp=4407256599.34,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.86,timestamp=4407256629.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.49,timestamp=4407256653.2,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.73,timestamp=4407256683.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.26,timestamp=4407256706.92,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.68,timestamp=4407256736.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:90, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe800 recvbuff 0x7f24073fe800 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe800,7f24073fe800,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:90,optimizer_step time: 41.661441802978516
rank:90, finish optimizer.step profile ...
rank:90, Before memory release - Allocated: 45981748224, Reserved: 78343307264
rank:90, trace log has been written to txt...
rank:90, finish release GPU memory ...
rank:90, After memory release - Allocated: 15753135104, Reserved: 37908119552
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe600 recvbuff 0x7f24073fe600 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe600,7f24073fe600,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe800 recvbuff 0x7f24073fe800 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe800,7f24073fe800,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fea00 recvbuff 0x7f24073fea00 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fea00,7f24073fea00,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f182e000000 recvbuff 0x7f182e000000 count 103022592 datatype 7 op 0 root 0 comm 0xd317670 [nranks=1] stream 0x9a5f390
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f182e000000,7f182e000000,103022592,7,0,0,0xd317670,0x9a5f390)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 15): 1915514880
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1915514880 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fee00 recvbuff 0x7f24073fee00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fee00,7f24073fee00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 91, finish warm up ...
rank_id = 91, input_tensor_shapes: [(2048, 2, 12288)]
rank:91,cuda fwd time: 219.48109436035156
rank:91, fwd_subop num: 15, fwd_subop: ['trace_src_func=allreduce,duration=9.53,timestamp=4407258914.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.35,timestamp=4407258927.53,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.75,timestamp=4407258941.36,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.96,timestamp=4407258954.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.43,timestamp=4407258968.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.04,timestamp=4407258982.15,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.9,timestamp=4407258996.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.22,timestamp=4407259010.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.91,timestamp=4407259025.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.42,timestamp=4407259039.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.65,timestamp=4407259054.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.29,timestamp=4407259067.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=50.57,timestamp=4407259119.34,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.04,timestamp=4407259119.99,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.23,timestamp=4407259120.32,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 91, finish FWD profile ...
rank:91, bwd_subop num: 13, bwd_subop: ['trace_src_func=allreduce,duration=48.16,timestamp=4407259170.22,input__shape=[2048, 2, 12288],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=62.71,timestamp=4407259234.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.22,timestamp=4407259263.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.32,timestamp=4407259287.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.43,timestamp=4407259318.25,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.81,timestamp=4407259342.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.43,timestamp=4407259372.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.7,timestamp=4407259397.27,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.39,timestamp=4407259427.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.48,timestamp=4407259451.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.45,timestamp=4407259481.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.81,timestamp=4407259506.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=29.39,timestamp=4407259536.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:91, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fee00 recvbuff 0x7f24073fee00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fee00,7f24073fee00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:91,optimizer_step time: 37.433345794677734
rank:91, finish optimizer.step profile ...
rank:91, Before memory release - Allocated: 46804962304, Reserved: 63612911616
rank:91, trace log has been written to txt...
rank:91, finish release GPU memory ...
rank:91, After memory release - Allocated: 15753135104, Reserved: 45906657280
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fec00 recvbuff 0x7f24073fec00 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fec00,7f24073fec00,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fee00 recvbuff 0x7f24073fee00 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fee00,7f24073fee00,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073ff000 recvbuff 0x7f24073ff000 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073ff000,7f24073ff000,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f1846900000 recvbuff 0x7f1846900000 count 103022592 datatype 7 op 0 root 0 comm 0xd317670 [nranks=1] stream 0x9a5f390
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f1846900000,7f1846900000,103022592,7,0,0,0xd317670,0x9a5f390)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 15): 1915514880
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1915514880 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073ffc00 recvbuff 0x7f24073ffc00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073ffc00,7f24073ffc00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 92, finish warm up ...
rank_id = 92, input_tensor_shapes: [(2048, 2, 12288)]
rank:92,cuda fwd time: 209.1110382080078
rank:92, fwd_subop num: 15, fwd_subop: ['trace_src_func=allreduce,duration=10.01,timestamp=4407261053.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.63,timestamp=4407261065.98,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4407261081.28,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.08,timestamp=4407261093.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.0,timestamp=4407261108.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.46,timestamp=4407261120.97,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.19,timestamp=4407261134.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.47,timestamp=4407261147.12,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.24,timestamp=4407261160.66,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.43,timestamp=4407261173.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.19,timestamp=4407261186.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.48,timestamp=4407261199.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=45.83,timestamp=4407261248.15,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4407261248.74,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.23,timestamp=4407261249.06,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 92, finish FWD profile ...
rank:92, bwd_subop num: 13, bwd_subop: ['trace_src_func=allreduce,duration=44.78,timestamp=4407261296.89,input__shape=[2048, 2, 12288],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=61.73,timestamp=4407261359.9,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=27.59,timestamp=4407261389.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.9,timestamp=4407261412.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.37,timestamp=4407261442.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.9,timestamp=4407261465.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.39,timestamp=4407261495.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.94,timestamp=4407261519.01,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=27.64,timestamp=4407261548.47,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.93,timestamp=4407261572.08,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.42,timestamp=4407261601.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.93,timestamp=4407261625.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.44,timestamp=4407261654.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:92, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073ffc00 recvbuff 0x7f24073ffc00 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073ffc00,7f24073ffc00,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:92,optimizer_step time: 37.22547149658203
rank:92, finish optimizer.step profile ...
rank:92, Before memory release - Allocated: 46804962304, Reserved: 63612911616
rank:92, trace log has been written to txt...
rank:92, finish release GPU memory ...
rank:92, After memory release - Allocated: 15753135104, Reserved: 45302677504
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073ff200 recvbuff 0x7f24073ff200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073ff200,7f24073ff200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073ff400 recvbuff 0x7f24073ff400 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073ff400,7f24073ff400,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073ff600 recvbuff 0x7f24073ff600 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073ff600,7f24073ff600,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f1816000000 recvbuff 0x7f1816000000 count 103022592 datatype 7 op 0 root 0 comm 0xd317670 [nranks=1] stream 0x9a5f390
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f1816000000,7f1816000000,103022592,7,0,0,0xd317670,0x9a5f390)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 15): 1915514880
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1915514880 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f2391bfe200 recvbuff 0x7f2391bfe200 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f2391bfe200,7f2391bfe200,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 93, finish warm up ...
rank_id = 93, input_tensor_shapes: [(2048, 2, 12288)]
rank:93,cuda fwd time: 211.2419891357422
rank:93, fwd_subop num: 15, fwd_subop: ['trace_src_func=allreduce,duration=9.38,timestamp=4407263300.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.9,timestamp=4407263312.83,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.07,timestamp=4407263326.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.95,timestamp=4407263338.73,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.19,timestamp=4407263352.35,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.33,timestamp=4407263364.96,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.76,timestamp=4407263378.56,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.34,timestamp=4407263391.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4407263404.77,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.31,timestamp=4407263417.38,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.78,timestamp=4407263430.99,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.3,timestamp=4407263443.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=45.74,timestamp=4407263496.91,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.04,timestamp=4407263497.66,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.23,timestamp=4407263497.99,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 93, finish FWD profile ...
rank:93, bwd_subop num: 13, bwd_subop: ['trace_src_func=allreduce,duration=44.79,timestamp=4407263544.57,input__shape=[2048, 2, 12288],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=61.82,timestamp=4407263607.55,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=27.78,timestamp=4407263637.0,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.95,timestamp=4407263660.59,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.47,timestamp=4407263690.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.98,timestamp=4407263713.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.51,timestamp=4407263743.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.99,timestamp=4407263766.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.51,timestamp=4407263796.05,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.94,timestamp=4407263819.63,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.5,timestamp=4407263849.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.99,timestamp=4407263872.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.52,timestamp=4407263902.03,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:93, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073fe200 recvbuff 0x7f24073fe200 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073fe200,7f24073fe200,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:93,optimizer_step time: 37.22137451171875
rank:93, finish optimizer.step profile ...
rank:93, Before memory release - Allocated: 46804962304, Reserved: 63562579968
rank:93, trace log has been written to txt...
rank:93, finish release GPU memory ...
rank:93, After memory release - Allocated: 15753135104, Reserved: 45504004096
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073ff800 recvbuff 0x7f24073ff800 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073ff800,7f24073ff800,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073ffa00 recvbuff 0x7f24073ffa00 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073ffa00,7f24073ffa00,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073ffc00 recvbuff 0x7f24073ffc00 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073ffc00,7f24073ffc00,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f182e900000 recvbuff 0x7f182e900000 count 103022592 datatype 7 op 0 root 0 comm 0xd317670 [nranks=1] stream 0x9a5f390
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f182e900000,7f182e900000,103022592,7,0,0,0xd317670,0x9a5f390)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 15): 1915514880
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1915514880 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f2391bff800 recvbuff 0x7f2391bff800 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f2391bff800,7f2391bff800,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 94, finish warm up ...
rank_id = 94, input_tensor_shapes: [(2048, 2, 12288)]
rank:94,cuda fwd time: 222.0779571533203
rank:94, fwd_subop num: 15, fwd_subop: ['trace_src_func=allreduce,duration=9.88,timestamp=4407265490.39,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.42,timestamp=4407265503.5,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.68,timestamp=4407265517.54,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.14,timestamp=4407265530.82,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.77,timestamp=4407265545.11,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.29,timestamp=4407265558.52,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.07,timestamp=4407265572.78,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.3,timestamp=4407265586.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.04,timestamp=4407265600.49,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.31,timestamp=4407265613.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.39,timestamp=4407265636.61,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.12,timestamp=4407265649.84,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=47.12,timestamp=4407265698.01,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4407265698.63,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.23,timestamp=4407265698.95,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 94, finish FWD profile ...
rank:94, bwd_subop num: 13, bwd_subop: ['trace_src_func=allreduce,duration=45.27,timestamp=4407265745.84,input__shape=[2048, 2, 12288],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=62.34,timestamp=4407265809.3,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=27.96,timestamp=4407265838.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.15,timestamp=4407265862.69,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.61,timestamp=4407265892.24,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.19,timestamp=4407265916.02,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.64,timestamp=4407265945.58,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.2,timestamp=4407265969.37,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.64,timestamp=4407265998.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.2,timestamp=4407266022.7,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.74,timestamp=4407266052.4,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.41,timestamp=4407266076.41,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.87,timestamp=4407266106.19,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:94, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f2391bfe800 recvbuff 0x7f2391bfe800 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f2391bfe800,7f2391bfe800,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:94,optimizer_step time: 41.382911682128906
rank:94, finish optimizer.step profile ...
rank:94, Before memory release - Allocated: 46804962304, Reserved: 63596134400
rank:94, trace log has been written to txt...
rank:94, finish release GPU memory ...
rank:94, After memory release - Allocated: 15753135104, Reserved: 45504004096
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      20
    validation: 2
    test:       2
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(20, 2, 2), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f245b38d850>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from af0c292c27849f3ba008a4ed16ee6d02-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f24073ffe00 recvbuff 0x7f24073ffe00 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f24073ffe00,7f24073ffe00,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 293a6bd97906b1bd25135bd44457fa68-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f1baadf6000 recvbuff 0x7f1baadf6000 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f1baadf6000,7f1baadf6000,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 11438c5224f9b104e85fd3df0d4aba80-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f1baadf6200 recvbuff 0x7f1baadf6200 count 1 datatype 1 op 0 root 0 comm 0x9a487d0 [nranks=1] stream 0x9258f50
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f1baadf6200,7f1baadf6200,1,1,0,0,0x9a487d0,0x9258f50)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f2163200000 recvbuff 0x7f2163200000 count 103022592 datatype 7 op 0 root 0 comm 0xd317670 [nranks=1] stream 0x9a5f390
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f2163200000,7f2163200000,103022592,7,0,0,0xd317670,0x9a5f390)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 15): 1915514880
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1915514880 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f245b3e8820>)
> learning rate decay style: cosine
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f1fb0a1e800 recvbuff 0x7f1fb0a1e800 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f1fb0a1e800,7f1fb0a1e800,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank_id = 95, finish warm up ...
rank_id = 95, input_tensor_shapes: [(2048, 2, 12288)]
rank:95,cuda fwd time: 207.37432861328125
rank:95, fwd_subop num: 15, fwd_subop: ['trace_src_func=allreduce,duration=9.71,timestamp=4407267641.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.96,timestamp=4407267653.74,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.2,timestamp=4407267667.89,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.44,timestamp=4407267680.48,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.25,timestamp=4407267694.04,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.44,timestamp=4407267706.62,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.39,timestamp=4407267720.17,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.44,timestamp=4407267732.75,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.35,timestamp=4407267746.32,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.44,timestamp=4407267758.91,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=9.19,timestamp=4407267773.87,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.47,timestamp=4407267786.45,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=45.85,timestamp=4407267833.35,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4407267834.77,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.23,timestamp=4407267835.09,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 95, finish FWD profile ...
rank:95, bwd_subop num: 13, bwd_subop: ['trace_src_func=allreduce,duration=44.78,timestamp=4407267881.6,input__shape=[2048, 2, 12288],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=61.87,timestamp=4407267944.6,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=27.85,timestamp=4407267974.07,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.96,timestamp=4407267997.67,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.5,timestamp=4407268027.09,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=22.99,timestamp=4407268050.68,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.55,timestamp=4407268080.13,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.01,timestamp=4407268103.72,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.46,timestamp=4407268133.16,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.03,timestamp=4407268156.76,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.56,timestamp=4407268186.21,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=23.02,timestamp=4407268209.81,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.55,timestamp=4407268239.26,input__shape=[4096, 12288],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:95, finish BWD profile ...
proj187:3256792:3256792 NCCL CALL ncclGroupStart()
proj187:3256792:3256792 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f1fb0a1e800 recvbuff 0x7f1fb0a1e800 count 1 datatype 7 op 0 root 0 comm 0x19f6c600 [nranks=1] stream 0x9a68310
proj187:3256792:3256792 NCCL CALL ncclAllReduce(7f1fb0a1e800,7f1fb0a1e800,1,7,0,0,0x19f6c600,0x9a68310)
proj187:3256792:3256792 NCCL CALL ncclGroupEnd()
rank:95,optimizer_step time: 37.2408332824707
rank:95, finish optimizer.step profile ...
rank:95, Before memory release - Allocated: 46804962304, Reserved: 63596134400
rank:95, trace log has been written to txt...
rank:95, finish release GPU memory ...
rank:95, After memory release - Allocated: 15753135104, Reserved: 45504004096
