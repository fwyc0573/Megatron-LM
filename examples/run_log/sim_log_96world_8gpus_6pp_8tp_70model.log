using world size: 1, data-parallel size: 1, context-parallel size: 1 tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
using torch.float32 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.999
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. True
  add_position_embedding .......................... True
  add_qkv_bias .................................... False
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  apply_rope_fusion ............................... True
  async_tensor_model_parallel_allreduce ........... True
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  auto_detect_ckpt_format ......................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  bias_swiglu_fusion .............................. True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  check_for_nan_in_loss_and_grad .................. True
  check_weight_hash_across_dp_replicas_interval ... None
  ckpt_fully_parallel_save ........................ False
  ckpt_step ....................................... None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  clone_scatter_output_in_embedding ............... True
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_size ........................... 1
  create_attention_mask_in_dataloader ............. True
  data_cache_path ................................. None
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 1
  data_path ....................................... ['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  ddp_bucket_size ................................. None
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  decoupled_lr .................................... None
  decoupled_min_lr ................................ None
  delay_grad_reduce ............................... True
  delay_param_gather .............................. False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  disable_straggler_on_startup .................... False
  dist_ckpt_format ................................ torch_dist
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 10
  do_trace ........................................ True
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  enable_one_logger ............................... False
  encoder_num_layers .............................. 80
  encoder_seq_length .............................. 2048
  end_weight_decay ................................ 0.01
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_model_parallel_size ...................... 1
  fake_dp ......................................... 2
  fake_gpus_per_node .............................. 8
  fake_local_rank ................................. 0
  fake_pp ......................................... 6
  fake_tp ......................................... 8
  fake_world_size ................................. 96
  fake_wrank ...................................... 0
  ffn_hidden_size ................................. 32768
  finetune ........................................ False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 4
  gradient_accumulation_fusion .................... True
  group_query_attention ........................... False
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 8192
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  is_scaling_mode ................................. True
  iter_per_epoch .................................. 1250
  kv_channels ..................................... 128
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 100
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_progress .................................... False
  log_straggler ................................... False
  log_throughput .................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.00015
  lr_decay_iters .................................. 320000
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. 0.01
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  main_tokenizer_type ............................. GPT2BPETokenizer
  make_vocab_size_divisible_by .................... 128
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 2048
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/gpt2-merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_bin_files .................................. True
  mock_data ....................................... False
  moe_aux_loss_coeff .............................. 0.0
  moe_grouped_gemm ................................ False
  moe_input_jitter_eps ............................ None
  moe_per_layer_logging ........................... False
  moe_router_load_balancing_type .................. aux_loss
  moe_router_topk ................................. 2
  moe_token_dispatcher_type ....................... allgather
  moe_token_dropping .............................. False
  moe_z_loss_coeff ................................ None
  nccl_communicator_config_path ................... None
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... LayerNorm
  nsight_start .................................... 10
  num_attention_heads ............................. 64
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... None
  num_layers ...................................... 80
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 1
  num_workers ..................................... 2
  one_logger_entity ............................... hwinf_dcm
  one_logger_project .............................. e2e-tracking
  one_logger_run_name ............................. None
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  overlap_param_gather ............................ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.float32
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... learned_absolute
  pretrained_checkpoint ........................... None
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  qk_layernorm .................................... False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... None
  recompute_method ................................ None
  recompute_num_layers ............................ None
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_attention_gate ............................ 1
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_project_dir ............................... None
  retro_verify_neighbor_count ..................... True
  rotary_interleaved .............................. False
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 2048
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  spec ............................................ None
  split ........................................... 949,50,1
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.01
  straggler_ctrlr_port ............................ 65535
  straggler_minmax_count .......................... 1
  swiglu .......................................... False
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  test_mode ....................................... False
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. None
  tokenizer_type .................................. GPT2BPETokenizer
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. False
  tp_comm_overlap_ag .............................. True
  tp_comm_overlap_cfg ............................. None
  tp_comm_overlap_rs .............................. True
  tp_comm_overlap_rs_dgrad ........................ False
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  trace_start ..................................... 10
  train_data_path ................................. None
  train_iters ..................................... 10
  train_samples ................................... None
  transformer_impl ................................ transformer_engine
  transformer_pipeline_model_parallel_size ........ 1
  untie_embeddings_and_output_weights ............. False
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_dist_ckpt ................................... False
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. False
  use_mcore_models ................................ True
  use_one_sent_docs ............................... False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  use_tp_pp_dp_mapping ............................ False
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/gpt2-vocab.json
  vocab_size ...................................... 3200
  wandb_exp_name .................................. 
  wandb_project ................................... 
  wandb_save_dir .................................. 
  weight_decay .................................... 0.01
  weight_decay_incr_style ......................... constant
  world_size ...................................... 1
  yaml_cfg ........................................ None
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 2
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initialized tensor model parallel with size 1
> initialized pipeline model parallel with size 1
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/datasets'
make: Nothing to be done for 'default'.
make: Leaving directory '/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.051 seconds
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
> compiling and loading fused kernels ...
proj187:3296790:3296790 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ens81f0
proj187:3296790:3296790 [0] NCCL INFO NCCL_SOCKET_IFNAME set to ens81f0
proj187:3296790:3296790 [0] NCCL INFO Bootstrap : Using ens81f0:192.168.50.187<0>
proj187:3296790:3296790 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
proj187:3296790:3296790 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
proj187:3296790:3296790 NCCL CALL ncclGetUniqueId(0xc7e49534beab579e)
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.18.6+cuda12.1
proj187:3296790:3296790 [0] NCCL INFO init.cc:1584 Cuda Host Alloc Size 4 pointer 0x7f8b75400000
proj187:3296790:3297175 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
proj187:3296790:3297175 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ens81f0
proj187:3296790:3297175 [0] NCCL INFO NET/Socket : Using [0]ens81f0:192.168.50.187<0>
proj187:3296790:3297175 [0] NCCL INFO Using network Socket
proj187:3296790:3297175 [0] NCCL INFO comm 0x8d98720 rank 0 nranks 1 cudaDev 0 nvmlDev 7 busId ca000 commId 0xc7e49534beab579e - Init START
proj187:3296790:3297175 [0] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'ens81f0'
proj187:3296790:3297175 [0] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
proj187:3296790:3297175 [0] NCCL INFO CPU/1 (1/1/2)
proj187:3296790:3297175 [0] NCCL INFO + PCI[24.0] - PCI/C1000 (1000c01010000000)
proj187:3296790:3297175 [0] NCCL INFO               + PCI[24.0] - PCI/C8000 (1000c01010de13b8)
proj187:3296790:3297175 [0] NCCL INFO                             + PCI[24.0] - GPU/CA000 (0)
proj187:3296790:3297175 [0] NCCL INFO                                           + NVL[160.0] - NVS/0
proj187:3296790:3297175 [0] NCCL INFO + SYS[10.0] - CPU/0
proj187:3296790:3297175 [0] NCCL INFO CPU/0 (1/1/2)
proj187:3296790:3297175 [0] NCCL INFO + SYS[10.0] - CPU/1
proj187:3296790:3297175 [0] NCCL INFO + PCI[3.0] - NIC/17000
proj187:3296790:3297175 [0] NCCL INFO ==========================================
proj187:3296790:3297175 [0] NCCL INFO GPU/CA000 :GPU/CA000 (0/5000.000000/LOC) NVS/0 (1/160.000000/NVL) CPU/1 (3/24.000000/PHB) CPU/0 (4/10.000000/SYS) 
proj187:3296790:3297175 [0] NCCL INFO Setting affinity for GPU 7 to ffff0000,ffff0000
proj187:3296790:3297175 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 40.000000/40.000000, type LOC/PIX, sameChannels 1
proj187:3296790:3297175 [0] NCCL INFO  0 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO  1 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO  2 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO  3 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO  4 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO  5 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO  6 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO  7 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO  8 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO  9 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO 10 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO 11 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO 12 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO 13 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO 14 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO 15 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 40.000000/40.000000, type LOC/PIX, sameChannels 1
proj187:3296790:3297175 [0] NCCL INFO  0 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO  1 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO  2 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO  3 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO  4 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO  5 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO  6 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO  7 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO  8 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO  9 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO 10 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO 11 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO 12 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO 13 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO 14 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO 15 : GPU/0
proj187:3296790:3297175 [0] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297175 [0] NCCL INFO Channel 00/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 01/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 02/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 03/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 04/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 05/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 06/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 07/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 08/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 09/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 10/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 11/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 12/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 13/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 14/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 15/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 16/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 17/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 18/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 19/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 20/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 21/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 22/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 23/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 24/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 25/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 26/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 27/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 28/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 29/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 30/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Channel 31/32 :    0
proj187:3296790:3297175 [0] NCCL INFO Ring 00 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 01 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 02 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 03 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 04 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 05 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 06 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 07 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 08 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 09 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 10 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 11 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 12 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 13 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 14 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 15 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 16 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 17 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 18 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 19 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 20 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 21 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 22 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 23 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 24 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 25 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 26 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 27 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 28 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 29 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 30 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Ring 31 : 0 -> 0 -> 0
proj187:3296790:3297175 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
proj187:3296790:3297175 [0] NCCL INFO P2P Chunksize set to 131072
proj187:3296790:3297175 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c00000
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c00200
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c00400
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c00600
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c00800
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c00a00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c00c00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c00e00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c01000
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c01200
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c01400
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c01600
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c01800
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c01a00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c01c00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c01e00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c02000
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c02200
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c02400
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c02600
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c02800
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c02a00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c02c00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c02e00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c03000
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c03200
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c03400
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c03600
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c03800
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c03a00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c03c00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c03e00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c04000
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c04200
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c04400
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c04600
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c04800
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c04a00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c04c00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c04e00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c05000
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c05200
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c05400
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c05600
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c05800
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c05a00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c05c00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c05e00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c06000
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c06200
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c06400
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c06600
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c06800
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c06a00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c06c00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c06e00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c07000
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c07200
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c07400
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c07600
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c07800
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c07a00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c07c00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c07e00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c08000
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c08200
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c08400
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c08600
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c08800
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c08a00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c08c00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c08e00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c09000
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c09200
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c09400
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c09600
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c09800
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c09a00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c09c00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c09e00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c0a000
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c0a200
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c0a400
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c0a600
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c0a800
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c0aa00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c0ac00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c0ae00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c0b000
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c0b200
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c0b400
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c0b600
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c0b800
proj187:3296790:3297175 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c0ba00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c0bc00
proj187:3296790:3297175 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c0be00
proj187:3296790:3297175 [0] NCCL INFO Connected all rings
proj187:3296790:3297175 [0] NCCL INFO Connected all trees
proj187:3296790:3297175 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
proj187:3296790:3297185 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x7f8b58002f20
proj187:3296790:3297185 [0] NCCL INFO Allocated 4194660 bytes of shared memory in /dev/shm/nccl-qaRB4x
proj187:3296790:3297185 [0] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
proj187:3296790:3297185 [0] NCCL INFO proxyProgressAsync opId=0x7f8b4a1b33e0 op.type=1 op.reqBuff=0x7f8b58000bb0 op.respSize=16 done
proj187:3296790:3297175 [0] NCCL INFO ncclPollProxyResponse Received new opId=0x7f8b4a1b33e0
proj187:3296790:3297175 [0] NCCL INFO recvOpId=0x7f8b4a1b33e0 matches expected opId=0x7f8b4a1b33e0
proj187:3296790:3297185 [0] NCCL INFO Received and initiated operation=Init res=0
proj187:3296790:3297175 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x7f8b58003160
proj187:3296790:3297185 [0] NCCL INFO transport/net.cc:446 Cuda Alloc Size 67108864 pointer 0x7f8b3c000000
proj187:3296790:3297185 [0] NCCL INFO proxyProgressAsync opId=0x7f8b4a1b33e0 op.type=2 op.reqBuff=0x7f8b58005cc0 op.respSize=0 done
proj187:3296790:3297175 [0] NCCL INFO ncclPollProxyResponse Received new opId=0x7f8b4a1b33e0
proj187:3296790:3297185 [0] NCCL INFO Received and initiated operation=SharedInit res=0
proj187:3296790:3297175 [0] NCCL INFO recvOpId=0x7f8b4a1b33e0 matches expected opId=0x7f8b4a1b33e0
proj187:3296790:3297175 [0] NCCL INFO init.cc:387 Cuda Alloc Size 7728 pointer 0x7f8b47c0c000
proj187:3296790:3297175 [0] NCCL INFO init.cc:412 Cuda Host Alloc Size 33554432 pointer 0x7f8b3a000000
proj187:3296790:3297175 [0] NCCL INFO init.cc:418 Cuda Host Alloc Size 128 pointer 0x7f8b75400200
proj187:3296790:3297175 NCCL CALL ncclCommInitRank(0x8d98720, 1, 0xc7e49534beab579e, 0, 0)
proj187:3296790:3297175 [0] NCCL INFO comm 0x8d98720 rank 0 nranks 1 cudaDev 0 nvmlDev 7 busId ca000 commId 0xc7e49534beab579e - Init COMPLETE
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b75200200 recvbuff 0x7f8b75200200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b75200200,7f8b75200200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b75200000 recvbuff 0x7f8b75200000 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b75200000,7f8b75200000,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
>>> done with compiling and loading fused kernels. Compilation time: 0.880 seconds
/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/training/initialize.py:405: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400412039/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b75200000 recvbuff 0x7f8b75200000 count 1 datatype 8 op 3 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b75200000,7f8b75200000,1,8,3,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
time to initialize megatron (seconds): 2.050
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b75200200 recvbuff 0x7f8b75200200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b75200200,7f8b75200200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
[after megatron is initialized] datetime: 2024-09-22 17:12:51 
mpu_info:MPUInfo:
	dp_size=2
	tp_size=8
	pp_size=6
	mp_size=48
	world_size=96
	dp_groups=[[0, 8], [1, 9], [2, 10], [3, 11], [4, 12], [5, 13], [6, 14], [7, 15], [16, 24], [17, 25], [18, 26], [19, 27], [20, 28], [21, 29], [22, 30], [23, 31], [32, 40], [33, 41], [34, 42], [35, 43], [36, 44], [37, 45], [38, 46], [39, 47], [48, 56], [49, 57], [50, 58], [51, 59], [52, 60], [53, 61], [54, 62], [55, 63], [64, 72], [65, 73], [66, 74], [67, 75], [68, 76], [69, 77], [70, 78], [71, 79], [80, 88], [81, 89], [82, 90], [83, 91], [84, 92], [85, 93], [86, 94], [87, 95]]
	pp_groups=[[0, 16, 32, 48, 64, 80], [1, 17, 33, 49, 65, 81], [2, 18, 34, 50, 66, 82], [3, 19, 35, 51, 67, 83], [4, 20, 36, 52, 68, 84], [5, 21, 37, 53, 69, 85], [6, 22, 38, 54, 70, 86], [7, 23, 39, 55, 71, 87], [8, 24, 40, 56, 72, 88], [9, 25, 41, 57, 73, 89], [10, 26, 42, 58, 74, 90], [11, 27, 43, 59, 75, 91], [12, 28, 44, 60, 76, 92], [13, 29, 45, 61, 77, 93], [14, 30, 46, 62, 78, 94], [15, 31, 47, 63, 79, 95]]
	tp_groups=[[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31], [32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47], [48, 49, 50, 51, 52, 53, 54, 55], [56, 57, 58, 59, 60, 61, 62, 63], [64, 65, 66, 67, 68, 69, 70, 71], [72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87], [88, 89, 90, 91, 92, 93, 94, 95]]
	mp_groups=[[0, 1, 2, 3, 4, 5, 6, 7, 16, 17, 18, 19, 20, 21, 22, 23, 32, 33, 34, 35, 36, 37, 38, 39, 48, 49, 50, 51, 52, 53, 54, 55, 64, 65, 66, 67, 68, 69, 70, 71, 80, 81, 82, 83, 84, 85, 86, 87], [8, 9, 10, 11, 12, 13, 14, 15, 24, 25, 26, 27, 28, 29, 30, 31, 40, 41, 42, 43, 44, 45, 46, 47, 56, 57, 58, 59, 60, 61, 62, 63, 72, 73, 74, 75, 76, 77, 78, 79, 88, 89, 90, 91, 92, 93, 94, 95]]
	ep_groups=[[0, 80], [1, 81], [2, 82], [3, 83], [4, 84], [5, 85], [6, 86], [7, 87], [8, 88], [9, 89], [10, 90], [11, 91], [12, 92], [13, 93], [14, 94], [15, 95]]
	pep_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15]]
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b75200400 recvbuff 0x7f8b75200400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b75200400,7f8b75200400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b75200200 recvbuff 0x7f8b75200200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b75200200,7f8b75200200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b75200400 recvbuff 0x7f8b75200400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b75200400,7f8b75200400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGetUniqueId(0x1880bea2ffa04df7)
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO init.cc:1584 Cuda Host Alloc Size 4 pointer 0x7f8b75470400
proj187:3296790:3297499 [0] NCCL INFO Using network Socket
proj187:3296790:3297499 [0] NCCL INFO comm 0xc1bc3c0 rank 0 nranks 1 cudaDev 0 nvmlDev 7 busId ca000 commId 0x1880bea2ffa04df7 - Init START
proj187:3296790:3297499 [0] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'ens81f0'
proj187:3296790:3297499 [0] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
proj187:3296790:3297499 [0] NCCL INFO CPU/1 (1/1/2)
proj187:3296790:3297499 [0] NCCL INFO + PCI[24.0] - PCI/C1000 (1000c01010000000)
proj187:3296790:3297499 [0] NCCL INFO               + PCI[24.0] - PCI/C8000 (1000c01010de13b8)
proj187:3296790:3297499 [0] NCCL INFO                             + PCI[24.0] - GPU/CA000 (0)
proj187:3296790:3297499 [0] NCCL INFO                                           + NVL[160.0] - NVS/0
proj187:3296790:3297499 [0] NCCL INFO + SYS[10.0] - CPU/0
proj187:3296790:3297499 [0] NCCL INFO CPU/0 (1/1/2)
proj187:3296790:3297499 [0] NCCL INFO + SYS[10.0] - CPU/1
proj187:3296790:3297499 [0] NCCL INFO + PCI[3.0] - NIC/17000
proj187:3296790:3297499 [0] NCCL INFO ==========================================
proj187:3296790:3297499 [0] NCCL INFO GPU/CA000 :GPU/CA000 (0/5000.000000/LOC) NVS/0 (1/160.000000/NVL) CPU/1 (3/24.000000/PHB) CPU/0 (4/10.000000/SYS) 
proj187:3296790:3297499 [0] NCCL INFO Setting affinity for GPU 7 to ffff0000,ffff0000
proj187:3296790:3297499 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 40.000000/40.000000, type LOC/PIX, sameChannels 1
proj187:3296790:3297499 [0] NCCL INFO  0 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO  1 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO  2 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO  3 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO  4 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO  5 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO  6 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO  7 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO  8 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO  9 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO 10 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO 11 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO 12 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO 13 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO 14 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO 15 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 40.000000/40.000000, type LOC/PIX, sameChannels 1
proj187:3296790:3297499 [0] NCCL INFO  0 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO  1 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO  2 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO  3 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO  4 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO  5 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO  6 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO  7 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO  8 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO  9 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO 10 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO 11 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO 12 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO 13 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO 14 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO 15 : GPU/0
proj187:3296790:3297499 [0] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297499 [0] NCCL INFO Channel 00/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 01/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 02/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 03/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 04/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 05/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 06/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 07/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 08/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 09/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 10/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 11/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 12/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 13/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 14/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 15/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 16/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 17/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 18/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 19/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 20/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 21/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 22/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 23/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 24/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 25/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 26/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 27/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 28/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 29/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 30/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Channel 31/32 :    0
proj187:3296790:3297499 [0] NCCL INFO Ring 00 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 01 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 02 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 03 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 04 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 05 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 06 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 07 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 08 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 09 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 10 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 11 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 12 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 13 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 14 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 15 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 16 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 17 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 18 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 19 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 20 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 21 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 22 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 23 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 24 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 25 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 26 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 27 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 28 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 29 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 30 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Ring 31 : 0 -> 0 -> 0
proj187:3296790:3297499 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
proj187:3296790:3297499 [0] NCCL INFO P2P Chunksize set to 131072
proj187:3296790:3297499 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c0e000
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c0e200
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c0e400
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c0e600
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c0e800
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c0ea00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c0ec00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c0ee00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c0f000
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c0f200
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c0f400
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c0f600
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c0f800
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c0fa00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c0fc00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c0fe00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c10000
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c10200
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c10400
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c10600
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c10800
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c10a00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c10c00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c10e00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c11000
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c11200
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c11400
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c11600
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c11800
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c11a00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c11c00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c11e00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c12000
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c12200
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c12400
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c12600
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c12800
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c12a00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c12c00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c12e00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c13000
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c13200
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c13400
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c13600
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c13800
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c13a00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c13c00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c13e00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c14000
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c14200
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c14400
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c14600
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c14800
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c14a00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c14c00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c14e00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c15000
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c15200
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c15400
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c15600
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c15800
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c15a00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c15c00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c15e00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c16000
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c16200
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c16400
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c16600
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c16800
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c16a00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c16c00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c16e00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c17000
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c17200
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c17400
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c17600
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c17800
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c17a00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c17c00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c17e00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c18000
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c18200
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c18400
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c18600
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c18800
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c18a00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c18c00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c18e00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c19000
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c19200
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c19400
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c19600
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c19800
proj187:3296790:3297499 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c19a00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c19c00
proj187:3296790:3297499 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c19e00
proj187:3296790:3297499 [0] NCCL INFO Connected all rings
proj187:3296790:3297499 [0] NCCL INFO Connected all trees
proj187:3296790:3297499 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
proj187:3296790:3297500 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x7f89b4002f20
proj187:3296790:3297500 [0] NCCL INFO Allocated 4194660 bytes of shared memory in /dev/shm/nccl-Ryzahe
proj187:3296790:3297500 [0] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
proj187:3296790:3297500 [0] NCCL INFO proxyProgressAsync opId=0x7f89ac07f520 op.type=1 op.reqBuff=0x7f89b4000bb0 op.respSize=16 done
proj187:3296790:3297499 [0] NCCL INFO ncclPollProxyResponse Received new opId=0x7f89ac07f520
proj187:3296790:3297499 [0] NCCL INFO recvOpId=0x7f89ac07f520 matches expected opId=0x7f89ac07f520
proj187:3296790:3297500 [0] NCCL INFO Received and initiated operation=Init res=0
proj187:3296790:3297499 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x7f89b4003160
proj187:3296790:3297500 [0] NCCL INFO transport/net.cc:446 Cuda Alloc Size 67108864 pointer 0x7f89a8000000
proj187:3296790:3297500 [0] NCCL INFO proxyProgressAsync opId=0x7f89ac07f520 op.type=2 op.reqBuff=0x7f89b4005cc0 op.respSize=0 done
proj187:3296790:3297499 [0] NCCL INFO ncclPollProxyResponse Received new opId=0x7f89ac07f520
proj187:3296790:3297500 [0] NCCL INFO Received and initiated operation=SharedInit res=0
proj187:3296790:3297499 [0] NCCL INFO recvOpId=0x7f89ac07f520 matches expected opId=0x7f89ac07f520
proj187:3296790:3297499 [0] NCCL INFO init.cc:387 Cuda Alloc Size 7728 pointer 0x7f8b47c1a000
proj187:3296790:3297499 [0] NCCL INFO init.cc:412 Cuda Host Alloc Size 33554432 pointer 0x7f89a6000000
proj187:3296790:3297499 [0] NCCL INFO init.cc:418 Cuda Host Alloc Size 128 pointer 0x7f8b75470600
proj187:3296790:3297499 NCCL CALL ncclCommInitRank(0xc1bc3c0, 1, 0x1880bea2ffa04df7, 0, 0)
proj187:3296790:3297499 [0] NCCL INFO comm 0xc1bc3c0 rank 0 nranks 1 cudaDev 0 nvmlDev 7 busId ca000 commId 0x1880bea2ffa04df7 - Init COMPLETE
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b60000000 recvbuff 0x7f8b60000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b60000000,7f8b60000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1377643520
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1377643520 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGetUniqueId(0xc489f0c000ccf5dc)
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO init.cc:1584 Cuda Host Alloc Size 4 pointer 0x7f8b75470800
proj187:3296790:3297512 [0] NCCL INFO Using network Socket
proj187:3296790:3297512 [0] NCCL INFO comm 0x194705e0 rank 0 nranks 1 cudaDev 0 nvmlDev 7 busId ca000 commId 0xc489f0c000ccf5dc - Init START
proj187:3296790:3297512 [0] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'ens81f0'
proj187:3296790:3297512 [0] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
proj187:3296790:3297512 [0] NCCL INFO CPU/1 (1/1/2)
proj187:3296790:3297512 [0] NCCL INFO + PCI[24.0] - PCI/C1000 (1000c01010000000)
proj187:3296790:3297512 [0] NCCL INFO               + PCI[24.0] - PCI/C8000 (1000c01010de13b8)
proj187:3296790:3297512 [0] NCCL INFO                             + PCI[24.0] - GPU/CA000 (0)
proj187:3296790:3297512 [0] NCCL INFO                                           + NVL[160.0] - NVS/0
proj187:3296790:3297512 [0] NCCL INFO + SYS[10.0] - CPU/0
proj187:3296790:3297512 [0] NCCL INFO CPU/0 (1/1/2)
proj187:3296790:3297512 [0] NCCL INFO + SYS[10.0] - CPU/1
proj187:3296790:3297512 [0] NCCL INFO + PCI[3.0] - NIC/17000
proj187:3296790:3297512 [0] NCCL INFO ==========================================
proj187:3296790:3297512 [0] NCCL INFO GPU/CA000 :GPU/CA000 (0/5000.000000/LOC) NVS/0 (1/160.000000/NVL) CPU/1 (3/24.000000/PHB) CPU/0 (4/10.000000/SYS) 
proj187:3296790:3297512 [0] NCCL INFO Setting affinity for GPU 7 to ffff0000,ffff0000
proj187:3296790:3297512 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 40.000000/40.000000, type LOC/PIX, sameChannels 1
proj187:3296790:3297512 [0] NCCL INFO  0 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO  1 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO  2 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO  3 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO  4 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO  5 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO  6 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO  7 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO  8 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO  9 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO 10 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO 11 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO 12 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO 13 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO 14 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO 15 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 40.000000/40.000000, type LOC/PIX, sameChannels 1
proj187:3296790:3297512 [0] NCCL INFO  0 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO  1 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO  2 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO  3 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO  4 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO  5 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO  6 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO  7 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO  8 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO  9 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO 10 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO 11 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO 12 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO 13 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO 14 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO 15 : GPU/0
proj187:3296790:3297512 [0] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
proj187:3296790:3297512 [0] NCCL INFO Channel 00/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 01/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 02/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 03/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 04/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 05/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 06/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 07/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 08/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 09/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 10/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 11/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 12/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 13/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 14/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 15/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 16/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 17/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 18/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 19/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 20/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 21/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 22/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 23/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 24/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 25/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 26/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 27/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 28/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 29/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 30/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Channel 31/32 :    0
proj187:3296790:3297512 [0] NCCL INFO Ring 00 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 01 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 02 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 03 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 04 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 05 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 06 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 07 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 08 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 09 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 10 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 11 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 12 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 13 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 14 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 15 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 16 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 17 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 18 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 19 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 20 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 21 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 22 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 23 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 24 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 25 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 26 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 27 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 28 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 29 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 30 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Ring 31 : 0 -> 0 -> 0
proj187:3296790:3297512 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
proj187:3296790:3297512 [0] NCCL INFO P2P Chunksize set to 131072
proj187:3296790:3297512 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c5c800
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c5ca00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c5cc00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c5ce00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c5d000
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c5d200
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c5d400
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c5d600
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c5d800
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c5da00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c5dc00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c5de00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c5e000
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c5e200
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c5e400
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c5e600
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c5e800
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c5ea00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c5ec00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c5ee00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c5f000
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c5f200
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c5f400
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c5f600
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c5f800
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c5fa00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c5fc00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c5fe00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c60000
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c60200
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c60400
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c60600
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c60800
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c60a00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c60c00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c60e00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c61000
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c61200
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c61400
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c61600
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c61800
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c61a00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c61c00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c61e00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c62000
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c62200
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c62400
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c62600
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c62800
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c62a00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c62c00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c62e00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c63000
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c63200
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c63400
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c63600
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c63800
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c63a00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c63c00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c63e00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c64000
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c64200
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c64400
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c64600
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c64800
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c64a00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c64c00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c64e00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c65000
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c65200
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c65400
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c65600
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c65800
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c65a00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c65c00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c65e00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c66000
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c66200
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c66400
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c66600
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c66800
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c66a00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c66c00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c66e00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c67000
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c67200
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c67400
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c67600
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c67800
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c67a00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c67c00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c67e00
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c68000
proj187:3296790:3297512 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x7f8b47c68200
proj187:3296790:3297512 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x7f8b47c68400
proj187:3296790:3297512 [0] NCCL INFO channel.cc:54 Cuda Alloc Size 4 pointer 0x7f8b47c68600
proj187:3296790:3297512 [0] NCCL INFO Connected all rings
proj187:3296790:3297512 [0] NCCL INFO Connected all trees
proj187:3296790:3297512 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
proj187:3296790:3297513 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x7f83c0002f20
proj187:3296790:3297513 [0] NCCL INFO Allocated 4194660 bytes of shared memory in /dev/shm/nccl-nECVZa
proj187:3296790:3297513 [0] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
proj187:3296790:3297513 [0] NCCL INFO proxyProgressAsync opId=0x7f83b407f520 op.type=1 op.reqBuff=0x7f83c0000bb0 op.respSize=16 done
proj187:3296790:3297512 [0] NCCL INFO ncclPollProxyResponse Received new opId=0x7f83b407f520
proj187:3296790:3297513 [0] NCCL INFO Received and initiated operation=Init res=0
proj187:3296790:3297512 [0] NCCL INFO recvOpId=0x7f83b407f520 matches expected opId=0x7f83b407f520
proj187:3296790:3297512 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x7f83c0003160
proj187:3296790:3297513 [0] NCCL INFO transport/net.cc:446 Cuda Alloc Size 67108864 pointer 0x7f83b0000000
proj187:3296790:3297513 [0] NCCL INFO proxyProgressAsync opId=0x7f83b407f520 op.type=2 op.reqBuff=0x7f83c0005cc0 op.respSize=0 done
proj187:3296790:3297512 [0] NCCL INFO ncclPollProxyResponse Received new opId=0x7f83b407f520
proj187:3296790:3297513 [0] NCCL INFO Received and initiated operation=SharedInit res=0
proj187:3296790:3297512 [0] NCCL INFO recvOpId=0x7f83b407f520 matches expected opId=0x7f83b407f520
proj187:3296790:3297512 [0] NCCL INFO init.cc:387 Cuda Alloc Size 7728 pointer 0x7f8b47c68800
proj187:3296790:3297512 [0] NCCL INFO init.cc:412 Cuda Host Alloc Size 33554432 pointer 0x7f83ae000000
proj187:3296790:3297512 [0] NCCL INFO init.cc:418 Cuda Host Alloc Size 128 pointer 0x7f8b75470a00
proj187:3296790:3297512 NCCL CALL ncclCommInitRank(0x194705e0, 1, 0xc489f0c000ccf5dc, 0, 0)
proj187:3296790:3297512 [0] NCCL INFO comm 0x194705e0 rank 0 nranks 1 cudaDev 0 nvmlDev 7 busId ca000 commId 0xc489f0c000ccf5dc - Init COMPLETE
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 0, finish warm up ...
rank_id = 0, input_tensor_shapes: []
rank:0,cuda fwd time: 148.62940979003906
rank:0, fwd_subop num: 27, fwd_subop: ['trace_src_func=_reduce,duration=0.09,timestamp=4409804725.71,input__shape=[2, 2048, 8192],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.45,timestamp=4409804732.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.57,timestamp=4409804737.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.9,timestamp=4409804743.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409804748.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.9,timestamp=4409804754.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409804759.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.02,timestamp=4409804765.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409804769.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409804776.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4409804781.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409804787.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4409804792.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409804798.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4409804803.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409804810.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.56,timestamp=4409804814.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409804822.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4409804827.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409804833.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409804838.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409804844.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4409804849.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.78,timestamp=4409804856.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.57,timestamp=4409804860.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.65,timestamp=4409804867.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.35,timestamp=4409804872.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 0, finish FWD profile ...
rank:0, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.7,timestamp=4409804880.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.53,timestamp=4409804894.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.13,timestamp=4409804903.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4409804916.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.14,timestamp=4409804925.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4409804939.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.15,timestamp=4409804948.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.61,timestamp=4409804961.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.1,timestamp=4409804970.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4409804983.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4409804992.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.71,timestamp=4409805006.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.24,timestamp=4409805015.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.72,timestamp=4409805029.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409805038.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.72,timestamp=4409805051.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.23,timestamp=4409805060.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.69,timestamp=4409805074.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4409805083.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4409805097.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409805106.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.78,timestamp=4409805120.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409805129.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.71,timestamp=4409805143.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.18,timestamp=4409805151.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.6,timestamp=4409805165.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:0, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:0,optimizer_step time: 37.89311981201172
rank:0, finish optimizer.step profile ...
rank:0, Before memory release - Allocated: 22202685952, Reserved: 41615884288
rank:0, trace log has been written to txt...
rank:0, finish release GPU memory ...
rank:0, After memory release - Allocated: 11047318528, Reserved: 11196694528
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b75200200 recvbuff 0x7f8b75200200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b75200200,7f8b75200200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b75200200 recvbuff 0x7f8b75200200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b75200200,7f8b75200200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f883a000000 recvbuff 0x7f883a000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f883a000000,7f883a000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1377643520
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1377643520 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 1, finish warm up ...
rank_id = 1, input_tensor_shapes: []
rank:1,cuda fwd time: 157.2321319580078
rank:1, fwd_subop num: 27, fwd_subop: ['trace_src_func=_reduce,duration=0.29,timestamp=4409806397.1,input__shape=[2, 2048, 8192],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.41,timestamp=4409806404.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4409806408.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.05,timestamp=4409806414.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409806419.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.06,timestamp=4409806425.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409806430.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.05,timestamp=4409806436.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.7,timestamp=4409806441.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409806447.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.78,timestamp=4409806452.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409806459.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4409806463.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409806470.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.78,timestamp=4409806474.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409806492.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.78,timestamp=4409806496.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409806503.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4409806508.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409806514.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.73,timestamp=4409806519.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409806525.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4409806530.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409806537.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.56,timestamp=4409806541.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.14,timestamp=4409806548.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.43,timestamp=4409806552.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 1, finish FWD profile ...
rank:1, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.66,timestamp=4409806561.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.73,timestamp=4409806574.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.19,timestamp=4409806583.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.69,timestamp=4409806597.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.19,timestamp=4409806606.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4409806619.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.18,timestamp=4409806628.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.62,timestamp=4409806642.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.17,timestamp=4409806651.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.61,timestamp=4409806664.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.18,timestamp=4409806673.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.61,timestamp=4409806687.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.17,timestamp=4409806696.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.77,timestamp=4409806709.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.52,timestamp=4409806719.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409806733.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.5,timestamp=4409806742.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4409806756.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.42,timestamp=4409806765.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409806779.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409806788.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.7,timestamp=4409806801.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.22,timestamp=4409806810.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.61,timestamp=4409806824.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.16,timestamp=4409806833.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.6,timestamp=4409806846.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:1, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:1,optimizer_step time: 37.85625457763672
rank:1, finish optimizer.step profile ...
rank:1, Before memory release - Allocated: 33224575488, Reserved: 52594475008
rank:1, trace log has been written to txt...
rank:1, finish release GPU memory ...
rank:1, After memory release - Allocated: 11047318528, Reserved: 22118662144
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff800 recvbuff 0x7f8b753ff800 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff800,7f8b753ff800,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffa00 recvbuff 0x7f8b753ffa00 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffa00,7f8b753ffa00,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffc00 recvbuff 0x7f8b753ffc00 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffc00,7f8b753ffc00,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b60000000 recvbuff 0x7f8b60000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b60000000,7f8b60000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 1377643520
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1377643520 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffc00 recvbuff 0x7f88467ffc00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffc00,7f88467ffc00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 2, finish warm up ...
rank_id = 2, input_tensor_shapes: []
rank:2,cuda fwd time: 154.5103302001953
rank:2, fwd_subop num: 27, fwd_subop: ['trace_src_func=_reduce,duration=0.3,timestamp=4409807984.67,input__shape=[2, 2048, 8192],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.57,timestamp=4409807991.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.13,timestamp=4409807996.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.21,timestamp=4409808002.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.15,timestamp=4409808007.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.24,timestamp=4409808013.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.18,timestamp=4409808018.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.24,timestamp=4409808024.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409808028.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409808035.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.73,timestamp=4409808040.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409808046.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409808051.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.26,timestamp=4409808057.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409808062.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.35,timestamp=4409808069.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409808074.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.37,timestamp=4409808080.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409808085.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.22,timestamp=4409808092.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409808096.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.38,timestamp=4409808103.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409808108.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.77,timestamp=4409808114.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.07,timestamp=4409808119.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.9,timestamp=4409808128.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.38,timestamp=4409808133.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 2, finish FWD profile ...
rank:2, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.73,timestamp=4409808146.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.74,timestamp=4409808160.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.11,timestamp=4409808169.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4409808183.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.22,timestamp=4409808192.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409808205.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.41,timestamp=4409808214.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.96,timestamp=4409808228.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.52,timestamp=4409808238.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409808252.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409808261.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4409808275.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409808284.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.71,timestamp=4409808298.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.24,timestamp=4409808306.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.62,timestamp=4409808320.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.16,timestamp=4409808329.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.6,timestamp=4409808342.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.16,timestamp=4409808351.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.68,timestamp=4409808365.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4409808374.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.9,timestamp=4409808388.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.5,timestamp=4409808397.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409808411.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.51,timestamp=4409808420.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.05,timestamp=4409808434.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:2, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff800 recvbuff 0x7f8b753ff800 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff800,7f8b753ff800,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:2,optimizer_step time: 37.812225341796875
rank:2, finish optimizer.step profile ...
rank:2, Before memory release - Allocated: 33224575488, Reserved: 52594475008
rank:2, trace log has been written to txt...
rank:2, finish release GPU memory ...
rank:2, After memory release - Allocated: 11047318528, Reserved: 22118662144
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff000 recvbuff 0x7f88467ff000 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff000,7f88467ff000,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff200 recvbuff 0x7f88467ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff200,7f88467ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f883a000000 recvbuff 0x7f883a000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f883a000000,7f883a000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 1377643520
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1377643520 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 3, finish warm up ...
rank_id = 3, input_tensor_shapes: []
rank:3,cuda fwd time: 147.13037109375
rank:3, fwd_subop num: 27, fwd_subop: ['trace_src_func=_reduce,duration=0.31,timestamp=4409809571.7,input__shape=[2, 2048, 8192],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.58,timestamp=4409809578.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409809583.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.16,timestamp=4409809589.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409809593.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409809600.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409809604.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.17,timestamp=4409809610.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4409809615.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.23,timestamp=4409809621.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409809626.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.31,timestamp=4409809633.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409809637.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.23,timestamp=4409809644.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409809648.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409809655.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409809660.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.37,timestamp=4409809666.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409809671.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409809678.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409809683.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.33,timestamp=4409809689.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409809694.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.24,timestamp=4409809701.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4409809706.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.5,timestamp=4409809712.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.5,timestamp=4409809717.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 3, finish FWD profile ...
rank:3, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.8,timestamp=4409809725.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.82,timestamp=4409809739.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409809748.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409809762.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409809771.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.96,timestamp=4409809785.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.51,timestamp=4409809794.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409809808.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.52,timestamp=4409809817.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4409809831.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4409809841.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409809854.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409809863.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.72,timestamp=4409809877.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.21,timestamp=4409809886.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.6,timestamp=4409809900.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.17,timestamp=4409809908.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.64,timestamp=4409809922.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.24,timestamp=4409809931.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.77,timestamp=4409809945.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.41,timestamp=4409809954.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.96,timestamp=4409809968.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.51,timestamp=4409809977.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409809991.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.52,timestamp=4409810000.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.95,timestamp=4409810014.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:3, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:3,optimizer_step time: 37.82963180541992
rank:3, finish optimizer.step profile ...
rank:3, Before memory release - Allocated: 33224575488, Reserved: 52594475008
rank:3, trace log has been written to txt...
rank:3, finish release GPU memory ...
rank:3, After memory release - Allocated: 11047318528, Reserved: 22118662144
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff400 recvbuff 0x7f88467ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff400,7f88467ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff600 recvbuff 0x7f88467ff600 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff600,7f88467ff600,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff800 recvbuff 0x7f88467ff800 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff800,7f88467ff800,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b60000000 recvbuff 0x7f8b60000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b60000000,7f8b60000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 0): 1377643520
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1377643520 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 4, finish warm up ...
rank_id = 4, input_tensor_shapes: []
rank:4,cuda fwd time: 147.91885375976562
rank:4, fwd_subop num: 27, fwd_subop: ['trace_src_func=_reduce,duration=0.28,timestamp=4409811351.0,input__shape=[2, 2048, 8192],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.31,timestamp=4409811357.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409811362.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.98,timestamp=4409811368.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409811373.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.05,timestamp=4409811379.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409811384.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.03,timestamp=4409811390.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.67,timestamp=4409811394.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409811401.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409811406.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409811412.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409811417.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409811424.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409811428.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409811435.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409811440.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4409811446.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409811451.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409811457.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409811462.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409811469.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409811474.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409811481.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.67,timestamp=4409811485.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.03,timestamp=4409811492.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.43,timestamp=4409811497.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 4, finish FWD profile ...
rank:4, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.79,timestamp=4409811505.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.83,timestamp=4409811519.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.22,timestamp=4409811528.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409811542.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409811551.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.07,timestamp=4409811565.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409811574.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4409811588.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409811597.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4409811611.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409811620.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.96,timestamp=4409811634.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4409811643.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.87,timestamp=4409811657.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409811666.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.78,timestamp=4409811679.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409811688.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409811702.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4409811711.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4409811725.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4409811734.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.08,timestamp=4409811748.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409811757.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409811771.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409811780.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.08,timestamp=4409811794.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:4, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:4,optimizer_step time: 37.87263870239258
rank:4, finish optimizer.step profile ...
rank:4, Before memory release - Allocated: 33224575488, Reserved: 52594475008
rank:4, trace log has been written to txt...
rank:4, finish release GPU memory ...
rank:4, After memory release - Allocated: 11047318528, Reserved: 22118662144
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff200 recvbuff 0x7f88467ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff200,7f88467ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff000 recvbuff 0x7f88467ff000 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff000,7f88467ff000,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff200 recvbuff 0x7f88467ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff200,7f88467ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f883a000000 recvbuff 0x7f883a000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f883a000000,7f883a000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 0): 1377643520
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1377643520 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 5, finish warm up ...
rank_id = 5, input_tensor_shapes: []
rank:5,cuda fwd time: 154.461181640625
rank:5, fwd_subop num: 27, fwd_subop: ['trace_src_func=_reduce,duration=0.3,timestamp=4409813139.45,input__shape=[2, 2048, 8192],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.48,timestamp=4409813146.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4409813151.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.08,timestamp=4409813157.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409813161.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.08,timestamp=4409813168.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409813172.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.13,timestamp=4409813179.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409813183.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409813190.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409813195.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4409813201.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409813206.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409813213.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409813217.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409813224.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409813229.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409813235.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409813240.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4409813247.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409813251.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4409813258.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409813263.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.29,timestamp=4409813272.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.05,timestamp=4409813279.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.05,timestamp=4409813287.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.33,timestamp=4409813292.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 5, finish FWD profile ...
rank:5, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.79,timestamp=4409813301.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.88,timestamp=4409813314.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.2,timestamp=4409813323.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.72,timestamp=4409813337.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.24,timestamp=4409813345.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.71,timestamp=4409813359.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.22,timestamp=4409813368.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.78,timestamp=4409813381.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.41,timestamp=4409813390.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4409813404.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409813413.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409813427.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409813437.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409813451.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.62,timestamp=4409813460.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.09,timestamp=4409813474.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409813483.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4409813497.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4409813506.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.78,timestamp=4409813519.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409813528.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.78,timestamp=4409813542.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409813551.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.78,timestamp=4409813565.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409813574.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4409813587.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:5, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:5,optimizer_step time: 37.945343017578125
rank:5, finish optimizer.step profile ...
rank:5, Before memory release - Allocated: 33224575488, Reserved: 52594475008
rank:5, trace log has been written to txt...
rank:5, finish release GPU memory ...
rank:5, After memory release - Allocated: 11047318528, Reserved: 22118662144
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff400 recvbuff 0x7f88467ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff400,7f88467ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff600 recvbuff 0x7f88467ff600 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff600,7f88467ff600,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff800 recvbuff 0x7f88467ff800 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff800,7f88467ff800,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b60000000 recvbuff 0x7f8b60000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b60000000,7f8b60000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 0): 1377643520
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1377643520 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 6, finish warm up ...
rank_id = 6, input_tensor_shapes: []
rank:6,cuda fwd time: 146.5128936767578
rank:6, fwd_subop num: 27, fwd_subop: ['trace_src_func=_reduce,duration=0.3,timestamp=4409814730.43,input__shape=[2, 2048, 8192],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.59,timestamp=4409814737.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409814742.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.19,timestamp=4409814748.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.13,timestamp=4409814752.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.22,timestamp=4409814759.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.14,timestamp=4409814763.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.22,timestamp=4409814769.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.62,timestamp=4409814774.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.2,timestamp=4409814780.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409814785.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409814792.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409814796.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409814803.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409814807.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409814814.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409814819.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.25,timestamp=4409814825.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409814830.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4409814836.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409814841.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.32,timestamp=4409814848.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409814852.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4409814859.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4409814864.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.5,timestamp=4409814870.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.46,timestamp=4409814875.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 6, finish FWD profile ...
rank:6, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.8,timestamp=4409814883.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.93,timestamp=4409814897.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4409814906.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4409814920.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4409814929.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409814943.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409814952.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409814966.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.62,timestamp=4409814975.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.12,timestamp=4409814989.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.5,timestamp=4409814998.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4409815012.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.44,timestamp=4409815021.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.88,timestamp=4409815035.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409815044.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.8,timestamp=4409815058.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.28,timestamp=4409815066.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409815080.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409815089.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.99,timestamp=4409815103.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409815112.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.12,timestamp=4409815126.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4409815135.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4409815149.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4409815158.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4409815172.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:6, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:6,optimizer_step time: 38.14195251464844
rank:6, finish optimizer.step profile ...
rank:6, Before memory release - Allocated: 33224575488, Reserved: 52594475008
rank:6, trace log has been written to txt...
rank:6, finish release GPU memory ...
rank:6, After memory release - Allocated: 11047318528, Reserved: 22118662144
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff200 recvbuff 0x7f88467ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff200,7f88467ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff000 recvbuff 0x7f88467ff000 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff000,7f88467ff000,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff200 recvbuff 0x7f88467ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff200,7f88467ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f883a000000 recvbuff 0x7f883a000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f883a000000,7f883a000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 0): 1377643520
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1377643520 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 7, finish warm up ...
rank_id = 7, input_tensor_shapes: []
rank:7,cuda fwd time: 150.0938262939453
rank:7, fwd_subop num: 27, fwd_subop: ['trace_src_func=_reduce,duration=0.3,timestamp=4409816318.0,input__shape=[2, 2048, 8192],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.54,timestamp=4409816324.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409816329.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.21,timestamp=4409816335.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.14,timestamp=4409816340.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.21,timestamp=4409816346.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.14,timestamp=4409816351.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.24,timestamp=4409816357.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4409816362.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.05,timestamp=4409816370.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.15,timestamp=4409816375.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409816381.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409816386.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409816392.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409816397.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.31,timestamp=4409816404.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409816408.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.35,timestamp=4409816415.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409816420.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409816426.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409816431.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.25,timestamp=4409816439.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409816444.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.23,timestamp=4409816450.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.74,timestamp=4409816455.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.42,timestamp=4409816461.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.48,timestamp=4409816466.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 7, finish FWD profile ...
rank:7, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.85,timestamp=4409816475.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.03,timestamp=4409816488.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.16,timestamp=4409816497.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.68,timestamp=4409816511.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.21,timestamp=4409816520.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.69,timestamp=4409816533.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.24,timestamp=4409816542.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409816556.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409816564.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.91,timestamp=4409816578.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4409816587.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4409816601.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4409816611.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.25,timestamp=4409816625.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409816634.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4409816648.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4409816657.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.91,timestamp=4409816671.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4409816680.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409816694.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4409816702.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.72,timestamp=4409816716.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4409816725.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.75,timestamp=4409816738.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409816747.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4409816761.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:7, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:7,optimizer_step time: 37.828609466552734
rank:7, finish optimizer.step profile ...
rank:7, Before memory release - Allocated: 33224575488, Reserved: 52594475008
rank:7, trace log has been written to txt...
rank:7, finish release GPU memory ...
rank:7, After memory release - Allocated: 11047318528, Reserved: 22118662144
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff400 recvbuff 0x7f88467ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff400,7f88467ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff600 recvbuff 0x7f88467ff600 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff600,7f88467ff600,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff800 recvbuff 0x7f88467ff800 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff800,7f88467ff800,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b60000000 recvbuff 0x7f8b60000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b60000000,7f8b60000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1377643520 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 8, finish warm up ...
rank_id = 8, input_tensor_shapes: []
rank:8,cuda fwd time: 154.5668487548828
rank:8, fwd_subop num: 27, fwd_subop: ['trace_src_func=_reduce,duration=0.24,timestamp=4409817896.29,input__shape=[2, 2048, 8192],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.25,timestamp=4409817903.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409817907.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.66,timestamp=4409817914.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4409817918.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.07,timestamp=4409817925.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.13,timestamp=4409817929.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.2,timestamp=4409817935.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4409817940.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409817947.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409817951.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.32,timestamp=4409817958.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409817962.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409817969.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409817974.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.37,timestamp=4409817980.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409817985.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.43,timestamp=4409817992.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409817997.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.36,timestamp=4409818003.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409818008.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.47,timestamp=4409818015.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4409818026.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409818033.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.71,timestamp=4409818038.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.43,timestamp=4409818044.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.45,timestamp=4409818049.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 8, finish FWD profile ...
rank:8, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.48,timestamp=4409818057.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.58,timestamp=4409818071.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409818080.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409818093.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409818102.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4409818116.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409818125.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409818139.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409818149.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409818163.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409818172.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.05,timestamp=4409818186.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4409818195.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409818209.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4409818217.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.85,timestamp=4409818231.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409818240.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409818254.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409818263.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409818276.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409818285.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409818299.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4409818308.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409818322.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409818331.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409818345.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:8, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff000 recvbuff 0x7f88467ff000 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff000,7f88467ff000,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:8,optimizer_step time: 37.8152961730957
rank:8, finish optimizer.step profile ...
rank:8, Before memory release - Allocated: 33224575488, Reserved: 52594475008
rank:8, trace log has been written to txt...
rank:8, finish release GPU memory ...
rank:8, After memory release - Allocated: 11047318528, Reserved: 22118662144
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffc00 recvbuff 0x7f88467ffc00 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffc00,7f88467ffc00,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffe00 recvbuff 0x7f88467ffe00 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffe00,7f88467ffe00,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f883a000000 recvbuff 0x7f883a000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f883a000000,7f883a000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1377643520 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff600 recvbuff 0x7f88467ff600 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff600,7f88467ff600,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 9, finish warm up ...
rank_id = 9, input_tensor_shapes: []
rank:9,cuda fwd time: 147.5010528564453
rank:9, fwd_subop num: 27, fwd_subop: ['trace_src_func=_reduce,duration=0.31,timestamp=4409819474.91,input__shape=[2, 2048, 8192],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.66,timestamp=4409819481.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.12,timestamp=4409819486.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.18,timestamp=4409819492.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.13,timestamp=4409819497.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.24,timestamp=4409819503.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.14,timestamp=4409819508.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.2,timestamp=4409819514.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409819519.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409819525.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409819530.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.33,timestamp=4409819536.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409819541.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.33,timestamp=4409819547.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409819552.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.43,timestamp=4409819559.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409819563.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.42,timestamp=4409819570.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409819575.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.21,timestamp=4409819582.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.08,timestamp=4409819586.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.04,timestamp=4409819593.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409819598.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409819605.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.69,timestamp=4409819609.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.38,timestamp=4409819616.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.46,timestamp=4409819621.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 9, finish FWD profile ...
rank:9, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.79,timestamp=4409819629.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.15,timestamp=4409819643.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409819652.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409819665.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409819674.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409819688.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.17,timestamp=4409819697.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.73,timestamp=4409819710.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.23,timestamp=4409819719.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409819733.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.41,timestamp=4409819742.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409819756.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409819765.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4409819779.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.6,timestamp=4409819788.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4409819802.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409819811.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4409819825.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409819835.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.07,timestamp=4409819848.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4409819857.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409819871.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409819880.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409819894.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409819902.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409819916.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:9, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff600 recvbuff 0x7f88467ff600 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff600,7f88467ff600,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:9,optimizer_step time: 37.86751937866211
rank:9, finish optimizer.step profile ...
rank:9, Before memory release - Allocated: 33224575488, Reserved: 52594475008
rank:9, trace log has been written to txt...
rank:9, finish release GPU memory ...
rank:9, After memory release - Allocated: 11047318528, Reserved: 22118662144
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff000 recvbuff 0x7f88467ff000 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff000,7f88467ff000,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff200 recvbuff 0x7f88467ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff200,7f88467ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff000 recvbuff 0x7f88467ff000 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff000,7f88467ff000,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b60000000 recvbuff 0x7f8b60000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b60000000,7f8b60000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1377643520 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 10, finish warm up ...
rank_id = 10, input_tensor_shapes: []
rank:10,cuda fwd time: 147.25018310546875
rank:10, fwd_subop num: 27, fwd_subop: ['trace_src_func=_reduce,duration=0.31,timestamp=4409821073.38,input__shape=[2, 2048, 8192],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.64,timestamp=4409821080.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409821084.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.2,timestamp=4409821091.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409821095.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.22,timestamp=4409821101.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.14,timestamp=4409821106.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.24,timestamp=4409821112.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409821117.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.26,timestamp=4409821123.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409821128.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409821135.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409821139.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4409821146.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409821151.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409821157.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409821162.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409821168.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409821173.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.19,timestamp=4409821180.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409821185.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409821191.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409821196.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4409821203.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4409821207.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.44,timestamp=4409821214.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.47,timestamp=4409821219.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 10, finish FWD profile ...
rank:10, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=4.67,timestamp=4409821228.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.61,timestamp=4409821245.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.1,timestamp=4409821254.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.62,timestamp=4409821267.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.14,timestamp=4409821276.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4409821290.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.14,timestamp=4409821299.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.46,timestamp=4409821312.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4409821321.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4409821335.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409821344.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4409821358.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4409821367.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409821381.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409821391.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409821405.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409821414.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.91,timestamp=4409821427.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4409821436.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.88,timestamp=4409821450.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409821459.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.78,timestamp=4409821473.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4409821482.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4409821495.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.51,timestamp=4409821504.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4409821518.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:10, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:10,optimizer_step time: 37.76921463012695
rank:10, finish optimizer.step profile ...
rank:10, Before memory release - Allocated: 33224575488, Reserved: 52594475008
rank:10, trace log has been written to txt...
rank:10, finish release GPU memory ...
rank:10, After memory release - Allocated: 11047318528, Reserved: 22118662144
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff600 recvbuff 0x7f88467ff600 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff600,7f88467ff600,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff800 recvbuff 0x7f88467ff800 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff800,7f88467ff800,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f883a000000 recvbuff 0x7f883a000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f883a000000,7f883a000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1377643520 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff200 recvbuff 0x7f88467ff200 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff200,7f88467ff200,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 11, finish warm up ...
rank_id = 11, input_tensor_shapes: []
rank:11,cuda fwd time: 147.66796875
rank:11, fwd_subop num: 27, fwd_subop: ['trace_src_func=_reduce,duration=0.28,timestamp=4409822694.9,input__shape=[2, 2048, 8192],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.32,timestamp=4409822701.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409822706.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.17,timestamp=4409822712.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.14,timestamp=4409822717.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.18,timestamp=4409822723.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.13,timestamp=4409822728.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.21,timestamp=4409822734.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4409822739.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.18,timestamp=4409822745.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409822750.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.35,timestamp=4409822756.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409822761.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.38,timestamp=4409822768.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409822773.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.36,timestamp=4409822779.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409822784.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.4,timestamp=4409822791.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409822795.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.38,timestamp=4409822802.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409822807.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409822813.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409822818.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.16,timestamp=4409822825.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.75,timestamp=4409822829.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.47,timestamp=4409822836.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.5,timestamp=4409822841.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 11, finish FWD profile ...
rank:11, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.82,timestamp=4409822849.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.89,timestamp=4409822863.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.22,timestamp=4409822872.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.72,timestamp=4409822885.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.23,timestamp=4409822894.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.73,timestamp=4409822908.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4409822917.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409822930.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4409822939.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.09,timestamp=4409822953.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409822962.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.11,timestamp=4409822976.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.6,timestamp=4409822986.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4409823000.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409823009.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409823023.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409823032.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409823046.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409823055.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4409823069.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4409823078.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.71,timestamp=4409823092.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.24,timestamp=4409823101.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.79,timestamp=4409823114.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4409823123.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409823137.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:11, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff200 recvbuff 0x7f88467ff200 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff200,7f88467ff200,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:11,optimizer_step time: 37.79276657104492
rank:11, finish optimizer.step profile ...
rank:11, Before memory release - Allocated: 33224575488, Reserved: 52594475008
rank:11, trace log has been written to txt...
rank:11, finish release GPU memory ...
rank:11, After memory release - Allocated: 11047318528, Reserved: 22118662144
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffc00 recvbuff 0x7f88467ffc00 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffc00,7f88467ffc00,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffe00 recvbuff 0x7f88467ffe00 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffe00,7f88467ffe00,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff000 recvbuff 0x7f88467ff000 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff000,7f88467ff000,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b60000000 recvbuff 0x7f8b60000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b60000000,7f8b60000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1377643520 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff800 recvbuff 0x7f88467ff800 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff800,7f88467ff800,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 12, finish warm up ...
rank_id = 12, input_tensor_shapes: []
rank:12,cuda fwd time: 162.334716796875
rank:12, fwd_subop num: 27, fwd_subop: ['trace_src_func=_reduce,duration=0.28,timestamp=4409824297.14,input__shape=[2, 2048, 8192],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.39,timestamp=4409824304.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409824308.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.15,timestamp=4409824314.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.14,timestamp=4409824319.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.25,timestamp=4409824325.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.19,timestamp=4409824330.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.25,timestamp=4409824336.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409824341.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409824348.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409824352.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.36,timestamp=4409824359.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409824364.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.34,timestamp=4409824370.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409824375.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409824390.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409824395.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.26,timestamp=4409824401.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409824406.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.14,timestamp=4409824412.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409824417.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4409824423.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409824428.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.2,timestamp=4409824435.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.75,timestamp=4409824440.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.44,timestamp=4409824446.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.56,timestamp=4409824451.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 12, finish FWD profile ...
rank:12, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.85,timestamp=4409824466.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.8,timestamp=4409824480.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.14,timestamp=4409824489.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4409824502.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.17,timestamp=4409824511.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.62,timestamp=4409824525.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.19,timestamp=4409824533.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.65,timestamp=4409824547.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409824556.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.89,timestamp=4409824570.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.51,timestamp=4409824579.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4409824593.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.52,timestamp=4409824602.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.0,timestamp=4409824616.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.5,timestamp=4409824626.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409824640.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409824649.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.9,timestamp=4409824663.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409824672.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.84,timestamp=4409824686.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.27,timestamp=4409824695.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.67,timestamp=4409824708.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.22,timestamp=4409824717.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.75,timestamp=4409824731.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409824740.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409824754.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:12, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff800 recvbuff 0x7f88467ff800 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff800,7f88467ff800,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:12,optimizer_step time: 37.8521614074707
rank:12, finish optimizer.step profile ...
rank:12, Before memory release - Allocated: 33224575488, Reserved: 52594475008
rank:12, trace log has been written to txt...
rank:12, finish release GPU memory ...
rank:12, After memory release - Allocated: 11047318528, Reserved: 22118662144
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8adfffe200 recvbuff 0x7f8adfffe200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8adfffe200,7f8adfffe200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8adfffe000 recvbuff 0x7f8adfffe000 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8adfffe000,7f8adfffe000,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8adfffe200 recvbuff 0x7f8adfffe200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8adfffe200,7f8adfffe200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f883a000000 recvbuff 0x7f883a000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f883a000000,7f883a000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1377643520 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffa00 recvbuff 0x7f8b753ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffa00,7f8b753ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 13, finish warm up ...
rank_id = 13, input_tensor_shapes: []
rank:13,cuda fwd time: 148.2608642578125
rank:13, fwd_subop num: 27, fwd_subop: ['trace_src_func=_reduce,duration=0.3,timestamp=4409825932.34,input__shape=[2, 2048, 8192],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.42,timestamp=4409825939.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4409825943.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.07,timestamp=4409825950.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4409825954.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.08,timestamp=4409825961.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409825965.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.1,timestamp=4409825971.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.75,timestamp=4409825976.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409825983.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409825987.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4409825994.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409825999.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4409826005.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409826010.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409826017.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409826022.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4409826028.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409826033.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409826040.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409826044.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409826051.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409826056.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409826062.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.64,timestamp=4409826067.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.18,timestamp=4409826074.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.42,timestamp=4409826079.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 13, finish FWD profile ...
rank:13, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.93,timestamp=4409826087.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.23,timestamp=4409826101.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409826110.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.53,timestamp=4409826124.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409826133.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.8,timestamp=4409826147.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409826156.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.96,timestamp=4409826169.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409826179.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.08,timestamp=4409826193.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4409826202.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.11,timestamp=4409826216.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409826225.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409826239.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409826248.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409826262.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409826271.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409826285.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409826294.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.84,timestamp=4409826308.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409826317.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409826330.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409826339.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.95,timestamp=4409826353.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4409826362.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.09,timestamp=4409826376.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:13, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffa00 recvbuff 0x7f8b753ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffa00,7f8b753ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:13,optimizer_step time: 37.96275329589844
rank:13, finish optimizer.step profile ...
rank:13, Before memory release - Allocated: 33224575488, Reserved: 52594475008
rank:13, trace log has been written to txt...
rank:13, finish release GPU memory ...
rank:13, After memory release - Allocated: 11047318528, Reserved: 22118662144
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b60000000 recvbuff 0x7f8b60000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b60000000,7f8b60000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1377643520 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 14, finish warm up ...
rank_id = 14, input_tensor_shapes: []
rank:14,cuda fwd time: 149.52857971191406
rank:14, fwd_subop num: 27, fwd_subop: ['trace_src_func=_reduce,duration=0.28,timestamp=4409827523.87,input__shape=[2, 2048, 8192],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.21,timestamp=4409827530.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409827535.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.99,timestamp=4409827541.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4409827546.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.03,timestamp=4409827552.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4409827557.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.07,timestamp=4409827563.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.74,timestamp=4409827568.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409827574.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409827579.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4409827586.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409827591.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409827597.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409827602.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409827608.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409827613.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409827620.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409827625.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409827631.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409827636.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409827643.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409827647.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409827654.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.58,timestamp=4409827659.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.07,timestamp=4409827665.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.39,timestamp=4409827670.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 14, finish FWD profile ...
rank:14, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.88,timestamp=4409827681.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.4,timestamp=4409827695.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.22,timestamp=4409827704.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.77,timestamp=4409827717.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4409827726.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409827740.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4409827749.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.73,timestamp=4409827763.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.22,timestamp=4409827772.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.67,timestamp=4409827785.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409827794.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409827808.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4409827817.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4409827831.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.53,timestamp=4409827840.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.05,timestamp=4409827854.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4409827863.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4409827878.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409827887.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409827900.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409827909.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409827923.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.23,timestamp=4409827932.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.66,timestamp=4409827946.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.24,timestamp=4409827954.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.66,timestamp=4409827968.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:14, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:14,optimizer_step time: 37.94124984741211
rank:14, finish optimizer.step profile ...
rank:14, Before memory release - Allocated: 33224575488, Reserved: 52594475008
rank:14, trace log has been written to txt...
rank:14, finish release GPU memory ...
rank:14, After memory release - Allocated: 11047318528, Reserved: 22118662144
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff600 recvbuff 0x7f8b753ff600 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff600,7f8b753ff600,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff800 recvbuff 0x7f8b753ff800 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff800,7f8b753ff800,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffa00 recvbuff 0x7f8b753ffa00 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffa00,7f8b753ffa00,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f883a000000 recvbuff 0x7f883a000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f883a000000,7f883a000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1377643520 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 15, finish warm up ...
rank_id = 15, input_tensor_shapes: []
rank:15,cuda fwd time: 147.83999633789062
rank:15, fwd_subop num: 27, fwd_subop: ['trace_src_func=_reduce,duration=0.29,timestamp=4409829112.83,input__shape=[2, 2048, 8192],input__dtype=torch.float32,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.42,timestamp=4409829119.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409829124.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.05,timestamp=4409829130.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409829135.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409829141.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.13,timestamp=4409829146.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.15,timestamp=4409829152.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.75,timestamp=4409829157.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409829163.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409829168.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.24,timestamp=4409829174.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409829179.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409829186.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409829190.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.26,timestamp=4409829197.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409829202.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.32,timestamp=4409829208.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409829213.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.17,timestamp=4409829220.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409829224.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409829231.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409829236.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.22,timestamp=4409829243.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4409829247.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.47,timestamp=4409829254.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.52,timestamp=4409829259.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 15, finish FWD profile ...
rank:15, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.85,timestamp=4409829267.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.91,timestamp=4409829281.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4409829290.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409829304.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.51,timestamp=4409829313.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4409829327.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.53,timestamp=4409829336.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409829350.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.5,timestamp=4409829360.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.9,timestamp=4409829374.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409829383.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.75,timestamp=4409829396.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409829405.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.67,timestamp=4409829419.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409829428.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.78,timestamp=4409829442.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.41,timestamp=4409829451.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4409829465.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.61,timestamp=4409829474.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4409829488.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409829497.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.19,timestamp=4409829512.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4409829521.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.09,timestamp=4409829535.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409829544.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409829558.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:15, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:15,optimizer_step time: 37.90028762817383
rank:15, finish optimizer.step profile ...
rank:15, Before memory release - Allocated: 33224575488, Reserved: 52594475008
rank:15, trace log has been written to txt...
rank:15, finish release GPU memory ...
rank:15, After memory release - Allocated: 11047318528, Reserved: 22118662144
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 16, finish warm up ...
rank_id = 16, input_tensor_shapes: [(2048, 2, 8192)]
rank:16,cuda fwd time: 147.01055908203125
rank:16, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.62,timestamp=4409830903.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409830908.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.04,timestamp=4409830914.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409830919.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.05,timestamp=4409830925.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4409830929.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.1,timestamp=4409830936.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409830940.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.37,timestamp=4409830946.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.35,timestamp=4409830951.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.22,timestamp=4409830959.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.65,timestamp=4409830964.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.15,timestamp=4409830971.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409830975.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.26,timestamp=4409830982.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409830987.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.24,timestamp=4409830993.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409830998.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.23,timestamp=4409831005.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409831009.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409831016.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409831021.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409831027.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409831032.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.26,timestamp=4409831038.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4409831043.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 16, finish FWD profile ...
rank:16, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.85,timestamp=4409831052.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.14,timestamp=4409831066.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.5,timestamp=4409831075.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409831089.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.5,timestamp=4409831098.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409831112.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.52,timestamp=4409831121.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409831135.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.52,timestamp=4409831145.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.95,timestamp=4409831159.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409831168.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.65,timestamp=4409831181.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.13,timestamp=4409831190.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4409831204.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4409831213.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.75,timestamp=4409831226.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.42,timestamp=4409831236.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.95,timestamp=4409831250.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.53,timestamp=4409831259.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409831273.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.53,timestamp=4409831282.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409831296.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.53,timestamp=4409831305.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4409831319.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.42,timestamp=4409831328.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409831342.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:16, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:16,optimizer_step time: 35.969024658203125
rank:16, finish optimizer.step profile ...
rank:16, Before memory release - Allocated: 32399653376, Reserved: 51420069888
rank:16, trace log has been written to txt...
rank:16, finish release GPU memory ...
rank:16, After memory release - Allocated: 21790594048, Reserved: 22246588416
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 17, finish warm up ...
rank_id = 17, input_tensor_shapes: [(2048, 2, 8192)]
rank:17,cuda fwd time: 150.6693115234375
rank:17, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.6,timestamp=4409832699.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409832704.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.03,timestamp=4409832710.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409832715.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.08,timestamp=4409832721.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409832725.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.11,timestamp=4409832731.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409832736.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409832743.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4409832747.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.12,timestamp=4409832754.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=1.99,timestamp=4409832763.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409832770.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409832774.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.16,timestamp=4409832781.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409832785.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.19,timestamp=4409832792.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409832796.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409832803.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409832808.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.22,timestamp=4409832814.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.58,timestamp=4409832819.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.17,timestamp=4409832827.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.32,timestamp=4409832832.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.24,timestamp=4409832838.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409832843.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 17, finish FWD profile ...
rank:17, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.77,timestamp=4409832851.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.09,timestamp=4409832865.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.52,timestamp=4409832874.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4409832888.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.53,timestamp=4409832897.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.05,timestamp=4409832911.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4409832920.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.05,timestamp=4409832934.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4409832943.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.05,timestamp=4409832957.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409832967.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4409832980.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409832989.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.75,timestamp=4409833003.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409833012.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409833026.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4409833035.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.0,timestamp=4409833049.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4409833058.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4409833072.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4409833081.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4409833095.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409833104.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4409833118.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4409833127.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409833141.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:17, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:17,optimizer_step time: 35.974143981933594
rank:17, finish optimizer.step profile ...
rank:17, Before memory release - Allocated: 43142928896, Reserved: 62258151424
rank:17, trace log has been written to txt...
rank:17, finish release GPU memory ...
rank:17, After memory release - Allocated: 21790594048, Reserved: 32723959808
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 18, finish warm up ...
rank_id = 18, input_tensor_shapes: [(2048, 2, 8192)]
rank:18,cuda fwd time: 147.3280029296875
rank:18, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.64,timestamp=4409834532.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409834537.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.11,timestamp=4409834543.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409834548.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.16,timestamp=4409834554.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409834558.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.11,timestamp=4409834564.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409834569.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.12,timestamp=4409834578.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4409834582.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.17,timestamp=4409834589.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409834593.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.19,timestamp=4409834600.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.55,timestamp=4409834604.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.23,timestamp=4409834611.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409834616.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409834622.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409834627.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409834633.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409834638.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.59,timestamp=4409834645.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409834650.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.24,timestamp=4409834656.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409834661.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409834668.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409834672.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 18, finish FWD profile ...
rank:18, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.75,timestamp=4409834681.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.82,timestamp=4409834695.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.21,timestamp=4409834703.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.65,timestamp=4409834717.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.21,timestamp=4409834726.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4409834740.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.2,timestamp=4409834748.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.63,timestamp=4409834762.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.27,timestamp=4409834771.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.73,timestamp=4409834785.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4409834794.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409834808.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.62,timestamp=4409834817.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4409834831.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.64,timestamp=4409834840.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409834854.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.52,timestamp=4409834864.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.91,timestamp=4409834877.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.42,timestamp=4409834886.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.8,timestamp=4409834900.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.27,timestamp=4409834909.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.71,timestamp=4409834923.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.21,timestamp=4409834932.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.65,timestamp=4409834945.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4409834954.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.88,timestamp=4409834968.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:18, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:18,optimizer_step time: 36.157440185546875
rank:18, finish optimizer.step profile ...
rank:18, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:18, trace log has been written to txt...
rank:18, finish release GPU memory ...
rank:18, After memory release - Allocated: 21790594048, Reserved: 32723959808
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 19, finish warm up ...
rank_id = 19, input_tensor_shapes: [(2048, 2, 8192)]
rank:19,cuda fwd time: 145.2236785888672
rank:19, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.61,timestamp=4409840485.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409840490.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.11,timestamp=4409840496.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409840500.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409840506.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409840511.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.15,timestamp=4409840517.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409840522.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409840528.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409840533.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409840539.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409840544.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.33,timestamp=4409840551.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409840555.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.33,timestamp=4409840562.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409840567.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.3,timestamp=4409840573.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409840578.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.33,timestamp=4409840584.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409840589.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409840596.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409840600.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.3,timestamp=4409840607.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409840612.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.31,timestamp=4409840618.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409840623.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 19, finish FWD profile ...
rank:19, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.84,timestamp=4409840631.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.12,timestamp=4409840645.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409840654.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.8,timestamp=4409840668.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409840676.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.76,timestamp=4409840690.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409840699.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4409840713.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.43,timestamp=4409840722.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.08,timestamp=4409840736.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409840745.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.18,timestamp=4409840759.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409840768.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409840782.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409840791.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4409840805.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409840814.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.18,timestamp=4409840828.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409840838.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4409840852.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4409840861.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.78,timestamp=4409840874.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409840883.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4409840897.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409840906.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.09,timestamp=4409840920.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:19, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:19,optimizer_step time: 35.97926330566406
rank:19, finish optimizer.step profile ...
rank:19, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:19, trace log has been written to txt...
rank:19, finish release GPU memory ...
rank:19, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 1): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 20, finish warm up ...
rank_id = 20, input_tensor_shapes: [(2048, 2, 8192)]
rank:20,cuda fwd time: 144.74240112304688
rank:20, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.63,timestamp=4409846659.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409846664.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.09,timestamp=4409846670.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4409846674.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.1,timestamp=4409846680.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409846685.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.15,timestamp=4409846691.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409846696.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409846702.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409846707.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.31,timestamp=4409846713.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409846718.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.3,timestamp=4409846725.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409846729.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.36,timestamp=4409846736.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409846741.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409846747.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409846752.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.23,timestamp=4409846758.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.77,timestamp=4409846763.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409846769.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409846774.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.25,timestamp=4409846781.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409846785.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.3,timestamp=4409846792.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409846797.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 20, finish FWD profile ...
rank:20, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=4.78,timestamp=4409846805.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.72,timestamp=4409846819.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.22,timestamp=4409846828.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.65,timestamp=4409846842.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.23,timestamp=4409846851.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.68,timestamp=4409846865.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.28,timestamp=4409846874.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4409846888.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409846897.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409846910.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409846920.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.11,timestamp=4409846934.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409846943.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4409846957.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409846966.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4409846980.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4409846989.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4409847003.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.41,timestamp=4409847012.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.89,timestamp=4409847025.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409847034.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409847048.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409847057.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4409847071.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4409847080.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4409847094.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:20, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:20,optimizer_step time: 35.85740661621094
rank:20, finish optimizer.step profile ...
rank:20, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:20, trace log has been written to txt...
rank:20, finish release GPU memory ...
rank:20, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 1): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 21, finish warm up ...
rank_id = 21, input_tensor_shapes: [(2048, 2, 8192)]
rank:21,cuda fwd time: 149.16403198242188
rank:21, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.61,timestamp=4409852681.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409852686.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.08,timestamp=4409852692.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409852697.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409852703.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409852707.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.15,timestamp=4409852714.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409852718.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409852724.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=1.99,timestamp=4409852734.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.16,timestamp=4409852740.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409852745.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.22,timestamp=4409852751.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409852756.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.23,timestamp=4409852763.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409852767.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.26,timestamp=4409852774.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409852778.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409852785.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409852789.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.25,timestamp=4409852796.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409852801.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.3,timestamp=4409852807.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409852812.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.3,timestamp=4409852819.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409852823.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 21, finish FWD profile ...
rank:21, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.76,timestamp=4409852832.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.76,timestamp=4409852846.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.27,timestamp=4409852855.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.68,timestamp=4409852868.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.23,timestamp=4409852877.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.68,timestamp=4409852891.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.24,timestamp=4409852900.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.77,timestamp=4409852914.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409852923.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.76,timestamp=4409852936.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.23,timestamp=4409852945.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.65,timestamp=4409852959.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409852968.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.87,timestamp=4409852982.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.42,timestamp=4409852991.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.0,timestamp=4409853005.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.5,timestamp=4409853014.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4409853028.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.51,timestamp=4409853037.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.91,timestamp=4409853051.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409853060.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409853074.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409853083.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.72,timestamp=4409853097.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.17,timestamp=4409853105.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.62,timestamp=4409853119.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:21, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:21,optimizer_step time: 35.973121643066406
rank:21, finish optimizer.step profile ...
rank:21, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:21, trace log has been written to txt...
rank:21, finish release GPU memory ...
rank:21, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 1): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 22, finish warm up ...
rank_id = 22, input_tensor_shapes: [(2048, 2, 8192)]
rank:22,cuda fwd time: 145.00965881347656
rank:22, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.47,timestamp=4409857519.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409857524.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.1,timestamp=4409857530.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409857534.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409857541.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409857545.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.12,timestamp=4409857551.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409857556.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.12,timestamp=4409857562.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409857567.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4409857573.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409857578.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.24,timestamp=4409857585.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409857589.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409857596.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409857601.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.35,timestamp=4409857607.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409857612.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409857618.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409857623.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.3,timestamp=4409857630.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409857634.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409857641.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409857646.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.23,timestamp=4409857652.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409857657.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 22, finish FWD profile ...
rank:22, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.68,timestamp=4409857665.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.75,timestamp=4409857679.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.22,timestamp=4409857688.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.88,timestamp=4409857702.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409857711.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409857725.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.5,timestamp=4409857734.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409857748.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.52,timestamp=4409857757.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409857771.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.5,timestamp=4409857780.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.99,timestamp=4409857794.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4409857803.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.76,timestamp=4409857817.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.22,timestamp=4409857826.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.65,timestamp=4409857840.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.21,timestamp=4409857849.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409857862.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4409857871.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.95,timestamp=4409857885.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409857895.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409857909.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.51,timestamp=4409857918.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409857932.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.5,timestamp=4409857941.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409857955.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:22, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:22,optimizer_step time: 35.88915252685547
rank:22, finish optimizer.step profile ...
rank:22, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:22, trace log has been written to txt...
rank:22, finish release GPU memory ...
rank:22, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 1): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 23, finish warm up ...
rank_id = 23, input_tensor_shapes: [(2048, 2, 8192)]
rank:23,cuda fwd time: 153.24671936035156
rank:23, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.5,timestamp=4409862555.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409862560.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.97,timestamp=4409862566.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409862571.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.93,timestamp=4409862577.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409862582.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.99,timestamp=4409862588.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409862593.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4409862599.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.72,timestamp=4409862603.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.79,timestamp=4409862611.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=1.99,timestamp=4409862620.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.07,timestamp=4409862628.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.01,timestamp=4409862633.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409862639.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.77,timestamp=4409862644.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409862650.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.41,timestamp=4409862655.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.05,timestamp=4409862663.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4409862668.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409862675.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.77,timestamp=4409862679.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409862686.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4409862690.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409862697.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.78,timestamp=4409862702.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 23, finish FWD profile ...
rank:23, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.72,timestamp=4409862710.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.85,timestamp=4409862724.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.18,timestamp=4409862733.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409862746.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4409862755.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.91,timestamp=4409862769.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409862778.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4409862792.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409862801.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.08,timestamp=4409862815.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.42,timestamp=4409862824.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.07,timestamp=4409862838.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409862847.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.08,timestamp=4409862861.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409862870.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.95,timestamp=4409862884.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4409862893.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.88,timestamp=4409862906.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409862915.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4409862929.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409862938.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4409862952.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4409862961.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.11,timestamp=4409862975.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.64,timestamp=4409862984.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.24,timestamp=4409862998.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:23, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:23,optimizer_step time: 36.03763198852539
rank:23, finish optimizer.step profile ...
rank:23, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:23, trace log has been written to txt...
rank:23, finish release GPU memory ...
rank:23, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 24, finish warm up ...
rank_id = 24, input_tensor_shapes: [(2048, 2, 8192)]
rank:24,cuda fwd time: 145.52268981933594
rank:24, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.63,timestamp=4409867960.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409867964.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.12,timestamp=4409867970.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409867975.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.15,timestamp=4409867981.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409867986.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.17,timestamp=4409867992.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.16,timestamp=4409867997.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.24,timestamp=4409868003.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409868008.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.31,timestamp=4409868014.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.87,timestamp=4409868019.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409868025.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409868030.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.25,timestamp=4409868037.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409868042.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.31,timestamp=4409868048.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409868053.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4409868060.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409868064.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.32,timestamp=4409868071.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409868076.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.31,timestamp=4409868082.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409868087.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409868093.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409868098.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 24, finish FWD profile ...
rank:24, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.87,timestamp=4409868106.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.22,timestamp=4409868120.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.41,timestamp=4409868129.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.05,timestamp=4409868143.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409868152.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4409868166.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4409868175.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.87,timestamp=4409868189.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409868197.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409868211.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409868220.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409868234.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4409868243.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4409868256.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409868265.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4409868279.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409868288.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.05,timestamp=4409868302.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409868311.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.07,timestamp=4409868325.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.5,timestamp=4409868334.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4409868348.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4409868357.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.77,timestamp=4409868371.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4409868379.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.91,timestamp=4409868393.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:24, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:24,optimizer_step time: 35.85126495361328
rank:24, finish optimizer.step profile ...
rank:24, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:24, trace log has been written to txt...
rank:24, finish release GPU memory ...
rank:24, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 25, finish warm up ...
rank_id = 25, input_tensor_shapes: [(2048, 2, 8192)]
rank:25,cuda fwd time: 153.7822723388672
rank:25, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.5,timestamp=4409872605.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409872610.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.94,timestamp=4409872616.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409872621.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.02,timestamp=4409872627.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409872631.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.0,timestamp=4409872638.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409872642.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409872648.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.7,timestamp=4409872653.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.53,timestamp=4409872659.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.7,timestamp=4409872664.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409872671.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.74,timestamp=4409872675.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409872682.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.73,timestamp=4409872686.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.62,timestamp=4409872701.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.74,timestamp=4409872705.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409872712.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.77,timestamp=4409872717.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409872723.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4409872728.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409872734.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4409872739.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409872746.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4409872750.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 25, finish FWD profile ...
rank:25, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.79,timestamp=4409872760.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.75,timestamp=4409872774.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409872783.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4409872797.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409872806.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.1,timestamp=4409872820.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409872829.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.18,timestamp=4409872843.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409872852.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.18,timestamp=4409872866.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409872875.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4409872889.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409872898.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.09,timestamp=4409872912.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409872921.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4409872935.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4409872944.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.87,timestamp=4409872957.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409872966.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.0,timestamp=4409872980.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409872989.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.19,timestamp=4409873003.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.6,timestamp=4409873013.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.18,timestamp=4409873027.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409873036.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.08,timestamp=4409873050.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:25, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:25,optimizer_step time: 36.04787063598633
rank:25, finish optimizer.step profile ...
rank:25, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:25, trace log has been written to txt...
rank:25, finish release GPU memory ...
rank:25, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 26, finish warm up ...
rank_id = 26, input_tensor_shapes: [(2048, 2, 8192)]
rank:26,cuda fwd time: 145.7039337158203
rank:26, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.46,timestamp=4409878423.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409878428.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.94,timestamp=4409878434.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409878439.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.98,timestamp=4409878445.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409878450.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.97,timestamp=4409878456.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409878460.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4409878467.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.71,timestamp=4409878471.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409878478.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4409878482.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409878489.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.78,timestamp=4409878494.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409878500.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409878505.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409878511.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409878516.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409878523.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409878528.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409878534.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409878539.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409878545.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409878550.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409878557.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409878562.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 26, finish FWD profile ...
rank:26, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.84,timestamp=4409878570.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.15,timestamp=4409878584.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409878593.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.99,timestamp=4409878607.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409878616.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4409878630.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409878639.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.08,timestamp=4409878653.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.5,timestamp=4409878662.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.95,timestamp=4409878676.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4409878685.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.84,timestamp=4409878698.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409878707.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4409878721.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409878730.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409878744.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409878753.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4409878766.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409878775.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409878789.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409878798.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4409878812.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409878821.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.18,timestamp=4409878835.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409878844.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.19,timestamp=4409878858.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:26, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:26,optimizer_step time: 36.0447998046875
rank:26, finish optimizer.step profile ...
rank:26, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:26, trace log has been written to txt...
rank:26, finish release GPU memory ...
rank:26, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 27, finish warm up ...
rank_id = 27, input_tensor_shapes: [(2048, 2, 8192)]
rank:27,cuda fwd time: 150.6447296142578
rank:27, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.65,timestamp=4409883137.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409883141.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.11,timestamp=4409883147.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409883152.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.16,timestamp=4409883158.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409883163.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409883169.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409883173.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.48,timestamp=4409883180.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4409883184.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.18,timestamp=4409883191.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=1.99,timestamp=4409883202.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.17,timestamp=4409883208.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409883213.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409883220.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409883224.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409883231.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409883235.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.23,timestamp=4409883242.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409883247.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.25,timestamp=4409883253.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409883258.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409883264.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409883269.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4409883275.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409883280.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 27, finish FWD profile ...
rank:27, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.81,timestamp=4409883288.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.0,timestamp=4409883302.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409883311.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.72,timestamp=4409883324.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4409883333.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.76,timestamp=4409883347.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.19,timestamp=4409883356.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.75,timestamp=4409883369.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.24,timestamp=4409883378.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.76,timestamp=4409883392.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409883400.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.8,timestamp=4409883414.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409883423.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409883437.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.69,timestamp=4409883446.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.27,timestamp=4409883461.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.69,timestamp=4409883470.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.08,timestamp=4409883484.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4409883493.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.96,timestamp=4409883507.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4409883515.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4409883529.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409883538.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.76,timestamp=4409883552.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.24,timestamp=4409883560.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409883574.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:27, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:27,optimizer_step time: 35.85740661621094
rank:27, finish optimizer.step profile ...
rank:27, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:27, trace log has been written to txt...
rank:27, finish release GPU memory ...
rank:27, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 28, finish warm up ...
rank_id = 28, input_tensor_shapes: [(2048, 2, 8192)]
rank:28,cuda fwd time: 146.10841369628906
rank:28, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.47,timestamp=4409887635.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409887640.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.96,timestamp=4409887646.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409887650.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.98,timestamp=4409887656.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409887661.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.01,timestamp=4409887667.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409887672.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409887678.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.74,timestamp=4409887683.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409887689.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409887694.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4409887701.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409887705.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4409887712.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409887717.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409887723.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409887728.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409887735.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409887739.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.74,timestamp=4409887746.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409887751.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4409887758.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409887762.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409887769.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409887774.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 28, finish FWD profile ...
rank:28, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.9,timestamp=4409887782.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.21,timestamp=4409887796.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.44,timestamp=4409887805.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.0,timestamp=4409887819.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.44,timestamp=4409887828.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.08,timestamp=4409887842.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.52,timestamp=4409887851.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.08,timestamp=4409887865.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409887874.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.96,timestamp=4409887888.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409887897.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4409887910.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.28,timestamp=4409887919.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.75,timestamp=4409887933.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4409887942.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.79,timestamp=4409887955.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409887964.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4409887978.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.5,timestamp=4409887987.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.11,timestamp=4409888001.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.6,timestamp=4409888010.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.18,timestamp=4409888024.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409888033.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.07,timestamp=4409888047.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409888056.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.96,timestamp=4409888070.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:28, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:28,optimizer_step time: 36.0263671875
rank:28, finish optimizer.step profile ...
rank:28, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:28, trace log has been written to txt...
rank:28, finish release GPU memory ...
rank:28, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 29, finish warm up ...
rank_id = 29, input_tensor_shapes: [(2048, 2, 8192)]
rank:29,cuda fwd time: 145.31173706054688
rank:29, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.47,timestamp=4409891443.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409891448.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.95,timestamp=4409891454.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409891459.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.99,timestamp=4409891465.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409891470.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.98,timestamp=4409891476.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409891480.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409891487.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4409891491.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409891498.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.77,timestamp=4409891503.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409891509.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.77,timestamp=4409891514.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409891520.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4409891525.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409891531.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4409891536.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409891543.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409891547.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409891554.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409891559.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4409891565.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409891570.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409891577.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409891581.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 29, finish FWD profile ...
rank:29, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.87,timestamp=4409891590.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.14,timestamp=4409891604.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409891613.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409891627.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4409891636.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4409891649.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409891658.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.05,timestamp=4409891672.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4409891681.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4409891695.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409891705.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4409891719.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4409891728.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.96,timestamp=4409891742.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409891750.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409891764.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.27,timestamp=4409891773.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.8,timestamp=4409891787.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.28,timestamp=4409891796.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409891809.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409891818.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4409891832.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4409891841.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409891855.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.64,timestamp=4409891864.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.24,timestamp=4409891878.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:29, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:29,optimizer_step time: 36.06835174560547
rank:29, finish optimizer.step profile ...
rank:29, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:29, trace log has been written to txt...
rank:29, finish release GPU memory ...
rank:29, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 30, finish warm up ...
rank_id = 30, input_tensor_shapes: [(2048, 2, 8192)]
rank:30,cuda fwd time: 145.49913024902344
rank:30, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.54,timestamp=4409894576.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409894580.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.88,timestamp=4409894587.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4409894591.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.1,timestamp=4409894597.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4409894602.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.09,timestamp=4409894608.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409894613.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409894619.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409894624.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4409894630.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409894635.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409894641.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409894646.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.24,timestamp=4409894653.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409894657.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.31,timestamp=4409894664.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409894669.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409894675.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409894680.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.26,timestamp=4409894687.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409894691.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.25,timestamp=4409894698.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409894703.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.17,timestamp=4409894709.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409894714.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 30, finish FWD profile ...
rank:30, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.76,timestamp=4409894723.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.81,timestamp=4409894736.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.04,timestamp=4409894745.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.52,timestamp=4409894759.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.16,timestamp=4409894768.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4409894782.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409894791.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.73,timestamp=4409894805.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4409894814.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.88,timestamp=4409894828.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4409894837.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4409894851.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4409894860.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4409894874.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409894884.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.77,timestamp=4409894898.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4409894907.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.53,timestamp=4409894921.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4409894930.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.4,timestamp=4409894944.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.19,timestamp=4409894953.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.77,timestamp=4409894966.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409894976.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4409894990.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409894999.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.79,timestamp=4409895013.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:30, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:30,optimizer_step time: 36.21683120727539
rank:30, finish optimizer.step profile ...
rank:30, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:30, trace log has been written to txt...
rank:30, finish release GPU memory ...
rank:30, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 31, finish warm up ...
rank_id = 31, input_tensor_shapes: [(2048, 2, 8192)]
rank:31,cuda fwd time: 144.86834716796875
rank:31, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.57,timestamp=4409896316.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409896321.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.11,timestamp=4409896327.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409896331.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409896337.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409896342.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.1,timestamp=4409896348.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409896353.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.22,timestamp=4409896359.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409896364.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.12,timestamp=4409896370.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409896375.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409896381.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409896386.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409896393.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409896397.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409896404.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409896408.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.25,timestamp=4409896415.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409896420.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.24,timestamp=4409896426.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409896431.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.25,timestamp=4409896438.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409896442.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.3,timestamp=4409896449.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409896454.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 31, finish FWD profile ...
rank:31, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.78,timestamp=4409896462.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.92,timestamp=4409896476.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.14,timestamp=4409896485.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.68,timestamp=4409896499.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.22,timestamp=4409896507.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.78,timestamp=4409896521.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.42,timestamp=4409896530.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409896544.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409896554.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.96,timestamp=4409896568.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4409896577.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409896591.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4409896600.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.6,timestamp=4409896614.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.16,timestamp=4409896622.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.46,timestamp=4409896636.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.12,timestamp=4409896645.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.46,timestamp=4409896659.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4409896667.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.44,timestamp=4409896681.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.11,timestamp=4409896690.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.45,timestamp=4409896704.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.1,timestamp=4409896712.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.46,timestamp=4409896726.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409896735.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.89,timestamp=4409896749.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:31, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:31,optimizer_step time: 36.06630325317383
rank:31, finish optimizer.step profile ...
rank:31, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:31, trace log has been written to txt...
rank:31, finish release GPU memory ...
rank:31, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 32, finish warm up ...
rank_id = 32, input_tensor_shapes: [(2048, 2, 8192)]
rank:32,cuda fwd time: 145.36703491210938
rank:32, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.49,timestamp=4409899973.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409899978.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.03,timestamp=4409899984.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4409899988.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.1,timestamp=4409899995.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4409899999.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.1,timestamp=4409900005.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4409900010.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409900016.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4409900021.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.17,timestamp=4409900027.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409900032.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409900038.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409900043.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.26,timestamp=4409900050.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409900054.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.32,timestamp=4409900061.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409900066.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409900072.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409900077.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4409900084.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409900088.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.2,timestamp=4409900095.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.78,timestamp=4409900100.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.25,timestamp=4409900106.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409900111.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 32, finish FWD profile ...
rank:32, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.82,timestamp=4409900120.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.07,timestamp=4409900133.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409900142.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.87,timestamp=4409900156.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.28,timestamp=4409900165.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.95,timestamp=4409900179.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409900188.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409900202.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409900211.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.07,timestamp=4409900225.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409900234.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409900248.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409900257.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.84,timestamp=4409900271.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.27,timestamp=4409900280.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.73,timestamp=4409900293.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4409900302.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.71,timestamp=4409900316.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.23,timestamp=4409900324.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.78,timestamp=4409900338.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4409900347.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409900361.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409900370.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4409900384.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409900393.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.18,timestamp=4409900408.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:32, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:32,optimizer_step time: 36.06220626831055
rank:32, finish optimizer.step profile ...
rank:32, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:32, trace log has been written to txt...
rank:32, finish release GPU memory ...
rank:32, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 33, finish warm up ...
rank_id = 33, input_tensor_shapes: [(2048, 2, 8192)]
rank:33,cuda fwd time: 144.98406982421875
rank:33, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.63,timestamp=4409903157.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409903162.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.1,timestamp=4409903168.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4409903172.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.13,timestamp=4409903178.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409903183.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409903189.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409903194.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409903200.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409903205.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409903211.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409903216.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.35,timestamp=4409903222.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409903227.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.31,timestamp=4409903234.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409903238.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.3,timestamp=4409903245.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409903250.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409903256.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409903261.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.32,timestamp=4409903268.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409903272.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.32,timestamp=4409903279.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409903284.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.31,timestamp=4409903290.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409903295.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 33, finish FWD profile ...
rank:33, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.85,timestamp=4409903303.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.13,timestamp=4409903317.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409903326.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.9,timestamp=4409903340.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409903349.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409903363.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409903372.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4409903386.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.23,timestamp=4409903395.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4409903408.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409903417.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409903431.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409903440.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409903455.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4409903464.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.07,timestamp=4409903478.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409903487.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4409903501.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4409903509.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.85,timestamp=4409903523.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.28,timestamp=4409903532.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.75,timestamp=4409903546.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4409903554.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.52,timestamp=4409903568.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409903577.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409903591.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:33, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:33,optimizer_step time: 36.03456115722656
rank:33, finish optimizer.step profile ...
rank:33, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:33, trace log has been written to txt...
rank:33, finish release GPU memory ...
rank:33, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 2): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 34, finish warm up ...
rank_id = 34, input_tensor_shapes: [(2048, 2, 8192)]
rank:34,cuda fwd time: 145.6609344482422
rank:34, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.6,timestamp=4409906318.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409906323.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.09,timestamp=4409906329.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409906334.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.13,timestamp=4409906340.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409906344.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409906351.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.14,timestamp=4409906355.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.23,timestamp=4409906362.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409906366.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409906373.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409906378.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.34,timestamp=4409906384.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409906389.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.34,timestamp=4409906395.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409906400.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.4,timestamp=4409906407.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409906412.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.38,timestamp=4409906418.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409906423.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.36,timestamp=4409906430.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409906434.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.33,timestamp=4409906441.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409906446.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409906452.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409906457.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 34, finish FWD profile ...
rank:34, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.83,timestamp=4409906465.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.15,timestamp=4409906479.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4409906488.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4409906502.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409906511.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.09,timestamp=4409906525.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.53,timestamp=4409906534.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.0,timestamp=4409906548.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4409906557.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.0,timestamp=4409906571.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.43,timestamp=4409906580.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4409906594.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4409906603.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409906617.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409906626.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.8,timestamp=4409906639.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4409906648.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409906662.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409906671.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4409906685.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409906695.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409906709.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409906718.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4409906732.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409906741.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409906755.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:34, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:34,optimizer_step time: 37.195777893066406
rank:34, finish optimizer.step profile ...
rank:34, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:34, trace log has been written to txt...
rank:34, finish release GPU memory ...
rank:34, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 2): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 35, finish warm up ...
rank_id = 35, input_tensor_shapes: [(2048, 2, 8192)]
rank:35,cuda fwd time: 145.8350067138672
rank:35, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.48,timestamp=4409909535.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409909540.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.95,timestamp=4409909546.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409909551.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.99,timestamp=4409909557.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409909562.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.01,timestamp=4409909568.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4409909573.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409909579.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.77,timestamp=4409909583.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409909590.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409909595.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409909601.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409909606.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409909613.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4409909617.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409909624.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4409909629.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409909635.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4409909640.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409909646.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409909651.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409909658.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4409909663.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409909669.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409909674.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 35, finish FWD profile ...
rank:35, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.91,timestamp=4409909683.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.27,timestamp=4409909697.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409909706.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4409909720.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4409909729.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4409909743.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4409909752.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.85,timestamp=4409909765.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409909774.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409909788.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.24,timestamp=4409909796.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.76,timestamp=4409909810.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.24,timestamp=4409909819.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.75,timestamp=4409909833.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.27,timestamp=4409909841.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.85,timestamp=4409909855.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4409909864.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409909878.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409909887.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4409909901.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409909910.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.05,timestamp=4409909924.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409909933.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.96,timestamp=4409909947.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409909956.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.85,timestamp=4409909970.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:35, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:35,optimizer_step time: 36.127742767333984
rank:35, finish optimizer.step profile ...
rank:35, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:35, trace log has been written to txt...
rank:35, finish release GPU memory ...
rank:35, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 2): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 36, finish warm up ...
rank_id = 36, input_tensor_shapes: [(2048, 2, 8192)]
rank:36,cuda fwd time: 145.9804229736328
rank:36, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.49,timestamp=4409912554.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409912559.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.94,timestamp=4409912565.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409912570.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.95,timestamp=4409912576.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409912580.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.99,timestamp=4409912587.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409912591.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409912597.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4409912602.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409912609.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409912614.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409912620.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409912625.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4409912632.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409912636.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.63,timestamp=4409912643.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.78,timestamp=4409912648.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409912654.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4409912659.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409912665.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4409912670.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409912677.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409912681.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409912688.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409912693.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 36, finish FWD profile ...
rank:36, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.9,timestamp=4409912701.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.17,timestamp=4409912715.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4409912724.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.85,timestamp=4409912738.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409912747.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.89,timestamp=4409912761.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.44,timestamp=4409912770.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.05,timestamp=4409912784.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409912793.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4409912807.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4409912816.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4409912830.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409912839.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409912853.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4409912863.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4409912877.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409912885.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.85,timestamp=4409912899.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409912908.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.75,timestamp=4409912922.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409912931.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409912944.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.5,timestamp=4409912953.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.1,timestamp=4409912967.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409912976.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4409912990.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:36, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:36,optimizer_step time: 36.08063888549805
rank:36, finish optimizer.step profile ...
rank:36, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:36, trace log has been written to txt...
rank:36, finish release GPU memory ...
rank:36, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 2): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 37, finish warm up ...
rank_id = 37, input_tensor_shapes: [(2048, 2, 8192)]
rank:37,cuda fwd time: 147.49388122558594
rank:37, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.21,timestamp=4409915762.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409915767.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.94,timestamp=4409915773.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409915778.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.98,timestamp=4409915784.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409915788.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.97,timestamp=4409915795.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409915799.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4409915805.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4409915810.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409915817.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4409915822.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.12,timestamp=4409915829.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409915834.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409915841.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409915845.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409915852.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409915857.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409915863.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409915868.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409915875.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4409915879.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409915886.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4409915891.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409915897.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409915902.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 37, finish FWD profile ...
rank:37, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.82,timestamp=4409915911.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.06,timestamp=4409915925.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409915934.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409915947.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4409915956.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409915970.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409915979.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.88,timestamp=4409915992.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4409916001.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4409916015.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409916024.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409916038.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409916048.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409916062.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409916071.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4409916085.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409916094.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4409916108.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.44,timestamp=4409916117.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4409916131.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4409916140.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.71,timestamp=4409916153.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.24,timestamp=4409916162.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409916176.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4409916185.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4409916199.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:37, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:37,optimizer_step time: 36.24755096435547
rank:37, finish optimizer.step profile ...
rank:37, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:37, trace log has been written to txt...
rank:37, finish release GPU memory ...
rank:37, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 2): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 38, finish warm up ...
rank_id = 38, input_tensor_shapes: [(2048, 2, 8192)]
rank:38,cuda fwd time: 154.38336181640625
rank:38, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.51,timestamp=4409918853.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409918858.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.95,timestamp=4409918864.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409918869.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.0,timestamp=4409918875.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409918880.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.0,timestamp=4409918886.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409918890.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409918897.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.71,timestamp=4409918901.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409918908.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.74,timestamp=4409918912.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4409918928.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.77,timestamp=4409918933.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409918940.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4409918944.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409918951.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.77,timestamp=4409918955.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409918962.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4409918967.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409918973.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4409918978.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409918984.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4409918989.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409918995.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4409919000.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 38, finish FWD profile ...
rank:38, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.75,timestamp=4409919009.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.97,timestamp=4409919023.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4409919032.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4409919046.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409919055.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4409919069.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4409919078.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409919092.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409919101.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409919115.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409919124.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409919138.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4409919147.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.85,timestamp=4409919161.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.27,timestamp=4409919170.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.78,timestamp=4409919183.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409919192.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.89,timestamp=4409919206.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409919215.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409919229.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409919238.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409919252.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.6,timestamp=4409919261.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4409919275.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409919285.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4409919298.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:38, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:38,optimizer_step time: 35.99871826171875
rank:38, finish optimizer.step profile ...
rank:38, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:38, trace log has been written to txt...
rank:38, finish release GPU memory ...
rank:38, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 2): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 39, finish warm up ...
rank_id = 39, input_tensor_shapes: [(2048, 2, 8192)]
rank:39,cuda fwd time: 146.70745849609375
rank:39, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.49,timestamp=4409922218.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409922222.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.95,timestamp=4409922228.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409922233.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.99,timestamp=4409922239.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409922244.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.0,timestamp=4409922250.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409922255.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=1.9,timestamp=4409922261.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4409922266.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4409922273.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.74,timestamp=4409922278.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409922284.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409922289.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409922295.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409922300.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409922307.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409922312.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.59,timestamp=4409922318.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4409922323.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409922329.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4409922334.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409922341.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4409922345.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409922352.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4409922357.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 39, finish FWD profile ...
rank:39, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.75,timestamp=4409922366.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.07,timestamp=4409922380.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.51,timestamp=4409922389.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409922403.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409922412.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4409922426.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409922435.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.96,timestamp=4409922449.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4409922458.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.85,timestamp=4409922471.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4409922480.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.72,timestamp=4409922494.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.24,timestamp=4409922502.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.78,timestamp=4409922516.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4409922525.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409922539.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409922548.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409922562.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409922571.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409922585.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409922594.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.99,timestamp=4409922608.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4409922617.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.89,timestamp=4409922631.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409922640.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409922654.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:39, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:39,optimizer_step time: 36.05196762084961
rank:39, finish optimizer.step profile ...
rank:39, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:39, trace log has been written to txt...
rank:39, finish release GPU memory ...
rank:39, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 40, finish warm up ...
rank_id = 40, input_tensor_shapes: [(2048, 2, 8192)]
rank:40,cuda fwd time: 146.14016723632812
rank:40, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.53,timestamp=4409925679.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409925684.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.98,timestamp=4409925690.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409925694.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.99,timestamp=4409925700.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409925705.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.01,timestamp=4409925711.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409925716.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409925722.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409925727.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409925734.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409925738.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4409925745.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409925750.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409925756.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409925761.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4409925768.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409925772.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4409925779.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409925784.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409925790.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409925795.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409925802.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409925806.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409925813.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409925818.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 40, finish FWD profile ...
rank:40, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.81,timestamp=4409925826.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.0,timestamp=4409925840.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409925849.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.8,timestamp=4409925863.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409925872.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.84,timestamp=4409925885.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409925894.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.99,timestamp=4409925908.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409925917.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.18,timestamp=4409925931.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409925941.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409925955.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409925964.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.07,timestamp=4409925978.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409925987.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.96,timestamp=4409926000.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4409926009.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.75,timestamp=4409926023.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409926032.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.77,timestamp=4409926045.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4409926054.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409926068.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409926077.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.1,timestamp=4409926091.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409926100.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4409926114.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:40, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:40,optimizer_step time: 36.139007568359375
rank:40, finish optimizer.step profile ...
rank:40, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:40, trace log has been written to txt...
rank:40, finish release GPU memory ...
rank:40, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 41, finish warm up ...
rank_id = 41, input_tensor_shapes: [(2048, 2, 8192)]
rank:41,cuda fwd time: 145.4264373779297
rank:41, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.48,timestamp=4409928020.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409928024.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.97,timestamp=4409928031.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409928035.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.98,timestamp=4409928041.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409928046.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.01,timestamp=4409928052.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409928057.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409928063.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.74,timestamp=4409928068.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.74,timestamp=4409928074.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.55,timestamp=4409928079.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409928085.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4409928090.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409928096.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4409928101.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409928108.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409928112.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4409928119.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409928124.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4409928130.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409928135.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4409928142.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409928147.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409928153.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409928158.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 41, finish FWD profile ...
rank:41, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.82,timestamp=4409928166.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.07,timestamp=4409928180.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409928189.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.96,timestamp=4409928203.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409928212.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4409928226.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409928235.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.91,timestamp=4409928249.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4409928258.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4409928271.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409928280.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.76,timestamp=4409928294.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409928303.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4409928317.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.5,timestamp=4409928326.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409928340.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4409928349.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4409928363.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409928372.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4409928386.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4409928395.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409928409.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4409928418.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4409928432.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409928441.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.72,timestamp=4409928454.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:41, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:41,optimizer_step time: 36.303871154785156
rank:41, finish optimizer.step profile ...
rank:41, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:41, trace log has been written to txt...
rank:41, finish release GPU memory ...
rank:41, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 42, finish warm up ...
rank_id = 42, input_tensor_shapes: [(2048, 2, 8192)]
rank:42,cuda fwd time: 145.83602905273438
rank:42, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.5,timestamp=4409929847.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409929851.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.95,timestamp=4409929857.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409929862.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.96,timestamp=4409929868.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409929873.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.02,timestamp=4409929879.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409929884.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409929890.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.77,timestamp=4409929895.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409929901.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409929906.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409929913.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409929917.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409929924.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4409929929.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409929935.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4409929940.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.6,timestamp=4409929946.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4409929951.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409929958.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4409929962.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.74,timestamp=4409929969.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4409929974.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409929980.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409929985.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 42, finish FWD profile ...
rank:42, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.82,timestamp=4409929994.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.01,timestamp=4409930007.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4409930016.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4409930030.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.43,timestamp=4409930039.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.96,timestamp=4409930053.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4409930062.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409930076.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.43,timestamp=4409930085.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.87,timestamp=4409930099.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409930108.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.78,timestamp=4409930122.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409930130.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409930144.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4409930153.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.9,timestamp=4409930167.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.52,timestamp=4409930176.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409930190.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4409930199.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409930213.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409930222.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409930236.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4409930245.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4409930259.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4409930268.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409930282.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:42, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:42,optimizer_step time: 36.06630325317383
rank:42, finish optimizer.step profile ...
rank:42, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:42, trace log has been written to txt...
rank:42, finish release GPU memory ...
rank:42, After memory release - Allocated: 21790594048, Reserved: 33126612992
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 43, finish warm up ...
rank_id = 43, input_tensor_shapes: [(2048, 2, 8192)]
rank:43,cuda fwd time: 145.33120727539062
rank:43, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.58,timestamp=4409931635.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409931639.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.05,timestamp=4409931645.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4409931650.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.08,timestamp=4409931656.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409931661.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.11,timestamp=4409931667.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409931672.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409931678.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.17,timestamp=4409931682.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.2,timestamp=4409931689.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409931694.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4409931700.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409931705.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.36,timestamp=4409931712.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409931716.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.36,timestamp=4409931723.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409931728.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409931734.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409931739.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.26,timestamp=4409931746.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409931750.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.3,timestamp=4409931757.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409931762.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409931768.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.51,timestamp=4409931773.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 43, finish FWD profile ...
rank:43, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.78,timestamp=4409931781.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.09,timestamp=4409931795.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409931804.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.12,timestamp=4409931818.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409931827.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4409931841.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409931850.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409931864.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409931874.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.12,timestamp=4409931887.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4409931896.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4409931910.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4409931919.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409931933.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409931942.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.88,timestamp=4409931955.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409931964.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.08,timestamp=4409931978.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4409931987.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409932001.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409932011.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.18,timestamp=4409932025.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4409932034.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409932048.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.44,timestamp=4409932057.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409932071.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:43, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:43,optimizer_step time: 37.112831115722656
rank:43, finish optimizer.step profile ...
rank:43, Before memory release - Allocated: 43142928896, Reserved: 62241374208
rank:43, trace log has been written to txt...
rank:43, finish release GPU memory ...
rank:43, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 44, finish warm up ...
rank_id = 44, input_tensor_shapes: [(2048, 2, 8192)]
rank:44,cuda fwd time: 146.84364318847656
rank:44, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.62,timestamp=4409933366.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409933371.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.07,timestamp=4409933377.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409933381.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409933388.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409933392.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.13,timestamp=4409933398.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409933403.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409933409.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409933414.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409933420.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409933425.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409933432.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409933436.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.37,timestamp=4409933443.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409933448.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.41,timestamp=4409933454.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409933459.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.36,timestamp=4409933466.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409933470.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.34,timestamp=4409933477.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409933482.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.31,timestamp=4409933488.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409933493.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409933500.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409933504.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 44, finish FWD profile ...
rank:44, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.83,timestamp=4409933513.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.02,timestamp=4409933527.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409933536.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409933550.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409933559.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409933573.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409933581.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.76,timestamp=4409933595.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4409933604.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409933617.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4409933626.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.91,timestamp=4409933640.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409933649.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.85,timestamp=4409933663.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409933672.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.87,timestamp=4409933686.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409933695.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.91,timestamp=4409933709.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409933718.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.0,timestamp=4409933732.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.43,timestamp=4409933741.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.91,timestamp=4409933754.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409933763.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409933777.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409933786.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409933800.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:44, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:44,optimizer_step time: 36.05606460571289
rank:44, finish optimizer.step profile ...
rank:44, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:44, trace log has been written to txt...
rank:44, finish release GPU memory ...
rank:44, After memory release - Allocated: 21790594048, Reserved: 33126612992
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 45, finish warm up ...
rank_id = 45, input_tensor_shapes: [(2048, 2, 8192)]
rank:45,cuda fwd time: 158.5971221923828
rank:45, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.56,timestamp=4409935175.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409935180.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.08,timestamp=4409935186.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409935191.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.13,timestamp=4409935197.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409935201.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.16,timestamp=4409935208.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409935212.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.15,timestamp=4409935218.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.2,timestamp=4409935223.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.3,timestamp=4409935230.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409935234.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.31,timestamp=4409935241.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.08,timestamp=4409935252.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.17,timestamp=4409935265.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409935270.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.26,timestamp=4409935277.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409935281.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.22,timestamp=4409935288.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.8,timestamp=4409935292.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.01,timestamp=4409935300.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409935304.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409935311.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409935316.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409935322.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.5,timestamp=4409935327.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 45, finish FWD profile ...
rank:45, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=4.77,timestamp=4409935336.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.16,timestamp=4409935350.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409935359.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409935373.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4409935382.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.9,timestamp=4409935395.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409935404.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409935418.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409935427.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.73,timestamp=4409935441.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.23,timestamp=4409935449.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.73,timestamp=4409935463.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4409935472.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409935485.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409935494.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.7,timestamp=4409935508.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409935517.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409935530.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409935539.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4409935553.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4409935562.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4409935576.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4409935586.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409935599.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4409935608.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4409935622.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:45, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:45,optimizer_step time: 35.93830490112305
rank:45, finish optimizer.step profile ...
rank:45, Before memory release - Allocated: 43142928896, Reserved: 62241374208
rank:45, trace log has been written to txt...
rank:45, finish release GPU memory ...
rank:45, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 46, finish warm up ...
rank_id = 46, input_tensor_shapes: [(2048, 2, 8192)]
rank:46,cuda fwd time: 145.88723754882812
rank:46, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.59,timestamp=4409936937.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409936942.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.07,timestamp=4409936948.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409936952.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.11,timestamp=4409936958.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409936963.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.15,timestamp=4409936969.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409936974.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.13,timestamp=4409936980.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409936985.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.3,timestamp=4409936991.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409936996.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.36,timestamp=4409937003.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409937007.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409937014.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409937019.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.36,timestamp=4409937025.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409937030.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.25,timestamp=4409937037.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409937042.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.32,timestamp=4409937048.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409937053.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.3,timestamp=4409937060.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409937064.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.32,timestamp=4409937071.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409937076.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 46, finish FWD profile ...
rank:46, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.85,timestamp=4409937084.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.94,timestamp=4409937098.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409937107.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4409937121.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409937130.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4409937144.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409937153.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.87,timestamp=4409937166.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4409937175.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409937189.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409937198.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.11,timestamp=4409937212.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409937222.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.11,timestamp=4409937236.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4409937245.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.0,timestamp=4409937259.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4409937268.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.91,timestamp=4409937282.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409937290.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409937304.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409937313.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409937327.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4409937336.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4409937350.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409937359.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4409937373.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:46, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:46,optimizer_step time: 35.9813117980957
rank:46, finish optimizer.step profile ...
rank:46, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:46, trace log has been written to txt...
rank:46, finish release GPU memory ...
rank:46, After memory release - Allocated: 21790594048, Reserved: 33260830720
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 47, finish warm up ...
rank_id = 47, input_tensor_shapes: [(2048, 2, 8192)]
rank:47,cuda fwd time: 168.80947875976562
rank:47, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.59,timestamp=4409938697.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409938702.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.11,timestamp=4409938708.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409938712.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.13,timestamp=4409938718.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409938723.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.12,timestamp=4409938729.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409938734.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409938740.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4409938745.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.57,timestamp=4409938758.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=1.99,timestamp=4409938768.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.19,timestamp=4409938776.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.01,timestamp=4409938783.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.23,timestamp=4409938790.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409938794.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.19,timestamp=4409938801.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409938805.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.2,timestamp=4409938812.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409938817.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.18,timestamp=4409938823.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409938828.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409938834.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409938839.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.26,timestamp=4409938845.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409938850.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 47, finish FWD profile ...
rank:47, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.85,timestamp=4409938859.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.15,timestamp=4409938873.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409938882.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.12,timestamp=4409938896.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409938905.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.12,timestamp=4409938919.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409938928.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.2,timestamp=4409938942.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4409938951.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.25,timestamp=4409938966.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409938975.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4409938989.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409938998.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.89,timestamp=4409939011.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409939020.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4409939034.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409939044.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.27,timestamp=4409939058.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409939067.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.18,timestamp=4409939081.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409939090.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409939104.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409939113.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409939127.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409939136.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409939150.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:47, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:47,optimizer_step time: 35.994625091552734
rank:47, finish optimizer.step profile ...
rank:47, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:47, trace log has been written to txt...
rank:47, finish release GPU memory ...
rank:47, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 48, finish warm up ...
rank_id = 48, input_tensor_shapes: [(2048, 2, 8192)]
rank:48,cuda fwd time: 147.29933166503906
rank:48, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.61,timestamp=4409940495.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409940500.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.09,timestamp=4409940506.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409940511.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.12,timestamp=4409940517.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409940521.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.15,timestamp=4409940527.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409940532.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.14,timestamp=4409940538.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4409940543.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.15,timestamp=4409940549.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409940554.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.14,timestamp=4409940560.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409940565.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.13,timestamp=4409940571.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409940576.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.18,timestamp=4409940582.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.0,timestamp=4409940590.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.19,timestamp=4409940597.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409940601.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.23,timestamp=4409940608.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409940613.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.32,timestamp=4409940619.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409940624.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.26,timestamp=4409940631.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409940635.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 48, finish FWD profile ...
rank:48, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.74,timestamp=4409940644.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.94,timestamp=4409940658.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409940667.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.79,timestamp=4409940681.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409940690.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.7,timestamp=4409940703.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4409940712.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.71,timestamp=4409940726.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.23,timestamp=4409940735.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.7,timestamp=4409940749.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.23,timestamp=4409940757.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.78,timestamp=4409940771.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409940780.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409940794.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409940803.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4409940817.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.53,timestamp=4409940826.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.12,timestamp=4409940840.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.6,timestamp=4409940850.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4409940864.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.44,timestamp=4409940873.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409940887.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.27,timestamp=4409940896.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.72,timestamp=4409940909.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.2,timestamp=4409940918.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.6,timestamp=4409940932.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:48, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:48,optimizer_step time: 36.01203155517578
rank:48, finish optimizer.step profile ...
rank:48, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:48, trace log has been written to txt...
rank:48, finish release GPU memory ...
rank:48, After memory release - Allocated: 21790594048, Reserved: 33260830720
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 49, finish warm up ...
rank_id = 49, input_tensor_shapes: [(2048, 2, 8192)]
rank:49,cuda fwd time: 152.02713012695312
rank:49, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.49,timestamp=4409942309.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409942314.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.95,timestamp=4409942320.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4409942325.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.01,timestamp=4409942331.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409942336.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.01,timestamp=4409942342.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4409942346.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4409942352.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.73,timestamp=4409942357.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409942364.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.78,timestamp=4409942368.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409942375.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.77,timestamp=4409942379.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409942386.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4409942390.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409942397.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4409942402.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=1.81,timestamp=4409942416.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.67,timestamp=4409942420.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409942427.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4409942432.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409942438.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.77,timestamp=4409942443.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409942449.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.78,timestamp=4409942454.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 49, finish FWD profile ...
rank:49, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=4.54,timestamp=4409942463.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.89,timestamp=4409942477.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409942486.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.87,timestamp=4409942500.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4409942509.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.89,timestamp=4409942523.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.41,timestamp=4409942532.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409942546.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409942555.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.79,timestamp=4409942569.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409942578.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409942591.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4409942600.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409942614.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409942623.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.71,timestamp=4409942637.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.42,timestamp=4409942646.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409942660.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.61,timestamp=4409942669.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4409942683.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409942693.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409942707.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409942716.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409942730.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409942739.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409942753.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:49, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:49,optimizer_step time: 36.03148651123047
rank:49, finish optimizer.step profile ...
rank:49, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:49, trace log has been written to txt...
rank:49, finish release GPU memory ...
rank:49, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 3): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 50, finish warm up ...
rank_id = 50, input_tensor_shapes: [(2048, 2, 8192)]
rank:50,cuda fwd time: 147.15289306640625
rank:50, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.61,timestamp=4409944105.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409944109.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.1,timestamp=4409944116.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409944120.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.12,timestamp=4409944126.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409944131.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409944137.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409944142.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409944148.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409944152.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.3,timestamp=4409944159.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409944164.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.34,timestamp=4409944170.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409944175.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.42,timestamp=4409944182.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409944186.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.37,timestamp=4409944193.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409944198.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.4,timestamp=4409944204.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409944209.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.34,timestamp=4409944216.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409944221.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.05,timestamp=4409944229.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409944233.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.87,timestamp=4409944240.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.77,timestamp=4409944245.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 50, finish FWD profile ...
rank:50, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.68,timestamp=4409944254.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.84,timestamp=4409944268.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.12,timestamp=4409944277.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4409944291.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.17,timestamp=4409944300.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.62,timestamp=4409944313.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409944322.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4409944336.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409944345.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.08,timestamp=4409944359.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.6,timestamp=4409944369.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4409944383.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409944392.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409944406.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409944415.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409944430.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.6,timestamp=4409944439.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.0,timestamp=4409944453.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.27,timestamp=4409944462.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.69,timestamp=4409944475.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4409944484.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.72,timestamp=4409944498.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409944507.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4409944521.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409944530.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.19,timestamp=4409944544.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:50, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:50,optimizer_step time: 36.10726547241211
rank:50, finish optimizer.step profile ...
rank:50, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:50, trace log has been written to txt...
rank:50, finish release GPU memory ...
rank:50, After memory release - Allocated: 21790594048, Reserved: 33260830720
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 3): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 51, finish warm up ...
rank_id = 51, input_tensor_shapes: [(2048, 2, 8192)]
rank:51,cuda fwd time: 146.76889038085938
rank:51, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.62,timestamp=4409945847.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409945852.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.09,timestamp=4409945858.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409945863.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.15,timestamp=4409945869.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409945873.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409945880.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.12,timestamp=4409945884.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.15,timestamp=4409945890.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409945895.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.24,timestamp=4409945902.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409945906.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.32,timestamp=4409945913.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409945918.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.35,timestamp=4409945924.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409945929.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.37,timestamp=4409945936.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409945940.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.33,timestamp=4409945947.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.73,timestamp=4409945952.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.27,timestamp=4409945959.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.33,timestamp=4409945964.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409945971.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409945976.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409945982.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409945987.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 51, finish FWD profile ...
rank:51, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=4.59,timestamp=4409945996.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.06,timestamp=4409946010.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409946019.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.88,timestamp=4409946032.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409946041.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4409946055.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409946064.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409946078.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409946087.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409946101.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409946110.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409946124.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.51,timestamp=4409946133.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.09,timestamp=4409946147.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409946156.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.11,timestamp=4409946170.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409946179.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.0,timestamp=4409946193.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4409946202.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.78,timestamp=4409946216.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.28,timestamp=4409946225.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.71,timestamp=4409946238.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.22,timestamp=4409946247.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.75,timestamp=4409946261.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4409946270.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4409946284.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:51, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:51,optimizer_step time: 36.02534484863281
rank:51, finish optimizer.step profile ...
rank:51, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:51, trace log has been written to txt...
rank:51, finish release GPU memory ...
rank:51, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 3): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 52, finish warm up ...
rank_id = 52, input_tensor_shapes: [(2048, 2, 8192)]
rank:52,cuda fwd time: 152.89447021484375
rank:52, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.62,timestamp=4409947584.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409947588.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.1,timestamp=4409947594.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409947599.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.16,timestamp=4409947605.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409947610.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.11,timestamp=4409947616.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409947621.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.19,timestamp=4409947627.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409947631.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.17,timestamp=4409947638.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409947642.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.23,timestamp=4409947649.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409947653.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.18,timestamp=4409947660.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409947664.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.16,timestamp=4409947679.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409947684.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409947690.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409947695.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.18,timestamp=4409947702.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409947707.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4409947713.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409947718.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.32,timestamp=4409947725.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4409947729.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 52, finish FWD profile ...
rank:52, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.76,timestamp=4409947738.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.9,timestamp=4409947751.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409947760.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.79,timestamp=4409947774.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.18,timestamp=4409947783.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.71,timestamp=4409947796.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.21,timestamp=4409947805.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.69,timestamp=4409947819.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.23,timestamp=4409947828.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.71,timestamp=4409947841.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409947850.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.79,timestamp=4409947864.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409947873.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409947886.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409947895.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.18,timestamp=4409947910.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409947919.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409947933.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409947942.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409947956.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4409947965.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409947978.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.27,timestamp=4409947987.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.75,timestamp=4409948001.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409948010.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.91,timestamp=4409948024.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:52, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:52,optimizer_step time: 36.00998306274414
rank:52, finish optimizer.step profile ...
rank:52, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:52, trace log has been written to txt...
rank:52, finish release GPU memory ...
rank:52, After memory release - Allocated: 21790594048, Reserved: 33260830720
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 3): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 53, finish warm up ...
rank_id = 53, input_tensor_shapes: [(2048, 2, 8192)]
rank:53,cuda fwd time: 153.22726440429688
rank:53, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.62,timestamp=4409949314.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409949319.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.09,timestamp=4409949325.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409949330.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409949336.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409949340.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.16,timestamp=4409949346.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409949351.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409949357.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409949362.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=1.92,timestamp=4409949370.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409949374.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.23,timestamp=4409949383.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.37,timestamp=4409949387.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.22,timestamp=4409949396.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.03,timestamp=4409949403.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.25,timestamp=4409949410.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409949415.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.24,timestamp=4409949421.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409949426.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.79,timestamp=4409949433.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.16,timestamp=4409949438.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409949444.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409949449.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.2,timestamp=4409949456.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409949460.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 53, finish FWD profile ...
rank:53, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.57,timestamp=4409949469.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.16,timestamp=4409949483.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409949492.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.12,timestamp=4409949506.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409949516.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409949530.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.59,timestamp=4409949539.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409949553.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409949562.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4409949576.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4409949584.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409949598.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4409949607.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409949621.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409949630.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.87,timestamp=4409949643.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409949652.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.11,timestamp=4409949666.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4409949676.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.27,timestamp=4409949690.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409949699.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4409949713.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409949722.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409949736.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.44,timestamp=4409949745.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409949759.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:53, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:53,optimizer_step time: 36.00076675415039
rank:53, finish optimizer.step profile ...
rank:53, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:53, trace log has been written to txt...
rank:53, finish release GPU memory ...
rank:53, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 3): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 54, finish warm up ...
rank_id = 54, input_tensor_shapes: [(2048, 2, 8192)]
rank:54,cuda fwd time: 145.66400146484375
rank:54, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.49,timestamp=4409951141.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409951146.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.95,timestamp=4409951152.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409951157.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.97,timestamp=4409951163.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409951168.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.99,timestamp=4409951174.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409951178.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.75,timestamp=4409951185.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.75,timestamp=4409951189.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409951196.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4409951200.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409951207.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4409951212.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409951218.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4409951223.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409951229.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409951234.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4409951241.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409951246.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409951252.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409951257.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4409951264.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409951268.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409951275.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409951280.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 54, finish FWD profile ...
rank:54, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.22,timestamp=4409951289.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.16,timestamp=4409951303.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4409951312.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409951326.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4409951335.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.11,timestamp=4409951349.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409951358.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.25,timestamp=4409951372.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409951381.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409951395.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4409951405.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4409951418.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.42,timestamp=4409951427.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4409951441.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409951450.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409951464.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409951473.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.84,timestamp=4409951487.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4409951495.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409951509.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409951519.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.19,timestamp=4409951533.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409951542.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.24,timestamp=4409951556.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4409951565.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.11,timestamp=4409951579.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:54, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:54,optimizer_step time: 36.153343200683594
rank:54, finish optimizer.step profile ...
rank:54, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:54, trace log has been written to txt...
rank:54, finish release GPU memory ...
rank:54, After memory release - Allocated: 21790594048, Reserved: 33260830720
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 3): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 55, finish warm up ...
rank_id = 55, input_tensor_shapes: [(2048, 2, 8192)]
rank:55,cuda fwd time: 145.30355834960938
rank:55, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.58,timestamp=4409952930.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409952934.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.07,timestamp=4409952941.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409952945.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.15,timestamp=4409952951.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409952956.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.15,timestamp=4409952962.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.16,timestamp=4409952967.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.2,timestamp=4409952973.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409952978.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.25,timestamp=4409952984.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409952989.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.26,timestamp=4409952996.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409953000.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409953007.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.65,timestamp=4409953012.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.25,timestamp=4409953018.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409953023.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.2,timestamp=4409953029.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409953034.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.32,timestamp=4409953041.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409953046.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.3,timestamp=4409953052.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409953057.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.24,timestamp=4409953063.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409953068.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 55, finish FWD profile ...
rank:55, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.28,timestamp=4409953077.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.07,timestamp=4409953091.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409953100.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4409953114.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409953123.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409953137.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.41,timestamp=4409953146.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.0,timestamp=4409953159.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4409953168.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409953182.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.28,timestamp=4409953191.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.72,timestamp=4409953204.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4409953213.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.79,timestamp=4409953227.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4409953236.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4409953250.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4409953259.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.26,timestamp=4409953273.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409953283.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.26,timestamp=4409953297.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409953306.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4409953320.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409953329.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409953343.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409953352.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.84,timestamp=4409953366.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:55, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:55,optimizer_step time: 36.04684829711914
rank:55, finish optimizer.step profile ...
rank:55, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:55, trace log has been written to txt...
rank:55, finish release GPU memory ...
rank:55, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 56, finish warm up ...
rank_id = 56, input_tensor_shapes: [(2048, 2, 8192)]
rank:56,cuda fwd time: 148.35098266601562
rank:56, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.59,timestamp=4409954658.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409954663.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.08,timestamp=4409954669.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409954673.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.12,timestamp=4409954679.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409954684.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409954690.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409954695.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409954701.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409954706.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.14,timestamp=4409954712.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409954717.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.18,timestamp=4409954723.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409954728.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.18,timestamp=4409954734.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.42,timestamp=4409954739.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.14,timestamp=4409954745.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409954750.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=1.88,timestamp=4409954760.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409954765.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409954772.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409954776.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.2,timestamp=4409954783.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409954788.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409954794.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409954799.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 56, finish FWD profile ...
rank:56, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=4.81,timestamp=4409954808.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.99,timestamp=4409954822.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409954831.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.77,timestamp=4409954844.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409954853.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409954867.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4409954876.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409954890.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409954899.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409954913.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409954922.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.12,timestamp=4409954936.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4409954945.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4409954959.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409954969.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409954983.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.5,timestamp=4409954992.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4409955005.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409955014.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409955028.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.25,timestamp=4409955037.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.75,timestamp=4409955051.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.41,timestamp=4409955060.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4409955073.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409955083.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.11,timestamp=4409955097.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:56, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:56,optimizer_step time: 36.10111999511719
rank:56, finish optimizer.step profile ...
rank:56, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:56, trace log has been written to txt...
rank:56, finish release GPU memory ...
rank:56, After memory release - Allocated: 21790594048, Reserved: 33260830720
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 57, finish warm up ...
rank_id = 57, input_tensor_shapes: [(2048, 2, 8192)]
rank:57,cuda fwd time: 146.23948669433594
rank:57, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.62,timestamp=4409956391.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409956396.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.06,timestamp=4409956402.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409956407.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.12,timestamp=4409956413.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409956418.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.16,timestamp=4409956424.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.18,timestamp=4409956428.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.26,timestamp=4409956435.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409956439.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.25,timestamp=4409956446.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409956451.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.35,timestamp=4409956457.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409956462.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.39,timestamp=4409956469.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409956474.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.38,timestamp=4409956480.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409956485.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.31,timestamp=4409956491.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409956496.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.24,timestamp=4409956503.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409956508.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409956515.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409956519.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409956526.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409956531.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 57, finish FWD profile ...
rank:57, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=4.57,timestamp=4409956539.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.09,timestamp=4409956553.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409956562.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.89,timestamp=4409956576.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4409956585.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4409956599.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.61,timestamp=4409956608.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.19,timestamp=4409956622.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409956631.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4409956645.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.44,timestamp=4409956654.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.85,timestamp=4409956668.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4409956677.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409956691.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4409956700.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.95,timestamp=4409956714.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409956723.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.12,timestamp=4409956737.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409956746.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.26,timestamp=4409956760.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409956769.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4409956783.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.53,timestamp=4409956793.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409956806.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4409956815.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.88,timestamp=4409956829.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:57, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:57,optimizer_step time: 36.06425476074219
rank:57, finish optimizer.step profile ...
rank:57, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:57, trace log has been written to txt...
rank:57, finish release GPU memory ...
rank:57, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 58, finish warm up ...
rank_id = 58, input_tensor_shapes: [(2048, 2, 8192)]
rank:58,cuda fwd time: 150.36927795410156
rank:58, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.62,timestamp=4409958131.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4409958135.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.1,timestamp=4409958141.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409958146.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.13,timestamp=4409958152.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409958157.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409958163.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409958168.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.12,timestamp=4409958174.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409958178.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.14,timestamp=4409958185.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.0,timestamp=4409958196.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409958202.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409958207.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4409958213.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409958218.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.25,timestamp=4409958224.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409958229.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.2,timestamp=4409958235.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409958240.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4409958247.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409958251.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.3,timestamp=4409958258.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409958263.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.25,timestamp=4409958269.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4409958274.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 58, finish FWD profile ...
rank:58, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=4.67,timestamp=4409958283.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.93,timestamp=4409958297.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4409958306.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409958320.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409958329.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409958343.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409958352.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409958366.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409958375.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409958389.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409958398.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4409958412.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409958421.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.85,timestamp=4409958435.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.41,timestamp=4409958444.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.07,timestamp=4409958458.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.68,timestamp=4409958467.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.26,timestamp=4409958481.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409958490.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.19,timestamp=4409958504.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409958513.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409958527.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409958536.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4409958550.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4409958559.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.87,timestamp=4409958573.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:58, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:58,optimizer_step time: 35.957759857177734
rank:58, finish optimizer.step profile ...
rank:58, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:58, trace log has been written to txt...
rank:58, finish release GPU memory ...
rank:58, After memory release - Allocated: 21790594048, Reserved: 33260830720
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 59, finish warm up ...
rank_id = 59, input_tensor_shapes: [(2048, 2, 8192)]
rank:59,cuda fwd time: 151.446533203125
rank:59, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.6,timestamp=4409959898.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409959902.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.07,timestamp=4409959908.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409959913.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.15,timestamp=4409959919.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409959924.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.18,timestamp=4409959930.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.18,timestamp=4409959935.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.19,timestamp=4409959941.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409959946.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.13,timestamp=4409959952.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409959957.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409959963.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409959968.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409959975.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409959979.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.31,timestamp=4409959986.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409959991.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409959997.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409960002.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.31,timestamp=4409960008.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409960013.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.34,timestamp=4409960020.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409960025.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.23,timestamp=4409960037.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409960042.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 59, finish FWD profile ...
rank:59, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=4.64,timestamp=4409960051.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.92,timestamp=4409960065.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.23,timestamp=4409960073.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.67,timestamp=4409960087.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.23,timestamp=4409960096.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.69,timestamp=4409960110.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.24,timestamp=4409960118.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409960132.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4409960141.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4409960155.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4409960164.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.25,timestamp=4409960178.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4409960188.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409960202.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4409960211.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4409960225.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4409960234.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.91,timestamp=4409960248.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409960257.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.84,timestamp=4409960270.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409960279.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409960293.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409960302.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4409960316.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409960325.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.22,timestamp=4409960339.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:59, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:59,optimizer_step time: 35.9997444152832
rank:59, finish optimizer.step profile ...
rank:59, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:59, trace log has been written to txt...
rank:59, finish release GPU memory ...
rank:59, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 60, finish warm up ...
rank_id = 60, input_tensor_shapes: [(2048, 2, 8192)]
rank:60,cuda fwd time: 152.26162719726562
rank:60, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.57,timestamp=4409961639.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409961644.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.07,timestamp=4409961650.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409961655.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.13,timestamp=4409961661.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409961666.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409961672.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.14,timestamp=4409961676.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.2,timestamp=4409961683.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409961687.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409961694.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409961699.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.3,timestamp=4409961705.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409961710.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.32,timestamp=4409961716.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409961721.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.34,timestamp=4409961728.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409961733.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409961739.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409961744.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.22,timestamp=4409961753.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.05,timestamp=4409961757.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.64,timestamp=4409961766.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409961770.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.18,timestamp=4409961777.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409961781.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 60, finish FWD profile ...
rank:60, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.77,timestamp=4409961790.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.95,timestamp=4409961804.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409961813.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.8,timestamp=4409961826.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409961835.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.8,timestamp=4409961849.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.27,timestamp=4409961858.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409961871.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.26,timestamp=4409961880.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409961894.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409961903.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.88,timestamp=4409961916.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409961925.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409961940.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.64,timestamp=4409961949.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.09,timestamp=4409961963.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.63,timestamp=4409961972.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.19,timestamp=4409961986.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409961995.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409962009.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.43,timestamp=4409962018.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.9,timestamp=4409962032.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409962041.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409962055.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409962064.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.85,timestamp=4409962077.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:60, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:60,optimizer_step time: 36.04889678955078
rank:60, finish optimizer.step profile ...
rank:60, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:60, trace log has been written to txt...
rank:60, finish release GPU memory ...
rank:60, After memory release - Allocated: 21790594048, Reserved: 33260830720
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 61, finish warm up ...
rank_id = 61, input_tensor_shapes: [(2048, 2, 8192)]
rank:61,cuda fwd time: 151.23968505859375
rank:61, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.59,timestamp=4409963402.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409963407.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.08,timestamp=4409963413.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409963418.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.13,timestamp=4409963424.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409963429.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.15,timestamp=4409963435.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409963439.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.05,timestamp=4409963446.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4409963450.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.11,timestamp=4409963462.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4409963467.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.31,timestamp=4409963473.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409963478.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409963484.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409963489.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.31,timestamp=4409963496.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409963500.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409963507.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409963512.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.38,timestamp=4409963518.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409963523.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.36,timestamp=4409963530.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409963534.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4409963542.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409963547.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 61, finish FWD profile ...
rank:61, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=4.46,timestamp=4409963556.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.94,timestamp=4409963569.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.28,timestamp=4409963578.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.78,timestamp=4409963592.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.24,timestamp=4409963601.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.7,timestamp=4409963614.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.23,timestamp=4409963623.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409963637.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409963646.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409963659.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.27,timestamp=4409963668.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4409963682.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409963691.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.31,timestamp=4409963705.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.77,timestamp=4409963715.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.37,timestamp=4409963729.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.75,timestamp=4409963738.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4409963752.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4409963761.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4409963775.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4409963784.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409963798.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409963807.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4409963821.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4409963830.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.06,timestamp=4409963844.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:61, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:61,optimizer_step time: 36.04070281982422
rank:61, finish optimizer.step profile ...
rank:61, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:61, trace log has been written to txt...
rank:61, finish release GPU memory ...
rank:61, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 62, finish warm up ...
rank_id = 62, input_tensor_shapes: [(2048, 2, 8192)]
rank:62,cuda fwd time: 146.48626708984375
rank:62, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.41,timestamp=4409965162.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409965167.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.1,timestamp=4409965173.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409965178.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.11,timestamp=4409965184.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409965189.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409965195.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.19,timestamp=4409965200.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.25,timestamp=4409965206.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409965211.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409965217.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409965222.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.33,timestamp=4409965229.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409965233.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.34,timestamp=4409965240.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409965245.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.36,timestamp=4409965251.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409965256.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.38,timestamp=4409965263.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409965268.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.35,timestamp=4409965274.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409965279.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.36,timestamp=4409965286.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409965290.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.34,timestamp=4409965297.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409965302.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 62, finish FWD profile ...
rank:62, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.91,timestamp=4409965310.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.28,timestamp=4409965324.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409965333.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.22,timestamp=4409965348.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409965357.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4409965371.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.5,timestamp=4409965380.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.9,timestamp=4409965394.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4409965403.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.85,timestamp=4409965416.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409965425.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409965439.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409965448.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.84,timestamp=4409965462.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409965470.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409965484.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4409965493.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.09,timestamp=4409965507.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4409965516.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.24,timestamp=4409965531.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4409965540.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4409965554.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4409965563.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409965577.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.43,timestamp=4409965586.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4409965600.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:62, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:62,optimizer_step time: 35.934207916259766
rank:62, finish optimizer.step profile ...
rank:62, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:62, trace log has been written to txt...
rank:62, finish release GPU memory ...
rank:62, After memory release - Allocated: 21790594048, Reserved: 33260830720
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 63, finish warm up ...
rank_id = 63, input_tensor_shapes: [(2048, 2, 8192)]
rank:63,cuda fwd time: 145.81759643554688
rank:63, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.61,timestamp=4409966917.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409966922.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.1,timestamp=4409966928.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409966933.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409966939.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409966943.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.16,timestamp=4409966950.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409966954.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.12,timestamp=4409966960.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409966965.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.17,timestamp=4409966971.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409966976.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409966983.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409966987.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.34,timestamp=4409966994.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409966999.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.37,timestamp=4409967005.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409967010.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.32,timestamp=4409967017.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409967022.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.23,timestamp=4409967028.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409967033.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.35,timestamp=4409967040.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409967045.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.34,timestamp=4409967051.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409967056.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 63, finish FWD profile ...
rank:63, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.84,timestamp=4409967064.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.06,timestamp=4409967078.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409967087.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4409967101.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409967110.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.19,timestamp=4409967124.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4409967133.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.26,timestamp=4409967148.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409967157.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.18,timestamp=4409967171.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409967180.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409967194.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4409967203.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409967217.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.27,timestamp=4409967225.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409967239.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409967248.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.8,timestamp=4409967262.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409967271.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409967284.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409967293.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409967307.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409967316.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4409967330.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409967339.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.23,timestamp=4409967353.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:63, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:63,optimizer_step time: 36.02431869506836
rank:63, finish optimizer.step profile ...
rank:63, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:63, trace log has been written to txt...
rank:63, finish release GPU memory ...
rank:63, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 64, finish warm up ...
rank_id = 64, input_tensor_shapes: [(2048, 2, 8192)]
rank:64,cuda fwd time: 146.85594177246094
rank:64, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.61,timestamp=4409968666.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409968671.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.1,timestamp=4409968677.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409968682.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.09,timestamp=4409968688.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409968692.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.15,timestamp=4409968699.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409968703.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4409968709.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409968714.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.13,timestamp=4409968720.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4409968725.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.17,timestamp=4409968731.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.0,timestamp=4409968738.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.17,timestamp=4409968744.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409968749.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409968755.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409968760.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.25,timestamp=4409968767.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409968771.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.23,timestamp=4409968778.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409968783.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.13,timestamp=4409968790.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409968795.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.19,timestamp=4409968801.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409968806.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 64, finish FWD profile ...
rank:64, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.82,timestamp=4409968814.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.14,timestamp=4409968828.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409968837.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4409968851.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409968860.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409968874.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409968883.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409968897.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.43,timestamp=4409968906.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409968920.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409968929.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.95,timestamp=4409968943.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.35,timestamp=4409968952.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4409968966.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.41,timestamp=4409968975.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409968989.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.58,timestamp=4409968998.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.19,timestamp=4409969012.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.64,timestamp=4409969021.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.25,timestamp=4409969035.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409969045.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4409969059.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4409969068.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409969082.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409969091.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409969104.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:64, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:64,optimizer_step time: 35.928062438964844
rank:64, finish optimizer.step profile ...
rank:64, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:64, trace log has been written to txt...
rank:64, finish release GPU memory ...
rank:64, After memory release - Allocated: 21790594048, Reserved: 33260830720
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 65, finish warm up ...
rank_id = 65, input_tensor_shapes: [(2048, 2, 8192)]
rank:65,cuda fwd time: 148.71653747558594
rank:65, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.5,timestamp=4409970483.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409970488.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.99,timestamp=4409970494.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409970499.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.02,timestamp=4409970505.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4409970510.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.01,timestamp=4409970516.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4409970520.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4409970527.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.74,timestamp=4409970531.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409970538.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4409970542.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409970549.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.78,timestamp=4409970553.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409970564.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4409970569.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409970576.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.77,timestamp=4409970580.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409970587.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4409970591.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409970598.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4409970603.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409970609.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4409970614.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409970620.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409970625.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 65, finish FWD profile ...
rank:65, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.83,timestamp=4409970633.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.14,timestamp=4409970647.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4409970656.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.19,timestamp=4409970670.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.64,timestamp=4409970680.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.24,timestamp=4409970694.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4409970703.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.28,timestamp=4409970717.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.63,timestamp=4409970726.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.25,timestamp=4409970741.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4409970750.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.28,timestamp=4409970764.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409970773.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409970787.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409970796.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4409970809.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409970819.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4409970833.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4409970842.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.28,timestamp=4409970856.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409970865.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409970879.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4409970888.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.05,timestamp=4409970902.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4409970911.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409970925.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:65, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:65,optimizer_step time: 36.020225524902344
rank:65, finish optimizer.step profile ...
rank:65, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:65, trace log has been written to txt...
rank:65, finish release GPU memory ...
rank:65, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 66, finish warm up ...
rank_id = 66, input_tensor_shapes: [(2048, 2, 8192)]
rank:66,cuda fwd time: 153.133056640625
rank:66, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.52,timestamp=4409972347.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409972351.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.97,timestamp=4409972358.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4409972362.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.01,timestamp=4409972368.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409972373.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.13,timestamp=4409972379.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409972384.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.17,timestamp=4409972390.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409972395.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409972401.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409972406.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.3,timestamp=4409972412.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409972417.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.25,timestamp=4409972423.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409972428.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.37,timestamp=4409972435.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409972440.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.33,timestamp=4409972446.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409972451.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.38,timestamp=4409972457.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409972462.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409972469.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409972474.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.35,timestamp=4409972480.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409972485.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 66, finish FWD profile ...
rank:66, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.87,timestamp=4409972493.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.1,timestamp=4409972507.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409972516.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.91,timestamp=4409972530.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409972539.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4409972553.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409972562.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.05,timestamp=4409972576.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4409972585.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.2,timestamp=4409972599.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409972608.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409972622.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4409972631.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4409972645.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409972654.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409972667.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409972676.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.76,timestamp=4409972690.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409972699.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4409972713.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.5,timestamp=4409972722.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409972736.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.68,timestamp=4409972745.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.28,timestamp=4409972759.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409972768.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4409972782.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:66, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:66,optimizer_step time: 36.01203155517578
rank:66, finish optimizer.step profile ...
rank:66, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:66, trace log has been written to txt...
rank:66, finish release GPU memory ...
rank:66, After memory release - Allocated: 21790594048, Reserved: 33260830720
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 67, finish warm up ...
rank_id = 67, input_tensor_shapes: [(2048, 2, 8192)]
rank:67,cuda fwd time: 145.9804229736328
rank:67, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.6,timestamp=4409974076.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409974081.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.1,timestamp=4409974087.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409974091.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.11,timestamp=4409974098.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409974102.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.15,timestamp=4409974108.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.14,timestamp=4409974113.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.23,timestamp=4409974119.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409974124.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.24,timestamp=4409974130.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409974135.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.33,timestamp=4409974142.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409974147.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.39,timestamp=4409974153.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409974158.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.39,timestamp=4409974165.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409974169.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4409974176.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409974181.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.34,timestamp=4409974187.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409974192.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.39,timestamp=4409974199.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409974203.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409974210.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409974215.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 67, finish FWD profile ...
rank:67, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.95,timestamp=4409974223.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.39,timestamp=4409974237.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.62,timestamp=4409974247.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.2,timestamp=4409974261.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409974270.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.95,timestamp=4409974284.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4409974293.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.9,timestamp=4409974306.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409974315.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.74,timestamp=4409974329.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.27,timestamp=4409974338.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.76,timestamp=4409974351.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.28,timestamp=4409974360.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.75,timestamp=4409974374.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409974383.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.75,timestamp=4409974396.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409974405.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409974419.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.68,timestamp=4409974429.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.27,timestamp=4409974443.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409974452.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409974466.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4409974475.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409974489.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4409974498.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409974512.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:67, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:67,optimizer_step time: 36.005889892578125
rank:67, finish optimizer.step profile ...
rank:67, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:67, trace log has been written to txt...
rank:67, finish release GPU memory ...
rank:67, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 4): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 68, finish warm up ...
rank_id = 68, input_tensor_shapes: [(2048, 2, 8192)]
rank:68,cuda fwd time: 148.9602508544922
rank:68, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.62,timestamp=4409975826.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4409975830.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.09,timestamp=4409975837.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409975841.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.13,timestamp=4409975847.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.13,timestamp=4409975852.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.19,timestamp=4409975858.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.12,timestamp=4409975863.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.19,timestamp=4409975869.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409975874.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.38,timestamp=4409975880.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409975885.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.42,timestamp=4409975892.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409975897.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.4,timestamp=4409975903.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409975908.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.42,timestamp=4409975915.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409975919.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.38,timestamp=4409975926.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409975931.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.4,timestamp=4409975937.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409975942.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.35,timestamp=4409975949.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.07,timestamp=4409975956.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.33,timestamp=4409975962.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409975967.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 68, finish FWD profile ...
rank:68, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.91,timestamp=4409975976.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.18,timestamp=4409975990.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.44,timestamp=4409975999.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.95,timestamp=4409976012.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.45,timestamp=4409976022.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4409976035.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409976044.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.12,timestamp=4409976058.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4409976068.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.27,timestamp=4409976082.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.68,timestamp=4409976091.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409976105.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409976114.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4409976128.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.44,timestamp=4409976137.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.89,timestamp=4409976151.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409976160.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409976174.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409976183.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4409976196.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4409976205.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.1,timestamp=4409976219.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.64,timestamp=4409976229.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.24,timestamp=4409976243.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.63,timestamp=4409976252.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409976266.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:68, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:68,optimizer_step time: 36.06732940673828
rank:68, finish optimizer.step profile ...
rank:68, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:68, trace log has been written to txt...
rank:68, finish release GPU memory ...
rank:68, After memory release - Allocated: 21790594048, Reserved: 33260830720
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 4): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 69, finish warm up ...
rank_id = 69, input_tensor_shapes: [(2048, 2, 8192)]
rank:69,cuda fwd time: 147.67922973632812
rank:69, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.62,timestamp=4409977586.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409977591.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.09,timestamp=4409977597.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409977602.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409977608.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409977612.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.15,timestamp=4409977618.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409977623.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409977629.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409977634.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409977643.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409977648.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.26,timestamp=4409977654.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409977659.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409977665.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409977670.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.3,timestamp=4409977677.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409977681.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.26,timestamp=4409977688.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409977693.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.27,timestamp=4409977699.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409977704.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.3,timestamp=4409977710.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.5,timestamp=4409977715.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.55,timestamp=4409977722.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409977727.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 69, finish FWD profile ...
rank:69, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.82,timestamp=4409977735.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.96,timestamp=4409977749.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.23,timestamp=4409977758.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.8,timestamp=4409977771.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409977780.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.79,timestamp=4409977794.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409977803.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.85,timestamp=4409977817.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409977826.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409977839.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.37,timestamp=4409977848.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.07,timestamp=4409977862.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409977871.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.27,timestamp=4409977886.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4409977895.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409977909.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409977918.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409977932.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.43,timestamp=4409977941.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409977955.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4409977964.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409977977.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409977986.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.88,timestamp=4409978000.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409978009.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409978023.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:69, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:69,optimizer_step time: 36.01510238647461
rank:69, finish optimizer.step profile ...
rank:69, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:69, trace log has been written to txt...
rank:69, finish release GPU memory ...
rank:69, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 4): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 70, finish warm up ...
rank_id = 70, input_tensor_shapes: [(2048, 2, 8192)]
rank:70,cuda fwd time: 145.87802124023438
rank:70, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.6,timestamp=4409979329.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409979334.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.08,timestamp=4409979340.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409979345.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.13,timestamp=4409979351.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409979355.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409979361.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409979366.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.12,timestamp=4409979372.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409979377.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.33,timestamp=4409979384.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409979388.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.35,timestamp=4409979395.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409979400.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.37,timestamp=4409979406.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409979411.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.38,timestamp=4409979418.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409979423.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.4,timestamp=4409979429.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409979434.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409979441.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409979445.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.33,timestamp=4409979452.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409979457.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.3,timestamp=4409979463.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409979468.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 70, finish FWD profile ...
rank:70, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.03,timestamp=4409979477.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.1,timestamp=4409979491.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4409979500.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.87,timestamp=4409979513.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409979522.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.88,timestamp=4409979536.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.42,timestamp=4409979545.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409979559.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.52,timestamp=4409979568.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.18,timestamp=4409979582.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409979591.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.23,timestamp=4409979606.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4409979615.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.25,timestamp=4409979629.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.64,timestamp=4409979638.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.22,timestamp=4409979652.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4409979661.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.21,timestamp=4409979676.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.42,timestamp=4409979685.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4409979698.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409979707.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.85,timestamp=4409979721.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409979730.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.12,timestamp=4409979744.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409979753.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.26,timestamp=4409979767.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:70, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:70,optimizer_step time: 35.966976165771484
rank:70, finish optimizer.step profile ...
rank:70, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:70, trace log has been written to txt...
rank:70, finish release GPU memory ...
rank:70, After memory release - Allocated: 21790594048, Reserved: 33260830720
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 4): 1309355008
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 71, finish warm up ...
rank_id = 71, input_tensor_shapes: [(2048, 2, 8192)]
rank:71,cuda fwd time: 146.34701538085938
rank:71, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.6,timestamp=4409981073.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409981078.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.09,timestamp=4409981084.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409981089.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409981095.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.12,timestamp=4409981099.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.13,timestamp=4409981106.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409981110.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4409981116.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409981121.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.25,timestamp=4409981128.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409981133.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.33,timestamp=4409981139.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409981144.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.35,timestamp=4409981151.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409981155.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.33,timestamp=4409981162.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409981167.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.33,timestamp=4409981173.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409981178.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.34,timestamp=4409981185.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409981190.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.32,timestamp=4409981196.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409981201.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.62,timestamp=4409981208.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409981212.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 71, finish FWD profile ...
rank:71, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.73,timestamp=4409981221.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.15,timestamp=4409981235.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409981244.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4409981258.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409981267.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409981281.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409981290.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4409981304.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4409981313.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.1,timestamp=4409981327.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409981336.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.26,timestamp=4409981350.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409981359.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409981373.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.53,timestamp=4409981382.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409981396.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.44,timestamp=4409981405.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.95,timestamp=4409981419.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4409981428.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.87,timestamp=4409981442.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4409981451.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.9,timestamp=4409981465.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.52,timestamp=4409981474.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.18,timestamp=4409981488.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.7,timestamp=4409981497.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.25,timestamp=4409981511.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:71, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:71,optimizer_step time: 36.03558349609375
rank:71, finish optimizer.step profile ...
rank:71, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:71, trace log has been written to txt...
rank:71, finish release GPU memory ...
rank:71, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 72, finish warm up ...
rank_id = 72, input_tensor_shapes: [(2048, 2, 8192)]
rank:72,cuda fwd time: 149.7671661376953
rank:72, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.62,timestamp=4409982810.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409982815.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.12,timestamp=4409982821.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409982826.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.17,timestamp=4409982832.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409982837.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.16,timestamp=4409982843.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.13,timestamp=4409982847.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.13,timestamp=4409982854.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409982858.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.15,timestamp=4409982865.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.67,timestamp=4409982869.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.15,timestamp=4409982876.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409982880.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4409982887.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409982891.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.22,timestamp=4409982898.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.0,timestamp=4409982908.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4409982915.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409982919.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.26,timestamp=4409982926.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.72,timestamp=4409982931.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.2,timestamp=4409982937.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409982942.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4409982948.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409982953.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 72, finish FWD profile ...
rank:72, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.76,timestamp=4409982961.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.94,timestamp=4409982975.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409982984.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.85,timestamp=4409982998.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409983007.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409983020.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409983029.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409983043.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409983052.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409983066.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409983075.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409983088.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4409983097.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.85,timestamp=4409983111.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409983120.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.2,timestamp=4409983134.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4409983143.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.28,timestamp=4409983157.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409983167.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409983181.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4409983190.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.91,timestamp=4409983203.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409983212.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409983226.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409983235.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.87,timestamp=4409983249.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:72, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:72,optimizer_step time: 36.03558349609375
rank:72, finish optimizer.step profile ...
rank:72, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:72, trace log has been written to txt...
rank:72, finish release GPU memory ...
rank:72, After memory release - Allocated: 21790594048, Reserved: 33260830720
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 73, finish warm up ...
rank_id = 73, input_tensor_shapes: [(2048, 2, 8192)]
rank:73,cuda fwd time: 149.4036407470703
rank:73, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.52,timestamp=4409984630.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409984635.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.99,timestamp=4409984641.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4409984646.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.01,timestamp=4409984652.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4409984657.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.03,timestamp=4409984663.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4409984667.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409984674.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.75,timestamp=4409984678.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409984685.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4409984689.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409984696.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4409984700.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409984707.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.77,timestamp=4409984712.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409984718.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.0,timestamp=4409984727.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409984734.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4409984739.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409984745.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4409984750.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.43,timestamp=4409984757.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.78,timestamp=4409984761.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4409984768.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4409984772.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 73, finish FWD profile ...
rank:73, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.74,timestamp=4409984781.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.08,timestamp=4409984795.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409984804.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409984818.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.42,timestamp=4409984827.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409984841.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409984850.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409984863.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409984872.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.84,timestamp=4409984886.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.29,timestamp=4409984895.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.88,timestamp=4409984909.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.39,timestamp=4409984918.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.87,timestamp=4409984931.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.41,timestamp=4409984940.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.05,timestamp=4409984954.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4409984963.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.27,timestamp=4409984978.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4409984987.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409985001.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4409985010.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409985024.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.43,timestamp=4409985033.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4409985047.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409985056.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409985069.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:73, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:73,optimizer_step time: 36.19532775878906
rank:73, finish optimizer.step profile ...
rank:73, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:73, trace log has been written to txt...
rank:73, finish release GPU memory ...
rank:73, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 74, finish warm up ...
rank_id = 74, input_tensor_shapes: [(2048, 2, 8192)]
rank:74,cuda fwd time: 147.29624938964844
rank:74, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.61,timestamp=4409986377.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4409986382.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.12,timestamp=4409986388.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409986392.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.15,timestamp=4409986398.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409986403.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.16,timestamp=4409986409.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409986414.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.13,timestamp=4409986420.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409986425.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.24,timestamp=4409986433.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409986438.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.34,timestamp=4409986444.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.38,timestamp=4409986449.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.26,timestamp=4409986456.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409986461.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.37,timestamp=4409986467.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409986472.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.19,timestamp=4409986478.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.84,timestamp=4409986483.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4409986490.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409986494.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.34,timestamp=4409986501.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409986506.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409986512.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409986517.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 74, finish FWD profile ...
rank:74, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.83,timestamp=4409986525.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.01,timestamp=4409986539.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409986548.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4409986562.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.42,timestamp=4409986571.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.9,timestamp=4409986585.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409986594.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.84,timestamp=4409986608.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4409986616.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4409986630.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.44,timestamp=4409986639.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.11,timestamp=4409986653.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4409986663.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.26,timestamp=4409986677.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.68,timestamp=4409986686.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.25,timestamp=4409986700.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.68,timestamp=4409986709.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.27,timestamp=4409986724.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409986733.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.28,timestamp=4409986747.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4409986756.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.98,timestamp=4409986770.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.43,timestamp=4409986779.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4409986793.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4409986802.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409986816.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:74, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:74,optimizer_step time: 36.020225524902344
rank:74, finish optimizer.step profile ...
rank:74, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:74, trace log has been written to txt...
rank:74, finish release GPU memory ...
rank:74, After memory release - Allocated: 21790594048, Reserved: 33260830720
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 75, finish warm up ...
rank_id = 75, input_tensor_shapes: [(2048, 2, 8192)]
rank:75,cuda fwd time: 147.11807250976562
rank:75, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.61,timestamp=4409988127.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4409988131.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.1,timestamp=4409988138.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4409988142.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.13,timestamp=4409988148.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409988153.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409988159.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409988164.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.17,timestamp=4409988170.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.5,timestamp=4409988174.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.08,timestamp=4409988182.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409988186.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.15,timestamp=4409988194.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409988199.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.26,timestamp=4409988205.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4409988210.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.32,timestamp=4409988217.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409988221.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.31,timestamp=4409988228.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409988233.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.35,timestamp=4409988239.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409988244.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.31,timestamp=4409988251.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409988255.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.32,timestamp=4409988262.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409988267.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 75, finish FWD profile ...
rank:75, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.85,timestamp=4409988275.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=11.61,timestamp=4409988289.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.15,timestamp=4409988298.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.63,timestamp=4409988312.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.23,timestamp=4409988321.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.67,timestamp=4409988335.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409988344.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4409988357.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4409988366.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409988380.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.68,timestamp=4409988390.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.15,timestamp=4409988404.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4409988413.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.03,timestamp=4409988427.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.43,timestamp=4409988436.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.93,timestamp=4409988449.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4409988458.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409988472.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4409988481.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.99,timestamp=4409988495.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409988504.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.18,timestamp=4409988518.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.64,timestamp=4409988527.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.23,timestamp=4409988542.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409988551.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409988565.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:75, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:75,optimizer_step time: 36.05811309814453
rank:75, finish optimizer.step profile ...
rank:75, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:75, trace log has been written to txt...
rank:75, finish release GPU memory ...
rank:75, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 76, finish warm up ...
rank_id = 76, input_tensor_shapes: [(2048, 2, 8192)]
rank:76,cuda fwd time: 146.54054260253906
rank:76, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.49,timestamp=4409989956.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409989960.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.96,timestamp=4409989966.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409989971.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.0,timestamp=4409989977.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409989982.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.99,timestamp=4409989988.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409989993.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.55,timestamp=4409989999.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.78,timestamp=4409990004.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4409990010.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409990015.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409990022.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409990027.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.12,timestamp=4409990033.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409990038.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.15,timestamp=4409990045.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409990049.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4409990056.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409990061.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4409990067.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409990072.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409990079.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409990084.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409990090.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409990095.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 76, finish FWD profile ...
rank:76, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.9,timestamp=4409990104.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.23,timestamp=4409990118.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.62,timestamp=4409990127.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.22,timestamp=4409990141.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409990150.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.2,timestamp=4409990164.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4409990174.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.24,timestamp=4409990188.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409990197.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.04,timestamp=4409990211.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4409990220.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.84,timestamp=4409990234.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4409990243.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4409990257.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409990266.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.1,timestamp=4409990280.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.68,timestamp=4409990289.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.26,timestamp=4409990303.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4409990312.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4409990326.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4409990335.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.05,timestamp=4409990349.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.43,timestamp=4409990358.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.9,timestamp=4409990372.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.34,timestamp=4409990381.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.83,timestamp=4409990395.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:76, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:76,optimizer_step time: 36.92339324951172
rank:76, finish optimizer.step profile ...
rank:76, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:76, trace log has been written to txt...
rank:76, finish release GPU memory ...
rank:76, After memory release - Allocated: 21790594048, Reserved: 33260830720
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 77, finish warm up ...
rank_id = 77, input_tensor_shapes: [(2048, 2, 8192)]
rank:77,cuda fwd time: 147.16415405273438
rank:77, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.49,timestamp=4409991778.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409991783.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.95,timestamp=4409991789.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4409991794.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.01,timestamp=4409991800.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409991804.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.99,timestamp=4409991811.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409991815.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409991821.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.74,timestamp=4409991826.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409991833.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409991837.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.9,timestamp=4409991845.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.57,timestamp=4409991850.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4409991857.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4409991862.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409991868.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409991873.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.71,timestamp=4409991879.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4409991884.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409991891.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409991895.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409991902.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4409991907.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409991913.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.56,timestamp=4409991918.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 77, finish FWD profile ...
rank:77, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=4.72,timestamp=4409991927.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.2,timestamp=4409991941.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.5,timestamp=4409991950.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.11,timestamp=4409991964.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409991974.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.2,timestamp=4409991988.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4409991997.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.24,timestamp=4409992011.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.57,timestamp=4409992020.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.97,timestamp=4409992034.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409992043.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.84,timestamp=4409992057.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409992066.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.88,timestamp=4409992079.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.49,timestamp=4409992088.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4409992102.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409992112.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.27,timestamp=4409992126.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.67,timestamp=4409992135.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.28,timestamp=4409992149.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.66,timestamp=4409992158.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409992172.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.54,timestamp=4409992182.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.01,timestamp=4409992196.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.33,timestamp=4409992204.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409992218.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:77, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:77,optimizer_step time: 36.120574951171875
rank:77, finish optimizer.step profile ...
rank:77, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:77, trace log has been written to txt...
rank:77, finish release GPU memory ...
rank:77, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 78, finish warm up ...
rank_id = 78, input_tensor_shapes: [(2048, 2, 8192)]
rank:78,cuda fwd time: 149.05343627929688
rank:78, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.61,timestamp=4409993558.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409993563.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.08,timestamp=4409993569.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409993574.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.15,timestamp=4409993580.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409993584.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.13,timestamp=4409993590.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409993595.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4409993601.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4409993606.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.16,timestamp=4409993612.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409993617.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.16,timestamp=4409993623.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.0,timestamp=4409993632.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.2,timestamp=4409993639.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4409993643.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.15,timestamp=4409993650.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.48,timestamp=4409993654.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.19,timestamp=4409993662.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409993666.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.23,timestamp=4409993673.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409993677.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.28,timestamp=4409993684.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4409993689.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.32,timestamp=4409993695.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4409993700.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 78, finish FWD profile ...
rank:78, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.91,timestamp=4409993708.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.28,timestamp=4409993722.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.63,timestamp=4409993732.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.28,timestamp=4409993746.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.64,timestamp=4409993755.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4409993769.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4409993778.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.14,timestamp=4409993792.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.55,timestamp=4409993801.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.2,timestamp=4409993816.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.64,timestamp=4409993825.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.2,timestamp=4409993839.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.38,timestamp=4409993848.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409993862.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.47,timestamp=4409993871.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4409993885.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4409993894.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.26,timestamp=4409993908.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.64,timestamp=4409993917.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.17,timestamp=4409993931.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.56,timestamp=4409993940.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.02,timestamp=4409993954.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.48,timestamp=4409993963.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4409993977.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.36,timestamp=4409993986.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.86,timestamp=4409994000.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:78, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:78,optimizer_step time: 35.9628791809082
rank:78, finish optimizer.step profile ...
rank:78, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:78, trace log has been written to txt...
rank:78, finish release GPU memory ...
rank:78, After memory release - Allocated: 21790594048, Reserved: 33260830720
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1309355008 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 79, finish warm up ...
rank_id = 79, input_tensor_shapes: [(2048, 2, 8192)]
rank:79,cuda fwd time: 156.0166473388672
rank:79, fwd_subop num: 26, fwd_subop: ['trace_src_func=allreduce,duration=4.32,timestamp=4409995383.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409995388.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.97,timestamp=4409995394.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409995399.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.01,timestamp=4409995405.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4409995410.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.99,timestamp=4409995416.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409995421.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.01,timestamp=4409995430.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.74,timestamp=4409995435.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.85,timestamp=4409995441.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.74,timestamp=4409995446.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409995452.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=1.99,timestamp=4409995465.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.89,timestamp=4409995471.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.78,timestamp=4409995476.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4409995482.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.75,timestamp=4409995487.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4409995494.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4409995498.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409995505.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4409995509.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4409995516.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4409995521.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409995527.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4409995532.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 79, finish FWD profile ...
rank:79, bwd_subop num: 26, bwd_subop: ['trace_src_func=allreduce,duration=5.88,timestamp=4409995541.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.12,timestamp=4409995554.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.27,timestamp=4409995563.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.79,timestamp=4409995577.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409995586.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.81,timestamp=4409995600.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.3,timestamp=4409995609.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.84,timestamp=4409995622.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409995631.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.94,timestamp=4409995645.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.4,timestamp=4409995654.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.92,timestamp=4409995668.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.64,timestamp=4409995677.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.28,timestamp=4409995691.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.65,timestamp=4409995700.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.16,timestamp=4409995714.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.52,timestamp=4409995723.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.0,timestamp=4409995737.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.42,timestamp=4409995746.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.89,timestamp=4409995760.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.32,timestamp=4409995769.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.82,timestamp=4409995783.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.31,timestamp=4409995792.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.87,timestamp=4409995805.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.46,timestamp=4409995814.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=13.13,timestamp=4409995828.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:79, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:79,optimizer_step time: 36.12876892089844
rank:79, finish optimizer.step profile ...
rank:79, Before memory release - Allocated: 43142928896, Reserved: 62157488128
rank:79, trace log has been written to txt...
rank:79, finish release GPU memory ...
rank:79, After memory release - Allocated: 21790594048, Reserved: 32992395264
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8858000000 recvbuff 0x7f8858000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8858000000,7f8858000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 1360882688
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1360882688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f870a9ff000 recvbuff 0x7f870a9ff000 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f870a9ff000,7f870a9ff000,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 80, finish warm up ...
rank_id = 80, input_tensor_shapes: [(2048, 2, 8192)]
rank:80,cuda fwd time: 170.18882751464844
rank:80, fwd_subop num: 29, fwd_subop: ['trace_src_func=allreduce,duration=4.65,timestamp=4409997250.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4409997254.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.96,timestamp=4409997260.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409997265.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.89,timestamp=4409997271.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4409997276.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.03,timestamp=4409997282.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4409997287.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.13,timestamp=4409997293.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.12,timestamp=4409997298.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409997304.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409997308.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4409997315.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409997320.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.22,timestamp=4409997326.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4409997331.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.24,timestamp=4409997337.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4409997342.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4409997349.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.51,timestamp=4409997353.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.59,timestamp=4409997360.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.52,timestamp=4409997365.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.44,timestamp=4409997371.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.54,timestamp=4409997376.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.32,timestamp=4409997383.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.52,timestamp=4409997388.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=23.68,timestamp=4409997413.15,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4409997413.72,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.17,timestamp=4409997413.97,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 80, finish FWD profile ...
rank:80, bwd_subop num: 27, bwd_subop: ['trace_src_func=allreduce,duration=22.78,timestamp=4409997438.71,input__shape=[2048, 2, 8192],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.78,timestamp=4409997468.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.45,timestamp=4409997482.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.22,timestamp=4409997491.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.67,timestamp=4409997504.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.21,timestamp=4409997513.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.67,timestamp=4409997527.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.14,timestamp=4409997535.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.62,timestamp=4409997549.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.13,timestamp=4409997558.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.61,timestamp=4409997571.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.14,timestamp=4409997580.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.62,timestamp=4409997593.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.14,timestamp=4409997602.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.62,timestamp=4409997615.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.13,timestamp=4409997624.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.63,timestamp=4409997638.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.15,timestamp=4409997646.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.62,timestamp=4409997660.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.16,timestamp=4409997669.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.62,timestamp=4409997682.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.12,timestamp=4409997691.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.62,timestamp=4409997704.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.15,timestamp=4409997713.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.61,timestamp=4409997727.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.13,timestamp=4409997735.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.6,timestamp=4409997749.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:80, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f870a9ff000 recvbuff 0x7f870a9ff000 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f870a9ff000,7f870a9ff000,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:80,optimizer_step time: 30.282751083374023
rank:80, finish optimizer.step profile ...
rank:80, Before memory release - Allocated: 32811940864, Reserved: 63134760960
rank:80, trace log has been written to txt...
rank:80, finish release GPU memory ...
rank:80, After memory release - Allocated: 11181601792, Reserved: 27858567168
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff600 recvbuff 0x7f8b753ff600 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff600,7f8b753ff600,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff800 recvbuff 0x7f8b753ff800 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff800,7f8b753ff800,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffa00 recvbuff 0x7f8b753ffa00 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffa00,7f8b753ffa00,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f7e0c000000 recvbuff 0x7f7e0c000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f7e0c000000,7f7e0c000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 5): 1360882688
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1360882688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff200 recvbuff 0x7f88467ff200 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff200,7f88467ff200,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 81, finish warm up ...
rank_id = 81, input_tensor_shapes: [(2048, 2, 8192)]
rank:81,cuda fwd time: 167.0102996826172
rank:81, fwd_subop num: 29, fwd_subop: ['trace_src_func=allreduce,duration=4.63,timestamp=4409999113.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4409999118.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.08,timestamp=4409999124.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4409999129.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.1,timestamp=4409999135.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4409999139.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.13,timestamp=4409999145.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4409999150.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.15,timestamp=4409999156.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4409999161.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4409999167.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.7,timestamp=4409999171.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.59,timestamp=4409999178.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.38,timestamp=4409999182.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.4,timestamp=4409999189.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.37,timestamp=4409999194.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.32,timestamp=4409999200.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.4,timestamp=4409999205.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.25,timestamp=4409999211.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.4,timestamp=4409999216.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.3,timestamp=4409999222.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.37,timestamp=4409999227.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.36,timestamp=4409999233.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.37,timestamp=4409999238.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.31,timestamp=4409999244.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.39,timestamp=4409999249.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=22.82,timestamp=4409999273.54,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4409999274.09,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.17,timestamp=4409999274.34,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 81, finish FWD profile ...
rank:81, bwd_subop num: 27, bwd_subop: ['trace_src_func=allreduce,duration=22.31,timestamp=4409999298.57,input__shape=[2048, 2, 8192],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=27.91,timestamp=4409999327.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.12,timestamp=4409999340.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.03,timestamp=4409999349.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.54,timestamp=4409999363.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4409999371.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4409999385.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4409999393.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4409999407.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4409999415.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4409999429.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4409999438.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4409999451.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4409999460.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4409999473.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4409999482.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4409999495.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4409999504.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4409999517.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4409999526.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4409999539.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.09,timestamp=4409999548.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.54,timestamp=4409999562.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4409999570.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4409999584.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4409999592.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4409999606.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:81, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f870abff600 recvbuff 0x7f870abff600 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f870abff600,7f870abff600,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:81,optimizer_step time: 26.969087600708008
rank:81, finish optimizer.step profile ...
rank:81, Before memory release - Allocated: 33224838144, Reserved: 52529463296
rank:81, trace log has been written to txt...
rank:81, finish release GPU memory ...
rank:81, After memory release - Allocated: 11181601792, Reserved: 33239859200
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffc00 recvbuff 0x7f8b753ffc00 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffc00,7f8b753ffc00,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88466cc000 recvbuff 0x7f88466cc000 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88466cc000,7f88466cc000,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f7b72000000 recvbuff 0x7f7b72000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f7b72000000,7f7b72000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 5): 1360882688
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1360882688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff000 recvbuff 0x7f88467ff000 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff000,7f88467ff000,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 82, finish warm up ...
rank_id = 82, input_tensor_shapes: [(2048, 2, 8192)]
rank:82,cuda fwd time: 170.3690185546875
rank:82, fwd_subop num: 29, fwd_subop: ['trace_src_func=allreduce,duration=4.64,timestamp=4410000934.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4410000939.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.09,timestamp=4410000945.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4410000949.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.12,timestamp=4410000956.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4410000960.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.18,timestamp=4410000966.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4410000971.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.19,timestamp=4410000977.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.14,timestamp=4410000982.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.26,timestamp=4410000988.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.16,timestamp=4410000993.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.64,timestamp=4410000999.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.49,timestamp=4410001004.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.51,timestamp=4410001010.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.58,timestamp=4410001015.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.5,timestamp=4410001022.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.56,timestamp=4410001027.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.54,timestamp=4410001033.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.55,timestamp=4410001038.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.53,timestamp=4410001045.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.54,timestamp=4410001049.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.35,timestamp=4410001056.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.58,timestamp=4410001061.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.53,timestamp=4410001068.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.58,timestamp=4410001072.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=23.82,timestamp=4410001097.69,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4410001098.22,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.17,timestamp=4410001098.48,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 82, finish FWD profile ...
rank:82, bwd_subop num: 27, bwd_subop: ['trace_src_func=allreduce,duration=23.09,timestamp=4410001124.53,input__shape=[2048, 2, 8192],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.2,timestamp=4410001153.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.06,timestamp=4410001167.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.12,timestamp=4410001176.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4410001189.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410001198.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.52,timestamp=4410001211.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.09,timestamp=4410001220.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.63,timestamp=4410001234.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.18,timestamp=4410001242.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.67,timestamp=4410001256.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.17,timestamp=4410001265.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.68,timestamp=4410001278.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.16,timestamp=4410001287.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.71,timestamp=4410001301.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.19,timestamp=4410001310.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.71,timestamp=4410001323.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.17,timestamp=4410001332.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.6,timestamp=4410001345.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.09,timestamp=4410001354.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.6,timestamp=4410001368.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.1,timestamp=4410001376.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.61,timestamp=4410001390.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.12,timestamp=4410001398.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4410001412.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410001421.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.63,timestamp=4410001434.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:82, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff000 recvbuff 0x7f88467ff000 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff000,7f88467ff000,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:82,optimizer_step time: 27.026432037353516
rank:82, finish optimizer.step profile ...
rank:82, Before memory release - Allocated: 33224838144, Reserved: 52535754752
rank:82, trace log has been written to txt...
rank:82, finish release GPU memory ...
rank:82, After memory release - Allocated: 11181601792, Reserved: 27667726336
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff200 recvbuff 0x7f8b753ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff200,7f8b753ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff400 recvbuff 0x7f8b753ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff400,7f8b753ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8016000000 recvbuff 0x7f8016000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8016000000,7f8016000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 5): 1360882688
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1360882688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff000 recvbuff 0x7f88467ff000 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff000,7f88467ff000,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 83, finish warm up ...
rank_id = 83, input_tensor_shapes: [(2048, 2, 8192)]
rank:83,cuda fwd time: 168.17254638671875
rank:83, fwd_subop num: 29, fwd_subop: ['trace_src_func=allreduce,duration=4.65,timestamp=4410002754.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4410002758.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.07,timestamp=4410002764.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4410002769.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4410002775.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4410002780.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.16,timestamp=4410002786.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4410002790.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.37,timestamp=4410002797.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4410002801.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.16,timestamp=4410002807.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4410002812.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4410002818.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4410002823.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.19,timestamp=4410002829.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4410002834.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4410002840.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4410002845.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.2,timestamp=4410002851.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4410002856.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.17,timestamp=4410002862.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.34,timestamp=4410002868.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.46,timestamp=4410002875.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.42,timestamp=4410002879.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.41,timestamp=4410002886.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.4,timestamp=4410002891.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=22.82,timestamp=4410002914.92,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4410002915.71,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.17,timestamp=4410002915.97,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 83, finish FWD profile ...
rank:83, bwd_subop num: 27, bwd_subop: ['trace_src_func=allreduce,duration=22.32,timestamp=4410002941.79,input__shape=[2048, 2, 8192],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=27.91,timestamp=4410002970.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.15,timestamp=4410002984.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410002992.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4410003006.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410003014.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4410003028.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410003037.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410003050.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4410003059.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.51,timestamp=4410003072.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410003081.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410003094.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410003103.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410003116.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410003125.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410003138.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410003147.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4410003161.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410003169.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4410003183.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4410003191.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4410003205.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410003213.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410003227.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410003235.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410003249.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:83, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff000 recvbuff 0x7f88467ff000 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff000,7f88467ff000,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:83,optimizer_step time: 27.040767669677734
rank:83, finish optimizer.step profile ...
rank:83, Before memory release - Allocated: 33224838144, Reserved: 52533657600
rank:83, trace log has been written to txt...
rank:83, finish release GPU memory ...
rank:83, After memory release - Allocated: 11181601792, Reserved: 27571257344
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff600 recvbuff 0x7f8b753ff600 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff600,7f8b753ff600,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ff800 recvbuff 0x7f8b753ff800 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ff800,7f8b753ff800,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffa00 recvbuff 0x7f8b753ffa00 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffa00,7f8b753ffa00,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f7b72000000 recvbuff 0x7f7b72000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f7b72000000,7f7b72000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 5): 1360882688
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1360882688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff600 recvbuff 0x7f88467ff600 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff600,7f88467ff600,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 84, finish warm up ...
rank_id = 84, input_tensor_shapes: [(2048, 2, 8192)]
rank:84,cuda fwd time: 170.27687072753906
rank:84, fwd_subop num: 29, fwd_subop: ['trace_src_func=allreduce,duration=4.64,timestamp=4410004575.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4410004580.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.08,timestamp=4410004586.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4410004591.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.11,timestamp=4410004597.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4410004601.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.16,timestamp=4410004607.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4410004612.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.7,timestamp=4410004618.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4410004623.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.13,timestamp=4410004630.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4410004635.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.2,timestamp=4410004641.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.05,timestamp=4410004646.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.18,timestamp=4410004655.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.47,timestamp=4410004659.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4410004666.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4410004670.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.23,timestamp=4410004677.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.61,timestamp=4410004681.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.51,timestamp=4410004688.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.41,timestamp=4410004692.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.37,timestamp=4410004699.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.42,timestamp=4410004703.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.37,timestamp=4410004710.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.4,timestamp=4410004714.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=22.84,timestamp=4410004738.89,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4410004739.45,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.17,timestamp=4410004739.7,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 84, finish FWD profile ...
rank:84, bwd_subop num: 27, bwd_subop: ['trace_src_func=allreduce,duration=22.31,timestamp=4410004763.7,input__shape=[2048, 2, 8192],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=27.91,timestamp=4410004792.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.11,timestamp=4410004806.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410004814.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.54,timestamp=4410004828.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410004836.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410004850.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4410004859.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4410004872.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4410004881.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410004894.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410004903.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4410004916.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410004925.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.6,timestamp=4410004938.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410004947.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4410004960.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.04,timestamp=4410004969.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410004983.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410004991.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410005005.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410005013.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4410005027.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4410005035.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410005049.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410005058.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410005071.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:84, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff200 recvbuff 0x7f88467ff200 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff200,7f88467ff200,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:84,optimizer_step time: 27.060224533081055
rank:84, finish optimizer.step profile ...
rank:84, Before memory release - Allocated: 33224838144, Reserved: 52539949056
rank:84, trace log has been written to txt...
rank:84, finish release GPU memory ...
rank:84, After memory release - Allocated: 11181601792, Reserved: 27839692800
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffc00 recvbuff 0x7f8b753ffc00 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffc00,7f8b753ffc00,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753ffe00 recvbuff 0x7f8b753ffe00 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753ffe00,7f8b753ffe00,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff000 recvbuff 0x7f88467ff000 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff000,7f88467ff000,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f7fd2000000 recvbuff 0x7f7fd2000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f7fd2000000,7f7fd2000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 5): 1360882688
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1360882688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffc00 recvbuff 0x7f88467ffc00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffc00,7f88467ffc00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 85, finish warm up ...
rank_id = 85, input_tensor_shapes: [(2048, 2, 8192)]
rank:85,cuda fwd time: 166.97036743164062
rank:85, fwd_subop num: 29, fwd_subop: ['trace_src_func=allreduce,duration=4.65,timestamp=4410006401.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4410006405.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.9,timestamp=4410006412.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4410006416.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.09,timestamp=4410006422.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4410006427.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.07,timestamp=4410006433.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4410006438.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.6,timestamp=4410006444.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4410006448.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4410006455.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4410006459.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.18,timestamp=4410006466.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4410006470.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.2,timestamp=4410006477.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4410006481.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4410006488.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4410006492.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4410006499.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.6,timestamp=4410006503.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.78,timestamp=4410006510.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.41,timestamp=4410006514.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.52,timestamp=4410006521.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.38,timestamp=4410006526.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.35,timestamp=4410006532.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.4,timestamp=4410006537.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=22.82,timestamp=4410006561.04,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4410006561.58,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.17,timestamp=4410006561.84,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 85, finish FWD profile ...
rank:85, bwd_subop num: 27, bwd_subop: ['trace_src_func=allreduce,duration=22.31,timestamp=4410006585.92,input__shape=[2048, 2, 8192],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=27.94,timestamp=4410006614.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.1,timestamp=4410006628.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.02,timestamp=4410006636.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4410006650.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410006659.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.54,timestamp=4410006672.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410006681.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4410006694.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.04,timestamp=4410006703.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4410006716.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410006725.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410006738.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.04,timestamp=4410006747.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410006760.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410006769.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410006783.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410006791.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4410006805.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410006813.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4410006827.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410006835.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4410006849.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410006858.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410006871.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.04,timestamp=4410006880.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.54,timestamp=4410006893.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:85, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffc00 recvbuff 0x7f88467ffc00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffc00,7f88467ffc00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:85,optimizer_step time: 27.108352661132812
rank:85, finish optimizer.step profile ...
rank:85, Before memory release - Allocated: 33224838144, Reserved: 52539949056
rank:85, trace log has been written to txt...
rank:85, finish release GPU memory ...
rank:85, After memory release - Allocated: 11181601792, Reserved: 27839692800
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753fe400 recvbuff 0x7f8b753fe400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753fe400,7f8b753fe400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753fe200 recvbuff 0x7f8b753fe200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753fe200,7f8b753fe200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8b753fe400 recvbuff 0x7f8b753fe400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8b753fe400,7f8b753fe400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8274000000 recvbuff 0x7f8274000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8274000000,7f8274000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 5): 1360882688
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1360882688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffc00 recvbuff 0x7f88467ffc00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffc00,7f88467ffc00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 86, finish warm up ...
rank_id = 86, input_tensor_shapes: [(2048, 2, 8192)]
rank:86,cuda fwd time: 166.54949951171875
rank:86, fwd_subop num: 29, fwd_subop: ['trace_src_func=allreduce,duration=4.66,timestamp=4410008213.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4410008218.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.08,timestamp=4410008224.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4410008228.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.12,timestamp=4410008235.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4410008239.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.15,timestamp=4410008245.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4410008250.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.63,timestamp=4410008256.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.78,timestamp=4410008261.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.17,timestamp=4410008267.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4410008272.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.19,timestamp=4410008278.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4410008282.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.94,timestamp=4410008289.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4410008293.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4410008300.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4410008304.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.24,timestamp=4410008311.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4410008315.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.63,timestamp=4410008322.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.4,timestamp=4410008326.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.45,timestamp=4410008333.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.39,timestamp=4410008337.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.42,timestamp=4410008344.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.33,timestamp=4410008349.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=22.83,timestamp=4410008372.99,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4410008373.52,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.17,timestamp=4410008373.78,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 86, finish FWD profile ...
rank:86, bwd_subop num: 27, bwd_subop: ['trace_src_func=allreduce,duration=22.31,timestamp=4410008397.74,input__shape=[2048, 2, 8192],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=27.9,timestamp=4410008426.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.05,timestamp=4410008440.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410008448.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4410008462.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410008470.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4410008484.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410008492.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410008506.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4410008515.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410008528.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410008537.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410008550.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410008559.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.54,timestamp=4410008572.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4410008581.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410008594.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.03,timestamp=4410008603.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410008616.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410008625.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410008639.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410008647.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4410008661.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410008669.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410008683.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410008691.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410008705.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:86, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffc00 recvbuff 0x7f88467ffc00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffc00,7f88467ffc00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:86,optimizer_step time: 26.93631935119629
rank:86, finish optimizer.step profile ...
rank:86, Before memory release - Allocated: 33224838144, Reserved: 52556726272
rank:86, trace log has been written to txt...
rank:86, finish release GPU memory ...
rank:86, After memory release - Allocated: 11181601792, Reserved: 27363639296
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff200 recvbuff 0x7f88467ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff200,7f88467ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff000 recvbuff 0x7f88467ff000 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff000,7f88467ff000,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff200 recvbuff 0x7f88467ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff200,7f88467ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f7fd2000000 recvbuff 0x7f7fd2000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f7fd2000000,7f7fd2000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 5): 1360882688
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1360882688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 87, finish warm up ...
rank_id = 87, input_tensor_shapes: [(2048, 2, 8192)]
rank:87,cuda fwd time: 173.0160675048828
rank:87, fwd_subop num: 29, fwd_subop: ['trace_src_func=allreduce,duration=4.47,timestamp=4410010124.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.95,timestamp=4410010129.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.93,timestamp=4410010135.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4410010140.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.0,timestamp=4410010146.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4410010150.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.99,timestamp=4410010157.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4410010161.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4410010167.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4410010172.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4410010178.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.7,timestamp=4410010183.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.0,timestamp=4410010194.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.7,timestamp=4410010199.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.92,timestamp=4410010206.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.73,timestamp=4410010210.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4410010217.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.72,timestamp=4410010221.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.86,timestamp=4410010228.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.73,timestamp=4410010232.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.9,timestamp=4410010239.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.27,timestamp=4410010243.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.06,timestamp=4410010250.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.25,timestamp=4410010255.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.0,timestamp=4410010261.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.27,timestamp=4410010266.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=22.8,timestamp=4410010290.48,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.04,timestamp=4410010291.09,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.15,timestamp=4410010291.35,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 87, finish FWD profile ...
rank:87, bwd_subop num: 27, bwd_subop: ['trace_src_func=allreduce,duration=22.3,timestamp=4410010315.72,input__shape=[2048, 2, 8192],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=27.94,timestamp=4410010344.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.13,timestamp=4410010358.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.02,timestamp=4410010366.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410010380.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410010388.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4410010402.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410010410.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410010424.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4410010433.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410010446.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4410010455.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.6,timestamp=4410010468.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410010477.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410010490.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410010499.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4410010512.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410010521.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4410010534.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410010543.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4410010557.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4410010565.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.65,timestamp=4410010579.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.13,timestamp=4410010588.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410010601.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410010610.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4410010623.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:87, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:87,optimizer_step time: 27.297792434692383
rank:87, finish optimizer.step profile ...
rank:87, Before memory release - Allocated: 33224838144, Reserved: 52550434816
rank:87, trace log has been written to txt...
rank:87, finish release GPU memory ...
rank:87, After memory release - Allocated: 11181601792, Reserved: 27363639296
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff400 recvbuff 0x7f88467ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff400,7f88467ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff600 recvbuff 0x7f88467ff600 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff600,7f88467ff600,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff800 recvbuff 0x7f88467ff800 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff800,7f88467ff800,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f7fe2000000 recvbuff 0x7f7fe2000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f7fe2000000,7f7fe2000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1360882688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 88, finish warm up ...
rank_id = 88, input_tensor_shapes: [(2048, 2, 8192)]
rank:88,cuda fwd time: 184.8548126220703
rank:88, fwd_subop num: 29, fwd_subop: ['trace_src_func=allreduce,duration=4.67,timestamp=4410012102.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4410012106.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4410012113.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.88,timestamp=4410012117.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.41,timestamp=4410012123.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4410012128.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.01,timestamp=4410012134.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.08,timestamp=4410012139.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.65,timestamp=4410012145.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4410012150.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.32,timestamp=4410012156.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4410012161.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.29,timestamp=4410012168.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.91,timestamp=4410012173.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.66,timestamp=4410012179.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.97,timestamp=4410012184.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.3,timestamp=4410012191.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4410012195.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.2,timestamp=4410012215.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.87,timestamp=4410012220.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=1.12,timestamp=4410012227.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.43,timestamp=4410012233.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.39,timestamp=4410012239.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.45,timestamp=4410012244.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.34,timestamp=4410012251.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.45,timestamp=4410012255.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=23.07,timestamp=4410012280.06,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4410012280.61,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.16,timestamp=4410012280.86,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 88, finish FWD profile ...
rank:88, bwd_subop num: 27, bwd_subop: ['trace_src_func=allreduce,duration=22.53,timestamp=4410012305.21,input__shape=[2048, 2, 8192],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=27.99,timestamp=4410012334.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.13,timestamp=4410012347.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410012356.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4410012369.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.11,timestamp=4410012378.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410012392.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.13,timestamp=4410012400.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.61,timestamp=4410012414.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.11,timestamp=4410012423.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410012436.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.13,timestamp=4410012445.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.61,timestamp=4410012458.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.13,timestamp=4410012467.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.62,timestamp=4410012481.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.15,timestamp=4410012489.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.62,timestamp=4410012503.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.15,timestamp=4410012512.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.62,timestamp=4410012525.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.11,timestamp=4410012534.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.6,timestamp=4410012547.87,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.13,timestamp=4410012556.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.62,timestamp=4410012570.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.09,timestamp=4410012578.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4410012592.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.14,timestamp=4410012601.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.6,timestamp=4410012614.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:88, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:88,optimizer_step time: 30.130176544189453
rank:88, finish optimizer.step profile ...
rank:88, Before memory release - Allocated: 33224838144, Reserved: 52533657600
rank:88, trace log has been written to txt...
rank:88, finish release GPU memory ...
rank:88, After memory release - Allocated: 11181601792, Reserved: 27571257344
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff200 recvbuff 0x7f88467ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff200,7f88467ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff000 recvbuff 0x7f88467ff000 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff000,7f88467ff000,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff200 recvbuff 0x7f88467ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff200,7f88467ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f7fc4000000 recvbuff 0x7f7fc4000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f7fc4000000,7f7fc4000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1360882688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 89, finish warm up ...
rank_id = 89, input_tensor_shapes: [(2048, 2, 8192)]
rank:89,cuda fwd time: 172.03506469726562
rank:89, fwd_subop num: 29, fwd_subop: ['trace_src_func=allreduce,duration=4.64,timestamp=4410013940.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4410013945.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.07,timestamp=4410013951.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4410013956.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4410013962.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4410013966.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.13,timestamp=4410013972.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4410013977.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.72,timestamp=4410013983.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4410013988.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.17,timestamp=4410013994.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4410013999.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.23,timestamp=4410014005.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4410014010.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4410014016.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4410014021.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4410014027.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4410014032.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4410014042.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.37,timestamp=4410014046.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.62,timestamp=4410014053.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.42,timestamp=4410014057.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.36,timestamp=4410014064.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.38,timestamp=4410014068.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.36,timestamp=4410014075.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=2.18,timestamp=4410014079.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=22.83,timestamp=4410014105.7,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4410014106.26,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.17,timestamp=4410014106.51,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 89, finish FWD profile ...
rank:89, bwd_subop num: 27, bwd_subop: ['trace_src_func=allreduce,duration=22.31,timestamp=4410014130.58,input__shape=[2048, 2, 8192],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=27.89,timestamp=4410014159.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.12,timestamp=4410014172.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410014181.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4410014195.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410014203.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410014217.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4410014225.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410014239.39,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410014248.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4410014261.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.09,timestamp=4410014270.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410014283.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410014292.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410014305.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410014314.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410014327.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410014336.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4410014349.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410014358.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4410014372.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4410014380.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4410014394.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410014402.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4410014416.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410014424.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4410014438.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:89, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:89,optimizer_step time: 27.1278076171875
rank:89, finish optimizer.step profile ...
rank:89, Before memory release - Allocated: 33224838144, Reserved: 52556726272
rank:89, trace log has been written to txt...
rank:89, finish release GPU memory ...
rank:89, After memory release - Allocated: 11181601792, Reserved: 27839692800
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff400 recvbuff 0x7f88467ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff400,7f88467ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff600 recvbuff 0x7f88467ff600 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff600,7f88467ff600,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff800 recvbuff 0x7f88467ff800 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff800,7f88467ff800,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f7fe2000000 recvbuff 0x7f7fe2000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f7fe2000000,7f7fe2000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1360882688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 90, finish warm up ...
rank_id = 90, input_tensor_shapes: [(2048, 2, 8192)]
rank:90,cuda fwd time: 174.3953857421875
rank:90, fwd_subop num: 29, fwd_subop: ['trace_src_func=allreduce,duration=4.64,timestamp=4410015749.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4410015754.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.06,timestamp=4410015760.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4410015765.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4410015771.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4410015775.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.15,timestamp=4410015782.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4410015786.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.69,timestamp=4410015792.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4410015797.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4410015803.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4410015808.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.18,timestamp=4410015814.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4410015819.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.24,timestamp=4410015825.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4410015830.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4410015844.41,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4410015849.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.99,timestamp=4410015855.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.61,timestamp=4410015859.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4410015866.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.42,timestamp=4410015871.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.49,timestamp=4410015877.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.42,timestamp=4410015882.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.37,timestamp=4410015888.65,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.4,timestamp=4410015893.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=22.83,timestamp=4410015917.13,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4410015917.68,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.17,timestamp=4410015917.93,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 90, finish FWD profile ...
rank:90, bwd_subop num: 27, bwd_subop: ['trace_src_func=allreduce,duration=22.31,timestamp=4410015942.42,input__shape=[2048, 2, 8192],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=26.7,timestamp=4410015971.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=10.11,timestamp=4410015984.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.04,timestamp=4410015993.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4410016006.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410016015.59,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410016029.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410016037.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410016051.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4410016059.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410016073.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410016081.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410016095.36,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410016104.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410016117.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.04,timestamp=4410016126.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.53,timestamp=4410016139.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410016148.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4410016161.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.04,timestamp=4410016170.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.52,timestamp=4410016183.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.04,timestamp=4410016192.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4410016205.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410016214.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.5,timestamp=4410016228.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410016236.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.53,timestamp=4410016250.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:90, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:90,optimizer_step time: 27.13804817199707
rank:90, finish optimizer.step profile ...
rank:90, Before memory release - Allocated: 33224838144, Reserved: 52539949056
rank:90, trace log has been written to txt...
rank:90, finish release GPU memory ...
rank:90, After memory release - Allocated: 11181601792, Reserved: 27839692800
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff200 recvbuff 0x7f88467ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff200,7f88467ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff000 recvbuff 0x7f88467ff000 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff000,7f88467ff000,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff200 recvbuff 0x7f88467ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff200,7f88467ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8274000000 recvbuff 0x7f8274000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8274000000,7f8274000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1360882688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 91, finish warm up ...
rank_id = 91, input_tensor_shapes: [(2048, 2, 8192)]
rank:91,cuda fwd time: 166.74610900878906
rank:91, fwd_subop num: 29, fwd_subop: ['trace_src_func=allreduce,duration=4.64,timestamp=4410017579.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4410017584.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.07,timestamp=4410017590.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4410017595.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.12,timestamp=4410017601.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4410017605.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.17,timestamp=4410017612.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4410017616.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.65,timestamp=4410017622.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4410017627.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.2,timestamp=4410017633.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4410017638.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.2,timestamp=4410017644.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4410017649.22,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4410017655.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4410017660.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.09,timestamp=4410017666.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.75,timestamp=4410017671.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.2,timestamp=4410017677.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4410017682.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.58,timestamp=4410017688.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.42,timestamp=4410017693.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.41,timestamp=4410017699.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.44,timestamp=4410017704.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.4,timestamp=4410017710.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.42,timestamp=4410017715.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=22.98,timestamp=4410017739.48,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4410017740.02,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.17,timestamp=4410017740.27,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 91, finish FWD profile ...
rank:91, bwd_subop num: 27, bwd_subop: ['trace_src_func=allreduce,duration=22.39,timestamp=4410017765.58,input__shape=[2048, 2, 8192],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=28.12,timestamp=4410017794.66,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.13,timestamp=4410017808.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410017816.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.53,timestamp=4410017830.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410017838.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410017852.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.04,timestamp=4410017861.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410017874.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410017883.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410017896.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410017905.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4410017918.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410017927.37,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410017940.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4410017949.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4410017962.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.04,timestamp=4410017971.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410017985.06,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410017993.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410018007.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410018015.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410018029.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410018037.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.6,timestamp=4410018051.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.13,timestamp=4410018060.18,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410018073.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:91, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:91,optimizer_step time: 27.088895797729492
rank:91, finish optimizer.step profile ...
rank:91, Before memory release - Allocated: 33224838144, Reserved: 52556726272
rank:91, trace log has been written to txt...
rank:91, finish release GPU memory ...
rank:91, After memory release - Allocated: 11181601792, Reserved: 27363639296
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff400 recvbuff 0x7f88467ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff400,7f88467ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff600 recvbuff 0x7f88467ff600 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff600,7f88467ff600,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff800 recvbuff 0x7f88467ff800 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff800,7f88467ff800,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f7fd4000000 recvbuff 0x7f7fd4000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f7fd4000000,7f7fd4000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1360882688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 92, finish warm up ...
rank_id = 92, input_tensor_shapes: [(2048, 2, 8192)]
rank:92,cuda fwd time: 172.35250854492188
rank:92, fwd_subop num: 29, fwd_subop: ['trace_src_func=allreduce,duration=4.64,timestamp=4410019406.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4410019411.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.08,timestamp=4410019417.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4410019422.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.13,timestamp=4410019428.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4410019433.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.12,timestamp=4410019439.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4410019443.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4410019449.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4410019454.43,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.18,timestamp=4410019460.81,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4410019465.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.12,timestamp=4410019476.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4410019480.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.23,timestamp=4410019486.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=1.98,timestamp=4410019493.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.93,timestamp=4410019499.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4410019504.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.19,timestamp=4410019510.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4410019515.13,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4410019521.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.42,timestamp=4410019526.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.39,timestamp=4410019532.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.42,timestamp=4410019537.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.34,timestamp=4410019543.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.39,timestamp=4410019548.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=22.83,timestamp=4410019572.19,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4410019572.74,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.17,timestamp=4410019573.0,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 92, finish FWD profile ...
rank:92, bwd_subop num: 27, bwd_subop: ['trace_src_func=allreduce,duration=22.3,timestamp=4410019598.16,input__shape=[2048, 2, 8192],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=27.93,timestamp=4410019627.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.12,timestamp=4410019640.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.02,timestamp=4410019649.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4410019662.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410019671.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4410019684.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410019693.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410019706.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.04,timestamp=4410019715.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410019729.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410019737.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4410019751.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410019759.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.54,timestamp=4410019773.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.04,timestamp=4410019781.92,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410019795.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410019804.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4410019817.51,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410019826.17,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4410019839.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410019848.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410019861.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.02,timestamp=4410019870.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.54,timestamp=4410019883.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410019892.52,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4410019905.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:92, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:92,optimizer_step time: 27.055103302001953
rank:92, finish optimizer.step profile ...
rank:92, Before memory release - Allocated: 33224838144, Reserved: 52550434816
rank:92, trace log has been written to txt...
rank:92, finish release GPU memory ...
rank:92, After memory release - Allocated: 11181601792, Reserved: 27363639296
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff200 recvbuff 0x7f88467ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff200,7f88467ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff000 recvbuff 0x7f88467ff000 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff000,7f88467ff000,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff200 recvbuff 0x7f88467ff200 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff200,7f88467ff200,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f7ffe000000 recvbuff 0x7f7ffe000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f7ffe000000,7f7ffe000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1360882688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 93, finish warm up ...
rank_id = 93, input_tensor_shapes: [(2048, 2, 8192)]
rank:93,cuda fwd time: 170.81549072265625
rank:93, fwd_subop num: 29, fwd_subop: ['trace_src_func=allreduce,duration=4.62,timestamp=4410021228.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.01,timestamp=4410021232.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.08,timestamp=4410021239.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4410021243.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.12,timestamp=4410021249.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4410021254.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.1,timestamp=4410021260.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4410021265.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.67,timestamp=4410021271.16,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.78,timestamp=4410021275.77,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.17,timestamp=4410021282.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4410021286.74,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.19,timestamp=4410021293.11,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4410021297.7,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.11,timestamp=4410021308.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4410021312.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.1,timestamp=4410021319.34,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4410021323.93,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.22,timestamp=4410021330.29,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.84,timestamp=4410021334.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.6,timestamp=4410021341.26,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.42,timestamp=4410021345.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.39,timestamp=4410021352.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.41,timestamp=4410021356.96,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.36,timestamp=4410021363.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.4,timestamp=4410021368.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=22.83,timestamp=4410021391.91,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4410021392.46,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.17,timestamp=4410021392.72,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 93, finish FWD profile ...
rank:93, bwd_subop num: 27, bwd_subop: ['trace_src_func=allreduce,duration=22.31,timestamp=4410021416.68,input__shape=[2048, 2, 8192],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=27.97,timestamp=4410021445.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.05,timestamp=4410021459.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=7.99,timestamp=4410021467.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.53,timestamp=4410021481.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410021489.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410021503.32,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410021511.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410021525.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4410021534.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4410021547.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4410021556.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4410021569.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410021578.33,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.6,timestamp=4410021591.8,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4410021600.44,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410021613.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410021622.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410021636.03,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4410021644.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4410021658.14,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410021666.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4410021680.25,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410021688.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410021702.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4410021710.99,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4410021724.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:93, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:93,optimizer_step time: 27.10220718383789
rank:93, finish optimizer.step profile ...
rank:93, Before memory release - Allocated: 33224838144, Reserved: 52533657600
rank:93, trace log has been written to txt...
rank:93, finish release GPU memory ...
rank:93, After memory release - Allocated: 11181601792, Reserved: 27571257344
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff400 recvbuff 0x7f88467ff400 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff400,7f88467ff400,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff600 recvbuff 0x7f88467ff600 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff600,7f88467ff600,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff800 recvbuff 0x7f88467ff800 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff800,7f88467ff800,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f7fbe000000 recvbuff 0x7f7fbe000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f7fbe000000,7f7fbe000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1360882688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8adffffc00 recvbuff 0x7f8adffffc00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f8adffffc00,7f8adffffc00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 94, finish warm up ...
rank_id = 94, input_tensor_shapes: [(2048, 2, 8192)]
rank:94,cuda fwd time: 170.48985290527344
rank:94, fwd_subop num: 29, fwd_subop: ['trace_src_func=allreduce,duration=4.63,timestamp=4410023062.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.0,timestamp=4410023066.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.07,timestamp=4410023073.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.05,timestamp=4410023077.71,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4410023083.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4410023088.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.14,timestamp=4410023094.55,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4410023099.15,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.69,timestamp=4410023105.28,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4410023109.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.17,timestamp=4410023116.27,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=1.98,timestamp=4410023122.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4410023128.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4410023133.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.21,timestamp=4410023139.82,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.82,timestamp=4410023144.42,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.18,timestamp=4410023151.45,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4410023156.04,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.2,timestamp=4410023164.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.42,timestamp=4410023168.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.6,timestamp=4410023174.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.41,timestamp=4410023179.57,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.43,timestamp=4410023186.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.39,timestamp=4410023190.68,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.33,timestamp=4410023197.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.39,timestamp=4410023201.78,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=22.83,timestamp=4410023225.69,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4410023226.24,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.17,timestamp=4410023226.5,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 94, finish FWD profile ...
rank:94, bwd_subop num: 27, bwd_subop: ['trace_src_func=allreduce,duration=22.32,timestamp=4410023250.83,input__shape=[2048, 2, 8192],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=27.78,timestamp=4410023279.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.07,timestamp=4410023293.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410023301.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410023315.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410023324.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410023337.47,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4410023346.12,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410023359.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410023368.24,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.53,timestamp=4410023381.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410023390.38,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.52,timestamp=4410023403.84,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410023412.49,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410023425.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410023434.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.54,timestamp=4410023448.07,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410023456.73,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410023470.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4410023478.85,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.54,timestamp=4410023492.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.08,timestamp=4410023500.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4410023514.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410023523.05,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4410023536.53,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410023545.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4410023558.64,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:94, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffe00 recvbuff 0x7f88467ffe00 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffe00,7f88467ffe00,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:94,optimizer_step time: 27.115520477294922
rank:94, finish optimizer.step profile ...
rank:94, Before memory release - Allocated: 33224838144, Reserved: 52556726272
rank:94, trace log has been written to txt...
rank:94, finish release GPU memory ...
rank:94, After memory release - Allocated: 11181601792, Reserved: 27839692800
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      40
    validation: 4
    test:       4
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(40, 4, 4), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._GPT2BPETokenizer object at 0x7f8bc9c42880>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_gpt2/my-gpt2_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 5e1e11971ef40f8dc73a2ef88be465ab-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 108411
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffa00 recvbuff 0x7f88467ffa00 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffa00,7f88467ffa00,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 110f523a905a9a1bc24b200899db174e-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 5789
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffc00 recvbuff 0x7f88467ffc00 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffc00,7f88467ffc00,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from f095252d24c4530dc562c92b8b082e5a-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 184
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ffe00 recvbuff 0x7f88467ffe00 count 1 datatype 1 op 0 root 0 comm 0x8d98720 [nranks=1] stream 0x85a8470
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ffe00,7f88467ffe00,1,1,0,0,0x8d98720,0x85a8470)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f7fe8000000 recvbuff 0x7f7fe8000000 count 51511296 datatype 7 op 0 root 0 comm 0xc1bc3c0 [nranks=1] stream 0x8daf2e0
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f7fe8000000,7f7fe8000000,51511296,7,0,0,0xc1bc3c0,0x8daf2e0)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (1360882688 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.11.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.mlp.linear_fc1.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.12.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.10.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.7.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.5.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.4.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.6.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=False, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f8bc9c9d820>)
> learning rate decay style: cosine
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff600 recvbuff 0x7f88467ff600 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff600,7f88467ff600,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank_id = 95, finish warm up ...
rank_id = 95, input_tensor_shapes: [(2048, 2, 8192)]
rank:95,cuda fwd time: 166.71743774414062
rank:95, fwd_subop num: 29, fwd_subop: ['trace_src_func=allreduce,duration=4.62,timestamp=4410024923.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.98,timestamp=4410024927.75,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.07,timestamp=4410024933.89,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.04,timestamp=4410024938.48,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.09,timestamp=4410024944.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.06,timestamp=4410024949.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=5.12,timestamp=4410024955.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.03,timestamp=4410024959.9,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.67,timestamp=4410024966.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.07,timestamp=4410024970.61,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.12,timestamp=4410024977.02,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.8,timestamp=4410024981.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.16,timestamp=4410024987.98,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.81,timestamp=4410024992.58,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.12,timestamp=4410024998.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.79,timestamp=4410025003.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4410025009.95,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.83,timestamp=4410025014.54,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=4.02,timestamp=4410025020.91,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.58,timestamp=4410025025.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.76,timestamp=4410025032.01,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.37,timestamp=4410025036.6,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.49,timestamp=4410025043.09,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.39,timestamp=4410025047.69,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.28,timestamp=4410025054.2,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=3.39,timestamp=4410025058.79,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=22.81,timestamp=4410025082.67,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.03,timestamp=4410025083.26,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.17,timestamp=4410025083.52,input__shape=[2048, 2],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 95, finish FWD profile ...
rank:95, bwd_subop num: 27, bwd_subop: ['trace_src_func=allreduce,duration=22.3,timestamp=4410025107.7,input__shape=[2048, 2, 8192],input__dtype=torch.float32,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=27.82,timestamp=4410025136.63,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.04,timestamp=4410025150.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.02,timestamp=4410025158.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.51,timestamp=4410025172.23,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410025180.88,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4410025194.35,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410025203.0,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4410025216.46,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410025225.1,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4410025238.56,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410025247.21,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.58,timestamp=4410025260.67,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410025269.31,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.56,timestamp=4410025282.76,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410025291.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.55,timestamp=4410025304.86,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410025313.5,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4410025326.97,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.02,timestamp=4410025335.62,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410025349.08,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.07,timestamp=4410025357.72,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4410025371.19,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.05,timestamp=4410025379.83,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.57,timestamp=4410025393.3,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=8.06,timestamp=4410025401.94,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=12.59,timestamp=4410025415.4,input__shape=[4096, 8192],input__dtype=torch.float32,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:95, finish BWD profile ...
proj187:3296790:3296790 NCCL CALL ncclGroupStart()
proj187:3296790:3296790 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f88467ff600 recvbuff 0x7f88467ff600 count 1 datatype 7 op 0 root 0 comm 0x194705e0 [nranks=1] stream 0x8db8260
proj187:3296790:3296790 NCCL CALL ncclAllReduce(7f88467ff600,7f88467ff600,1,7,0,0,0x194705e0,0x8db8260)
proj187:3296790:3296790 NCCL CALL ncclGroupEnd()
rank:95,optimizer_step time: 28.11084747314453
rank:95, finish optimizer.step profile ...
rank:95, Before memory release - Allocated: 33224838144, Reserved: 52539949056
rank:95, trace log has been written to txt...
rank:95, finish release GPU memory ...
rank:95, After memory release - Allocated: 11181601792, Reserved: 27839692800
